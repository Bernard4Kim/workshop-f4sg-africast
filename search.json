[
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "Tidy time series & forecasting in R",
    "section": "Course Overview",
    "text": "Course Overview\nForecasting is a valuable tool that allows organizations to make informed decisions about the future. Time series forecasting, in particular, uses historical data to predict future trends over time. This technique has extensive applications across a wide range of fields, including finance and economics, health and humanitarian operations, supply chain management, and more. By analyzing trends and patterns in data, time series forecasting can help decision-makers identify potential challenges and opportunities, and plan accordingly.\nIt is important for researchers in Low- and Middle-Income Countries (LMICs) to develop technical skill in data analysis and forecasting techniques, which are essential for accurate and reliable forecasting. By having these skills, researchers can analyze data, identify trends and patterns, and develop robust forecasting models to make informed decisions that can improve resource allocation and planning in LMICs. Additionally, researchers can collaborate with policy makers and stakeholders to ensure that the forecast results are integrated into decision-making processes, leading to more efficient and effective resource management strategies.\nThis workshop is part of the Forecasting for Social Good (F4SG) initiative, and will run online from the 23rd-27th October 2023."
  },
  {
    "objectID": "index.html#learning-objectives",
    "href": "index.html#learning-objectives",
    "title": "Tidy time series & forecasting in R",
    "section": "Learning objectives",
    "text": "Learning objectives\nDuring the training, participants will gain knowledge and skills in:\n\nPreparing time series data for analysis and exploration.\nExtracting and computing useful features from time series data and effectively visualizing it.\nIdentifying appropriate forecasting algorithms for time series and selecting the best approach for the data at hand."
  },
  {
    "objectID": "index.html#instructor",
    "href": "index.html#instructor",
    "title": "Tidy time series & forecasting in R",
    "section": "Instructor",
    "text": "Instructor\n Mitchell O’Hara-Wild (he/him) is a PhD student at Monash University, creating new techniques and tools for forecasting large collections of time series with Rob Hyndman and George Athanasopoulos. He is the lead developer of the tidy time-series forecasting tools fable and feasts, and has co-developed the widely used forecast package since 2015. Mitchell also operates a data consultancy, and has worked on many forecasting projects that have supported decision making and planning for businesses and governments. He is an award-winning educator, and has taught applied forecasting at Monash University and various forecasting workshops around the world."
  },
  {
    "objectID": "index.html#instructor-1",
    "href": "index.html#instructor-1",
    "title": "Tidy time series & forecasting in R",
    "section": "Instructor",
    "text": "Instructor\n\nBahman is a Reader (Associate Professor) in Data-Driven Decision Science at Cardiff Business School, Cardiff University, UK. He serves as the director of the Data Lab for Social Good Research Group at Cardiff University and is also the founder of the Forecasting for Social Good committee within the International Institute of Forecasters. Bahman specializes in the development and application of modelling, forecasting and management science tools and techniques providing informed insights for planning & decision-making processes in sectors contributing to social good, including healthcare operations, global health and humanitarian supply chains, agriculture and food, social sustainability, and governmental policy. His collaborative efforts have spanned a multitude of organisations, including notable bodies such as the National Health Service (NHS), Welsh Ambulance Service Trusts (WAST), United States Agency for International Developments (USAID), the International Committee of the Red Cross (ICRC), and John Snow Inc. (JSI). A remarkable highlight of his contributions is his pivotal role in disseminating forecasting knowledge especially in low and lower-middle income countries through the democratizing forecasting project sponsored by International Institute of Forecasters."
  },
  {
    "objectID": "index.html#required-equipment",
    "href": "index.html#required-equipment",
    "title": "Tidy time series & forecasting in R",
    "section": "Required equipment",
    "text": "Required equipment\nPlease have your own laptop capable of running R."
  },
  {
    "objectID": "index.html#required-software",
    "href": "index.html#required-software",
    "title": "Tidy time series & forecasting in R",
    "section": "Required software",
    "text": "Required software\nTo be able to complete the exercises of this workshop, please install a suitable IDE (such as RStudio), a recent version of R (4.1+) and the following packages.\n\nTime series packages and extensions\n\nfpp3, sugrrants\n\ntidyverse packages and friends\n\ntidyverse, fpp3\n\n\nThe following code will install the main packages needed for the workshop.\ninstall.packages(c(\"tidyverse\",\"fpp3\", \"GGally\", \"sugrrants\", \"astsa\"))\nPlease have the required software installed and pre-work completed before attending the workshop."
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "This work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Time\n\n\nSession\n\n\n\n\n\n\n08:00-09:30\n\n\nBasics of time series and data structures\n\n\n\n\n09:45-11:15\n\n\nTime series patterns and basic graphics\n\n\n\n\nCheck with your mentor on Slack\n\n\nExercises\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "schedule.html#day-2",
    "href": "schedule.html#day-2",
    "title": "Schedule",
    "section": "Day 2",
    "text": "Day 2\n\n\n\n\n\n\nTime\n\n\nSession\n\n\n\n\n\n\n08:00-09:30\n\n\nTransforming / adjusting time series\n\n\n\n\n09:45-11:15\n\n\nComputing and visualizing features\n\n\n\n\nCheck with your mentor on Slack\n\n\nExercises\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "schedule.html#day-3",
    "href": "schedule.html#day-3",
    "title": "Schedule",
    "section": "Day 3",
    "text": "Day 3\n\n\n\n\n\n\nTime\n\n\nSession\n\n\n\n\n\n\n08:00-09:30\n\n\nBasic modeling / forecasting\n\n\n\n\n09:45-11:15\n\n\nForecasting with regression, how to represent temporal structure with regressors\n\n\n\n\nCheck with your mentor on Slack\n\n\nExercises\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "schedule.html#day-4",
    "href": "schedule.html#day-4",
    "title": "Schedule",
    "section": "Day 4",
    "text": "Day 4\n\n\n\n\n\n\nTime\n\n\nSession\n\n\n\n\n\n\n08:00-09:30\n\n\nARIMA\n\n\n\n\n09:45-11:15\n\n\nETS\n\n\n\n\nCheck with your mentor on Slack\n\n\nExercises\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "schedule.html#day-5",
    "href": "schedule.html#day-5",
    "title": "Schedule",
    "section": "Day 5",
    "text": "Day 5\n\n\n\n\n\n\nTime\n\n\nSession\n\n\n\n\n\n\n08:00-09:30\n\n\nBasic training and test accuracy\n\n\n\n\n09:45-11:15\n\n\nResidual diagnostics and cross validation\n\n\n\n\nCheck with your mentor on Slack\n\n\nExercises\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "sessions/break.html",
    "href": "sessions/break.html",
    "title": "Coffee Break",
    "section": "",
    "text": "Time for a coffee break!\nFeel free to ask me some questions, or simply enjoy the break."
  },
  {
    "objectID": "sessions/day1/exercises.html",
    "href": "sessions/day1/exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "We’ve prepared an exercises project with some starter code for each of the sessions. You can download and open this project using:\n\nusethis::use_course(\"https://workshop.mitchelloharawild.com/f4sg-africa/exercises.zip\")"
  },
  {
    "objectID": "sessions/day1/exercises.html#creating-a-time-series-tibble-a-tsibble",
    "href": "sessions/day1/exercises.html#creating-a-time-series-tibble-a-tsibble",
    "title": "Exercises",
    "section": "Creating a time series tibble (a tsibble!)",
    "text": "Creating a time series tibble (a tsibble!)\nA tsibble is a rectangular data frame that contains:\n\na time column: the index\nidentifying column(s): the key variables\nvalues (the measured variables)\n\nYou usually create a tsibble by converting an existing dataset (read from a file) with as_tsibble(). For example, let’s look at the production of rice in Guinea.\n\n# Read in the dataset using readr\nlibrary(readr)\nguinea_rice <- read_csv(\"data/guinea_rice.csv\")\n\n# Convert the dataset to a tsibble\n# Here the index variable is 'Year', and there are no key variables.\n# The 'Production' variable is what we're interested in forecasting (the measured variable).\nlibrary(tsibble)\nguinea_rice <- as_tsibble(guinea_rice, index = Year)\n\nA tsibble enables time-aware data manipulation, which makes it easy to work with time series. It also has extra checks to prevent common errors, while these can be frustrating at first they are important in correctly analysing your data.\nThere are two common mistakes when creating a tsibble, which we’ll see in the next example of Australian accommodation.\n\n# Read in the dataset using readr\naus_accommodation <- read_csv(\"data/aus_accommodation.csv\")\n\n# Try to convert the dataset to a tsibble\naus_accommodation <- as_tsibble(aus_accommodation, index = Date)\n\nError in `validate_tsibble()`:\n! A valid tsibble must have distinct rows identified by key and index.\nℹ Please use `duplicates()` to check the duplicated rows.\n\n\n\n\n\n\n\n\nThat didn’t work…\n\n\n\nReading the error says we have ‘duplicated rows’. What this means is that we have two or more rows in the dataset for the same point in time. In time series it isn’t possible to get two different values at the same time, but it is possible to measure several different things at the same time.\n\n\nWhen you get this error, consider if any of the dataset’s variables can identify individual series.\n\n\n\n\n\n\nTip\n\n\n\nThe identifying key variables of a time series are usually character variables, and the measured variables are almost always numeric.\n\n\n\naus_accommodation\n\n# A tibble: 592 × 5\n   Date       State                        Takings Occupancy   CPI\n   <date>     <chr>                          <dbl>     <dbl> <dbl>\n 1 1998-01-01 Australian Capital Territory    24.3      65    67  \n 2 1998-04-01 Australian Capital Territory    22.3      59    67.4\n 3 1998-07-01 Australian Capital Territory    22.5      58    67.5\n 4 1998-10-01 Australian Capital Territory    24.4      59    67.8\n 5 1999-01-01 Australian Capital Territory    23.7      58    67.8\n 6 1999-04-01 Australian Capital Territory    25.4      61    68.1\n 7 1999-07-01 Australian Capital Territory    28.2      66    68.7\n 8 1999-10-01 Australian Capital Territory    25.8      60    69.1\n 9 2000-01-01 Australian Capital Territory    27.3      60.9  69.7\n10 2000-04-01 Australian Capital Territory    30.1      64.7  70.2\n# ℹ 582 more rows\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhich of these variable(s) identifies each time series?\n\n\nIn this dataset we have accommodation data from all 8 states in Australia, and so we need to specify State as a key variable when creating our tsibble.\n\n# Try to convert the dataset to a tsibble\naus_accommodation <- as_tsibble(aus_accommodation, index = Date, key = State)\naus_accommodation\n\n# A tsibble: 592 x 5 [1D]\n# Key:       State [8]\n   Date       State                        Takings Occupancy   CPI\n   <date>     <chr>                          <dbl>     <dbl> <dbl>\n 1 1998-01-01 Australian Capital Territory    24.3      65    67  \n 2 1998-04-01 Australian Capital Territory    22.3      59    67.4\n 3 1998-07-01 Australian Capital Territory    22.5      58    67.5\n 4 1998-10-01 Australian Capital Territory    24.4      59    67.8\n 5 1999-01-01 Australian Capital Territory    23.7      58    67.8\n 6 1999-04-01 Australian Capital Territory    25.4      61    68.1\n 7 1999-07-01 Australian Capital Territory    28.2      66    68.7\n 8 1999-10-01 Australian Capital Territory    25.8      60    69.1\n 9 2000-01-01 Australian Capital Territory    27.3      60.9  69.7\n10 2000-04-01 Australian Capital Territory    30.1      64.7  70.2\n# ℹ 582 more rows\n\n\nHurray, we have a tsibble! 🎉\n\n\n\n\n\n\nHowever there’s still one thing that isn’t right…\n\n\n\nIn the first row of the output we see [1D] - this means that the frequency of the data is daily.\n\n\nLooking at the index column (Date), we can see that each point in time is three months apart - or quarterly. This is another common mistake when working with time series, you need to set the appropriate temporal granularity.\n\n\n\n\n\n\nWhat is temporal granularity?\n\n\n\nTemporal granularity is the resolution in time. The time variable needs to match this resolution.\nIn this example, a date was used to represent quarters, but instead we must use yearquarter() to match the temporal granularity.\nHere’s a helpful list of common granularities:\n\nas.integer(): annual data (as above)\nyearquarter(): Quarterly data (shown here)\nyearmonth(): Monthly data\nyearweek(): Weekly data\nas.Date(): Daily data\nas.POSIXct(): Sub-daily data\n\n\n\nTo use the appropriate temporal granularity, we first must change our Date column before creating the tsibble.\n\n# Convert the `Date` column to quarterly with dplyr\nlibrary(dplyr)\naus_accommodation <- aus_accommodation |> \n  mutate(Date = yearquarter(Date)) |> \n  as_tsibble(index = Date, key = State)\naus_accommodation\n\n# A tsibble: 592 x 5 [1Q]\n# Key:       State [8]\n      Date State                        Takings Occupancy   CPI\n     <qtr> <chr>                          <dbl>     <dbl> <dbl>\n 1 1998 Q1 Australian Capital Territory    24.3      65    67  \n 2 1998 Q2 Australian Capital Territory    22.3      59    67.4\n 3 1998 Q3 Australian Capital Territory    22.5      58    67.5\n 4 1998 Q4 Australian Capital Territory    24.4      59    67.8\n 5 1999 Q1 Australian Capital Territory    23.7      58    67.8\n 6 1999 Q2 Australian Capital Territory    25.4      61    68.1\n 7 1999 Q3 Australian Capital Territory    28.2      66    68.7\n 8 1999 Q4 Australian Capital Territory    25.8      60    69.1\n 9 2000 Q1 Australian Capital Territory    27.3      60.9  69.7\n10 2000 Q2 Australian Capital Territory    30.1      64.7  70.2\n# ℹ 582 more rows\n\n\nNow we have a tsibble that’s ready to use! In the first row of the output you should now see [1Q] indicating that the data is quarterly. You can also see the second row shows us our key variable, State. Next to this is [8], which tells us that this dataset contains 8 time series (one for each of Australia’s states).\n\n\n\n\n\n\nPipes\n\n\n\nWhen chaining together multiple functions, it’s helpful to use the pipe operator (|>).\nThe pipe allows you to read the functions in the order that they are used - much like a sentence!\nMore information is here: https://r4ds.hadley.nz/workflow-style.html#sec-pipes\n\n\nThat’s all you need to know about creating a tidy time series tsibble 🌈.\n\n\n\n\n\n\nYour turn!\n\n\n\nCreate a tsibble for the number of tourists visiting Australia contained in data/tourism.csv.\nSome starter code has been provided for you in the day 1 exercises.\nHint: this dataset contains multiple key variables that need to be used together. You can specify multiple keys with as_tsibble(key = c(a, b, c))."
  },
  {
    "objectID": "sessions/day1/exercises.html#manipulating-time-series",
    "href": "sessions/day1/exercises.html#manipulating-time-series",
    "title": "Exercises",
    "section": "Manipulating time series",
    "text": "Manipulating time series\nOften you want to work with specific series, or perhaps the sum up the values across multiple series. We can use the same dplyr functions that are used in data analysis to explore our time series. Let’s focus on a single state from the Australian accommodation example - here we use filter() to keep only the Queensland data.\n\naus_accommodation |> \n  filter(State == \"Queensland\")\n\n# A tsibble: 74 x 5 [1Q]\n# Key:       State [1]\n      Date State      Takings Occupancy   CPI\n     <qtr> <chr>        <dbl>     <dbl> <dbl>\n 1 1998 Q1 Queensland    230.      54    67  \n 2 1998 Q2 Queensland    219.      54    67.4\n 3 1998 Q3 Queensland    268.      64    67.5\n 4 1998 Q4 Queensland    279.      61    67.8\n 5 1999 Q1 Queensland    241.      55    67.8\n 6 1999 Q2 Queensland    235.      56    68.1\n 7 1999 Q3 Queensland    286.      65    68.7\n 8 1999 Q4 Queensland    288.      61    69.1\n 9 2000 Q1 Queensland    253.      54.7  69.7\n10 2000 Q2 Queensland    253.      56.5  70.2\n# ℹ 64 more rows\n\n\nMaybe we wanted to focus on the more recent data, only keeping observations after 2010. Note that multiple conditions (both time and place) can be included inside a single filter() function.\n\naus_accommodation |> \n  filter(State == \"Queensland\", Date >= yearquarter(\"2010 Q1\"))\n\n# A tsibble: 26 x 5 [1Q]\n# Key:       State [1]\n      Date State      Takings Occupancy   CPI\n     <qtr> <chr>        <dbl>     <dbl> <dbl>\n 1 2010 Q1 Queensland    464.      57.4  95.2\n 2 2010 Q2 Queensland    461.      58.5  95.8\n 3 2010 Q3 Queensland    573.      68.9  96.5\n 4 2010 Q4 Queensland    562.      64.8  96.9\n 5 2011 Q1 Queensland    471.      58.1  98.3\n 6 2011 Q2 Queensland    489.      61    99.2\n 7 2011 Q3 Queensland    592.      70.5  99.8\n 8 2011 Q4 Queensland    587.      66.9  99.8\n 9 2012 Q1 Queensland    530.      62.3  99.9\n10 2012 Q2 Queensland    519.      62.6 100. \n# ℹ 16 more rows\n\n\nLet’s try seeing the total accommodation Takings and Occupancy for all of Australia. For this, we can use the summarise() function to summarise information across multiple rows.\n\naus_accommodation |> \n  summarise(Takings = sum(Takings), Occupancy = sum(Occupancy))\n\n# A tsibble: 74 x 3 [1Q]\n      Date Takings Occupancy\n     <qtr>   <dbl>     <dbl>\n 1 1998 Q1    949.      469 \n 2 1998 Q2    875.      431 \n 3 1998 Q3    981.      458 \n 4 1998 Q4   1036.      468 \n 5 1999 Q1    997.      460 \n 6 1999 Q2    940.      447 \n 7 1999 Q3   1062.      481 \n 8 1999 Q4   1105.      474 \n 9 2000 Q1   1088.      465.\n10 2000 Q2   1039.      460.\n# ℹ 64 more rows\n\n\n\n\n\n\n\n\nThe index and summarise()\n\n\n\nWe still have our Date variable as it is automatically grouped when working with tsibble.\n\n\nWhat about calculating the annual takings, not quarterly? For this we use a special grouping function called index_by().\n\nlibrary(lubridate)\naus_accommodation |> \n  index_by(Year = year(Date)) |> \n  summarise(Takings = sum(Takings), Occupancy = sum(Occupancy))\n\n# A tsibble: 19 x 3 [1Y]\n    Year Takings Occupancy\n   <dbl>   <dbl>     <dbl>\n 1  1998   3841.     1826 \n 2  1999   4104.     1862 \n 3  2000   4725.     1834.\n 4  2001   4766.     1819.\n 5  2002   4865.     1848 \n 6  2003   5277.     1887.\n 7  2004   5675.     1950.\n 8  2005   6189.     1996.\n 9  2006   6783.     2054.\n10  2007   7443.     2107.\n11  2008   7897.     2074.\n12  2009   7629.     2024.\n13  2010   8088.     2081 \n14  2011   8534.     2089.\n15  2012   8965.     2088 \n16  2013   8992.     2048.\n17  2014   9477.     2031.\n18  2015  10242.     2069.\n19  2016   5080.     1034.\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nUsing the tourism dataset, create an annual time series of the Purpose of travel for visitors to Australia (summing over State and Region)\nSome starter code has been provided for you in the day 1 exercises.\nHint: think about which key variables should be kept with group_by(), and how the index should be changed using index_by() then summarise().\n\n\nWhat if we didn’t want a time series at all? To calculate the total takings over all of time, we convert back to an ordinary data frame with as_tibble() and then summarise().\n\nlibrary(lubridate)\naus_accommodation |> \n  as_tibble() |> \n  summarise(Takings = sum(Takings), Occupancy = sum(Occupancy))\n\n# A tibble: 1 × 2\n  Takings Occupancy\n    <dbl>     <dbl>\n1 128571.    36720.\n\n\nWhich state has had the most accommodation takings in 2010? Let’s calculate total takings by state for 2010, and sort them with arrange().\n\naus_accommodation |> \n  filter(year(Date) == 2010) |> \n  as_tibble() |> \n  group_by(State) |> \n  summarise(Takings = sum(Takings), Occupancy = sum(Occupancy)) |> \n  arrange(desc(Takings))\n\n# A tibble: 8 × 3\n  State                        Takings Occupancy\n  <chr>                          <dbl>     <dbl>\n1 New South Wales                2595.      259.\n2 Queensland                     2061.      250.\n3 Victoria                       1517.      258.\n4 Western Australia               849.      259.\n5 South Australia                 381.      252.\n6 Northern Territory              265.      262.\n7 Australian Capital Territory    227.      304.\n8 Tasmania                        193.      238.\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nUsing the tourism dataset, which Purpose of travel is most common in each state?\nSome starter code has been provided for you in the day 1 exercises.\nHint: since you no longer want to consider changes over time, you’ll need to convert the data back to a tibble."
  },
  {
    "objectID": "sessions/day1/exercises.html#visualising-time-series",
    "href": "sessions/day1/exercises.html#visualising-time-series",
    "title": "Exercises",
    "section": "Visualising time series",
    "text": "Visualising time series\nThere are a few common visualisation techniques specific to time series, however cross-sectional graphics also work well for time series data. The main difference is that we like to maintain the ordered and connected nature of time.\n\nTime plots\nThe simplest graphic for time series is the time series plot, which shows the variable of interest (on the y-axis) against time (on the x-axis). This plot can be created manually with ggplot2, or automatically plotted from the tsibble with autoplot().\n\nlibrary(fable)\nlibrary(ggplot2)\nguinea_rice |> \n  autoplot(Production)\n\n\n\n\nIn this plot we can see that Production increases over time (known as trend). The increase is mostly smooth but there are a couple anomalies in 2001 and 2008.\n\n\n\n\n\n\nPlotting the time variable\n\n\n\nIn this plot, Production and Year are two continuous variables. We would often like to plot two continuous variables with a scatter plot, however in time-series we prefer to connect the observations from one year to the next to give this line chart.\n\n\nWe can also use autoplot() to produce a time plot of many series, but be careful not to plot too many lines at once!\n\naus_accommodation |> \n  autoplot(Takings)\n\n\n\n\nIn this plot of Australian accommodation takings, we see that most states have increasing takings over time (upward trend). We can also notice a repeating up and down pattern, which upon closer inspection repeats every year. This repeating annual pattern is known as seasonality, and we can see that some states are more seasonal than others.\nLet’s focus on the sunny holiday destination of Queensland, and use different plots to better understand the seasonality.\n\naus_accommodation |> \n  filter(State == \"Queensland\") |> \n  autoplot(Takings)\n\n\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nUsing the tourism dataset, create time plots of the data. Which patterns can you observe?\nSome starter code has been provided for you in the day 1 exercises.\nHint: there are too many series to show in a single plot, so filter and summarise series of interest to you.\n\n\n\n\nSeasonal plots\nIt can be tricky to see which quarter has maximum accommodation takings from a time plot. Instead, it is better to use a seasonal plot with gg_season() from feasts.\n\nlibrary(feasts)\naus_accommodation |> \n  filter(State == \"Queensland\") |> \n  gg_season(Takings)\n\n\n\n\nHere we can see that the Q3 and Q4 takings are higher than Q1 and Q2, this is known as the seasonal peak and trough respectively.\n\n\n\n\n\n\nThe season plot\n\n\n\nThe seasonal plot is very similar to the time plot, but the x-axis now wraps over years. This allows us to more easily compare the years and find common patterns, like which month or quarter is biggest and smallest.\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nUsing the tourism dataset, create a seasonal plot for the total holiday travel to Australia over time. In which quarter is holiday travel highest and lowest?\nSome starter code has been provided for you in the day 1 exercises.\n\n\n\n\nSeasonal subseries plot\nAnother useful plot to understand the seasonal pattern of a time series is the subseries plot, it can be created with gg_subseries(). This plot is splits each month / quarter into separate facets (mini-plots), which shows how the values within each season change over time. The blue lines represent the average, which is a useful way to see the overall seasonality at a glance.\n\naus_accommodation |> \n  filter(State == \"Queensland\") |> \n  gg_subseries(Takings)\n\n\n\n\n\n\n\n\n\n\nSeasonal sub-series plots\n\n\n\nThe upward lines in each facet of this plot shows the trend of the data, however if the lines went in different directions that would imply the shape of the seasonality is changing over time.\nSeasonal plots work best after removing trend, which we will see how to do tomorrow!\n\n\nLet’s see this plot with a different dataset, recent beer production in Australia.\n\naus_beer <- tsibbledata::aus_production |> \n  filter(Quarter >= yearquarter(\"1992 Q1\")) |> \n  select(Beer)\naus_beer |> \n  autoplot(Beer)\n\n\n\n\nAt a glance, this looks like the it is very seasonal and has a slight downward trend. However the seasonal subseries plot reveals that the trend is misleading!\n\naus_beer |> \n  gg_subseries(Beer)\n\n\n\n\nHere we see that only Q4 (the peak) has a downward trend, while the other quarters are staying roughly the same. The seasonality is changing shape over time.\n\n\n\n\n\n\nChanging seasonality\n\n\n\nLook back at the time plot and focus only on the Q4 peaks, can you see these values decreasing over time? Now look at the Q1-Q3 throughs, how do they change over time?\nThis can be tricky to notice in the time plot, which is why seasonal subseries plots can be particularly helpful!\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nUsing the tourism dataset, create a seasonal subseries plot for the total business travel to Victoria over time. Does the seasonal pattern change over time?\nSome starter code has been provided for you in the day 1 exercises.\n\n\n\n\nACF plots\nThese plots may look a bit strange at first, but they are very useful for seeing all of the time series dynamics in a single plot. ACF is the ‘auto-correlation function’, essentially a measure of how similar a time series is to the lags of itself. Looking at these correlations can reveal trends, seasonality, cycles, and more subtle patterns. You can create an ACF plot using a combination of ACF() and autoplot().\n\nguinea_rice |> \n  ACF(Production) |> \n  autoplot()\n\n\n\n\nThe rice production of Guinea has an upward trend, which produces a gradual decay in the ACF.\n\naus_beer |> \n  ACF(Beer) |> \n  autoplot()\n\n\n\n\nThe recent beer production of Australia has lots of seasonality and no trend, which creates large peaks at the seasonal lags in the ACF. Every 4 quarters we see a large ACF spike.\n\naus_accommodation |> \n  summarise(Occupancy = sum(Occupancy)) |> \n  ACF(Occupancy) |> \n  autoplot()\n\n\n\n\nThe total occupancy of Australia’s short-term accommodation is both trended and seasonal, which results in a slowly decaying ACF with peaks every seasonal lag (4, 8, 12, …).\nConsider the number of Snowshoe Hares which were traded by the Hudson Bay Company.\n\ntsibbledata::pelt |> \n  autoplot(Hare)\n\n\n\n\nTo the untrained eye, this series has lots of up and down patterns - a bit like seasonality. However this pattern is cyclical, not seasonal. The ACF plot can help us distinguish cycles from seasonality.\n\n\n\n\n\n\nSeasonal or cyclic?\n\n\n\nSeasonality is a consistent repeating pattern, where the shape shape with similar peak and trough repeats at the same time interval.\nCyclical patterns are less consistent, with varying peaks and troughs that repeats over a varied time period.\n\n\nLet’s see the ACF for this dataset\n\ntsibbledata::pelt |> \n  ACF(Hare) |> \n  autoplot()\n\n\n\n\nNotice that the peak at lag 10 is less symmetric and ‘sharp’, this is because the pattern usually repeats every 10 years but sometimes 9 or 11. This is unlike seasonality, which has a sharper peak in the ACF due to the consistent time period between patterns.\n\n\n\n\n\n\nYour turn!\n\n\n\nIdentify which ACF matches the time plots in the following figures by identifying the patterns of trend, seasonality, and cycles in the ACF plots.\n\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nUsing the tourism dataset, create an ACF plot for the total travel to Australia over time. Can you identify patterns of trend and seasonality from this plot?\nSome starter code has been provided for you in the day 1 exercises.\n\n\n\n\n\n\n\n\nACF model evaluation\n\n\n\nImportantly, ACF plots can also tell us when there are no patterns/autocorrelations in the data (white noise).\nWe’ll be revisiting this plot to evaluate our models on day 5. We hope that a model uses all available information, and ACF plots can show if there is any patterns left over."
  },
  {
    "objectID": "sessions/day1/exercises.html#about-the-dataset",
    "href": "sessions/day1/exercises.html#about-the-dataset",
    "title": "Exercises",
    "section": "About the dataset",
    "text": "About the dataset\nIn this exercise, we use a dataset containing dose of BCG (Cacille Calmette-Guérin), vaccine administrated in 9 regions of an African country from January 2013 untill December 2021. BCG is a widely administered vaccine primarily used to protect against tuberculosis (TB), a serious infection that primarily affects the lungs but can also affect other parts of the body. BCG vaccination is recommended for newborn babies at risk of tuberculosis (TB) and is typically administered shortly after birth, usually within the first 28 days of life.\nIn addition to the administered dose, it also includes data on the population of children under one year old, whether a strike occurred in a specific month and region, and whether a region is affected by flooding events.\nIn this exercise, you will apply what you have learned so far on a dataset containing doses of administered vaccine in an African country."
  },
  {
    "objectID": "sessions/day1/exercises.html#about-the-dataset-1",
    "href": "sessions/day1/exercises.html#about-the-dataset-1",
    "title": "Exercises",
    "section": "About the dataset",
    "text": "About the dataset\nIn this exercise, we use a dataset containing dose of BCG (Cacille Calmette-Guérin), vaccine administrated in 9 regions of an African country from January 2013 until December 2021. BCG is a widely administered vaccine primarily used to protect against tuberculosis (TB), a serious infection that primarily affects the lungs but can also affect other parts of the body. BCG vaccination is recommended for newborn babies at risk of tuberculosis (TB) and is typically administered shortly after birth, usually within the first 28 days of life.\nIn addition to the administered dose, it also includes data on the population of children under one year old, and whether a strike occurred in a specific month and region.\nIn this exercise, you will apply what you have learned about different steps in the forecasting workflow on this dataset.\n\n\n\n\n\n\nYour turn!\n\n\n\n\nImport vaccine_adminstrated.csv data into R\n\nCheck and modify the data types of variables as needed\n\nPrepare your data\n\nCheck and fix missing values\nCheck duplications and fix it\nCreate tsibble\nCheck and fix temporal gaps\n\nManipulating time series\n\nCreate monthly time series of total doses adminstrated in the country\nCreate quarterly time series of doses adminstrated in each region\nCreate quarterly time series of total doses adminstrated in the country\n\nVisualizing time series\n\nUse time plots and describe what patterns you observe\nCreate plots to see if any consistent pattern exsists in monthly and quarterly of dose admisntrated\nCreate plots to see how dose admisntrated chnage over time for each month/quarter and how it differs across differnt month/quarter"
  },
  {
    "objectID": "sessions/day2/exercises.html",
    "href": "sessions/day2/exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Transformations provide useful simplifications of the patterns in a time series. Simplifying the patterns makes them easier to model, and so transforming the data is a common preliminary step in producing forecasts. Some transformations standardise values to be comparable between countries or other series in the dataset, while others can regularise the variation in the data.\nLet’s look at the turnover of print media in Australia.\n\nlibrary(tsibbledata)\nlibrary(fable)\n\nLoading required package: fabletools\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\naus_print <- aus_retail |> \n  filter(Industry == \"Newspaper and book retailing\") |> \n  summarise(Turnover = sum(Turnover))\naus_print |> \n  autoplot(Turnover)\n\n\n\n\nTurnover has increased until the end of 2010, after which it has steadily declined. When looking at monetary value it is common to consider price indices to ensure that turnover is comparable over time. This allows you to identify patterns and changes such as turning points in real monetary terms.\n\n\n\n\n\n\nData for transformations\n\n\n\nIt can be useful to use other datasets that contain information for the transformation. To do this we can merge the datasets in time using join operations.\n\n\n\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\naus_economy <- global_economy |> \n  filter(Country == \"Australia\")\naus_print |> \n  mutate(Year = year(Month)) |> \n  left_join(aus_economy, by = \"Year\") |> \n  autoplot(Turnover/CPI)\n\nWarning: Removed 12 rows containing missing values (`geom_line()`).\n\n\n\n\n\nAfter taking into account CPI, the real monetary Turnover of the print media industry in Australia has been gradually declining since 1990-2000.\n\n\n\n\n\n\nYour turn!\n\n\n\nSelect a country of your choice from global_economy, then calculate and visualise the the GDP per capita over time (that is, the GDP scaled by the population).\n\n\n\n\n\n\n\n\nTricky to forecast\n\n\n\nWhile transformations help to make the patterns simpler to forecast, if additional information like CPI or Population are used then they will also need to be forecasted. While population is generally easy to forecast, CPI could be more complicated to forecast than the thing you’re originally forecasting!\n\n\nAnother useful transformation is calendar adjustments. This adjusts the observations in the time series to represent an equivalent time period, and can simplify seasonal patterns which result from these different lengths. This is particularly useful for monthly data, since the number of days in each month varies substantially.\n\nlibrary(feasts)\naus_print |> \n  autoplot(Turnover / days_in_month(Month))\n\n\n\naus_print |> \n  gg_subseries(Turnover / days_in_month(Month))\n\n\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nCalculate the monthly total Australian retail turnover from aus_retail and visualise the seasonal pattern. Then scale by the number of days in each month to calculate the daily average turnover and comparse the seasonal patterns.\n\n\nMathematical transformations are useful since they don’t require providing any future values to produce the forecasts. Log and power transformations (\\(y^k\\), for example square root, square, and inverse) are particularly helpful for regularising variation proportional to the level of the series.\n\naus_retail |> \n  filter(State == \"Victoria\", Industry == \"Cafes, restaurants and catering services\") |> \n  autoplot(Turnover)\n\n\n\n\nThis proportional variance is common in time series, in Victoria’s cafe and restaurant turnover you can see small changes when turnover is low (before 2000), and is much larger after 2010 when turnover is much larger.\n\naus_retail |> \n  filter(State == \"Victoria\", Industry == \"Cafes, restaurants and catering services\") |> \n  autoplot(log(Turnover))\n\n\n\n\nLog transforming the data changes this variation to be more consistent, where the variation before 2000 is now more similar to after 2010.\nThe box-cox transformation family parameterises the range of power transformations for more precise adjustments. The transformation parameter \\(\\lambda\\) controls the strength of the transformation, with \\(\\lambda=1\\) being no change in shape, \\(\\lambda = 0\\) being a log transformation and others being equivalent in shape to \\(y^\\lambda\\).\nThe log transformation above was a bit strong, so let’s try something slightly close to \\(\\lambda=1\\) - perhaps \\(\\lambda = 0.1\\)?\n\naus_retail |> \n  filter(State == \"Victoria\", Industry == \"Cafes, restaurants and catering services\") |> \n  autoplot(box_cox(Turnover, lambda = 0.1))\n\n\n\n\nThe variation is now consistent for the entire series, and the trend is linear.\n\n\n\n\n\n\nAutomatic box-cox transformations\n\n\n\nThe \\(\\lambda\\) parameter can be automatically computed using the guerrero() function.\n\n\nWe can calculate the optimal box-cox parameter using features() and guerrero():\n\naus_retail |> \n  filter(State == \"Victoria\", Industry == \"Cafes, restaurants and catering services\") |> \n  features(Turnover, features = guerrero)\n\n# A tibble: 1 × 3\n  State    Industry                                 lambda_guerrero\n  <chr>    <chr>                                              <dbl>\n1 Victoria Cafes, restaurants and catering services           0.173\n\n\nLooks like we were pretty close with \\(\\lambda = 0.1\\), let’s try using this more precise estimate:\n\naus_retail |> \n  filter(State == \"Victoria\", Industry == \"Cafes, restaurants and catering services\") |> \n  autoplot(box_cox(Turnover, lambda = guerrero(Turnover)))\n\n\n\n\nThe optimised box-cox transformation is very similar to \\(\\lambda = 0.1\\) - fortunately for us we don’t have to be precise since this transformation isn’t sensitive to your choice of parameter. So long as you are within \\(\\pm 0.1\\) the transformation should be okay. Additionally, if \\(\\lambda \\approx 0\\) then it is common to instead use the simpler log() transformation.\n\n\n\n\n\n\nYour turn!\n\n\n\nFind a suitable box-cox transformation for the monthly total Australian retail turnover, then compare your choice with the automatically selected parameter from the guerrero() feature.\n\n\n\n\n\nAnother commonly used transformation/adjustment requires a model to decompose the time series into its components. Seasonally adjusted time series are often used by analysts and policy makers to evaluate the underlying long term trends without the added complexity of seasonality. The STL decomposition is useful model which can isolate the seasonal pattern from the trend and remainder for many types of time series.\nThe STL decomposition separates time series into the form \\(Y = \\text{trend} + \\text{seasonality} + \\text{remainder}\\). Since this is an additive decomposition, we must first simplify any multiplicative patterns into additive ones using a suitable power transformation. Let’s try to remove the annual seasonality from Australia’s print media turnover.\n\naus_print |> \n  autoplot(Turnover)\n\n\n\n\nThe seasonality is more varied when turnover increases, so we must transform the data before estimating the STL model.\n\naus_print |> \n  autoplot(log(Turnover))\n\n\n\n\nThe log transformation (\\(\\lambda = 0\\)) does a great job at producing a consistent variation throughout the series. You could try to find a better transformation using the box-cox transformation family, however there is no need for it here.\nWe can estimate the STL model using STL() as follows:\n\nfit <- aus_print |> \n  model(STL(log(Turnover)))\nfit\n\n# A mable: 1 x 1\n  `STL(log(Turnover))`\n               <model>\n1                <STL>\n\n\nThe decomposition can be obtained from the model using components(), and then all of the components can be plotted with autoplot():\n\nfit |> \n  components() \n\n# A dable: 441 x 7 [1M]\n# Key:     .model [1]\n# :        log(Turnover) = trend + season_year + remainder\n   .model        Month `log(Turnover)` trend season_year remainder season_adjust\n   <chr>         <mth>           <dbl> <dbl>       <dbl>     <dbl>         <dbl>\n 1 STL(log(T… 1982 Apr            4.90  4.92     -0.0723   0.0528           4.97\n 2 STL(log(T… 1982 May            4.89  4.92     -0.0128  -0.0151           4.91\n 3 STL(log(T… 1982 Jun            4.84  4.93     -0.0942   0.00869          4.94\n 4 STL(log(T… 1982 Jul            4.86  4.93     -0.0480  -0.0303           4.90\n 5 STL(log(T… 1982 Aug            4.88  4.94     -0.0155  -0.0458           4.89\n 6 STL(log(T… 1982 Sep            4.90  4.94     -0.0453  -0.00317          4.94\n 7 STL(log(T… 1982 Oct            4.93  4.95     -0.0167   0.00114          4.95\n 8 STL(log(T… 1982 Nov            4.97  4.95      0.0181  -0.00245          4.95\n 9 STL(log(T… 1982 Dec            5.26  4.96      0.258    0.0398           5.00\n10 STL(log(T… 1983 Jan            4.92  4.97     -0.0180  -0.0249           4.94\n# ℹ 431 more rows\n\nfit |> \n  components() |> \n  autoplot()\n\n\n\n\nThe components are obtained using rolling estimation windows, which are the main way the decomposition is changed. A large window produces smooth components, and a small window produces flexible and quickly changing components.\n\naus_print |> \n  model(STL(log(Turnover) ~ trend(window = 5) + season(window = Inf))) |> \n  components() |> \n  autoplot()\n\n\n\n\nThe infinite window for the seasonality results in a seasonal pattern that doesn’t change over time, while the small trend window allows the trend to change very quickly. The best choice of estimation window should produce components that match the patterns in the original data while being as smooth as possible.\n\nfit <- aus_print |> \n  model(STL(log(Turnover) ~ trend(window = 25) + season(window = Inf))) \nfit |> \n  components() |> \n  autoplot()\n\n\n\n\nA trend window of 25 for this dataset produces a mostly smooth trend component which can still react to brief decreases in turnover. The constant seasonal pattern (infinite window) is reasonable for this dataset since the seasonality doesn’t change much over time.\n\n\n\n\n\n\nSeasonal adjustment\n\n\n\nYou can find the de-seasonalised data in the season_adjust column of the components() output.\n\n\n\nfit |> \n  components() |> \n  autoplot(season_adjust)\n\n\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nFind a suitable STL decomposition for the total Australian retail turnover, then produce and visualise the seasonally adjusted time series.\nHint: don’t forget to use the suitable transformation found previously!\n\n\nSeasonal decomposition also makes it easier to take a look at the seasonality - we can use a combination of seasonal plots and decomposition to more easily see seasonal patterns.\n\nfit |> \n  components() |> \n  gg_season(season_year)\n\n\n\n\nIf the seasonal window allows the seasonal component to change over time, the gg_subseries() plot is especially useful for seeing how the pattern changes.\n\naus_print |> \n  model(STL(log(Turnover) ~ trend(window = 25) + season(window = 9))) |> \n  components() |> \n  gg_subseries(season_year)\n\n\n\n\nJanuary and December seem to increase over time, while April, May and June are decreasing.\n\n\n\n\n\n\nYour turn!\n\n\n\nProduce appropriate seasonal plots of the seasonal component from your STL decomposition on Australian retail turnover.\n\n\n\n\n\nA useful technique for visualising large collections of time series is to produce summaries of their patterns known as features. Visualising many time series simultaneously is difficult since the scale and shape of patterns can vary substantially. The features() function will compute single value summaries over time such as the strength of trend or seasonality. There are many features available, but the features from STL decompositions are particularly interesting.\n\naus_retail |> \n  features(Turnover, feat_stl)\n\n# A tibble: 152 × 11\n   State       Industry trend_strength seasonal_strength_year seasonal_peak_year\n   <chr>       <chr>             <dbl>                  <dbl>              <dbl>\n 1 Australian… Cafes, …          0.989                  0.562                  0\n 2 Australian… Cafes, …          0.993                  0.629                  0\n 3 Australian… Clothin…          0.991                  0.923                  9\n 4 Australian… Clothin…          0.993                  0.957                  9\n 5 Australian… Departm…          0.977                  0.980                  9\n 6 Australian… Electri…          0.992                  0.933                  9\n 7 Australian… Food re…          0.999                  0.890                  9\n 8 Australian… Footwea…          0.982                  0.944                  9\n 9 Australian… Furnitu…          0.981                  0.687                  9\n10 Australian… Hardwar…          0.992                  0.900                  9\n# ℹ 142 more rows\n# ℹ 6 more variables: seasonal_trough_year <dbl>, spikiness <dbl>,\n#   linearity <dbl>, curvature <dbl>, stl_e_acf1 <dbl>, stl_e_acf10 <dbl>\n\n\nIn particular, features from STL decompositions allow you to compare the strength of trend and seasonality between many time series.\n\nlibrary(ggplot2)\naus_retail |> \n  features(Turnover, feat_stl) |> \n  ggplot(aes(x = trend_strength, y = seasonal_strength_year)) + \n  geom_point()\n\n\n\n\nFrom this we can see that almost all time series have a strong trend, while the strength of seasonality is more varied - some series have strong seasonality while others have less seasonality.\n\n\n\n\n\n\nYour turn!\n\n\n\nCalculate the STL features for the time series in the tourism dataset. Try colouring the points in the scatterplot by the purpose of travel, are some reasons more trended or seasonal than others?\n\n\nThere are many other features that you can use - check out the documentation for ?features_by_pkg. You can produce a feature set of similar features using the feature_set() function.\n\naus_retail |> \n  features(Turnover, feature_set(tags = \"autocorrelation\"))\n\n# A tibble: 152 × 13\n   State      Industry  acf1 acf10 diff1_acf1 diff1_acf10 diff2_acf1 diff2_acf10\n   <chr>      <chr>    <dbl> <dbl>      <dbl>       <dbl>      <dbl>       <dbl>\n 1 Australia… Cafes, … 0.973  8.59     -0.348       0.239     -0.572       0.502\n 2 Australia… Cafes, … 0.977  8.65     -0.327       0.259     -0.552       0.531\n 3 Australia… Clothin… 0.885  7.01     -0.276       0.251     -0.507       0.339\n 4 Australia… Clothin… 0.846  6.33     -0.303       0.201     -0.532       0.321\n 5 Australia… Departm… 0.500  1.60     -0.310       0.202     -0.540       0.315\n 6 Australia… Electri… 0.902  7.29     -0.247       0.324     -0.506       0.455\n 7 Australia… Food re… 0.984  9.13     -0.394       0.585     -0.611       1.12 \n 8 Australia… Footwea… 0.760  4.64     -0.325       0.155     -0.566       0.333\n 9 Australia… Furnitu… 0.952  7.67     -0.190       0.163     -0.530       0.394\n10 Australia… Hardwar… 0.957  7.67     -0.104       0.101     -0.497       0.311\n# ℹ 142 more rows\n# ℹ 5 more variables: season_acf1 <dbl>, pacf5 <dbl>, diff1_pacf5 <dbl>,\n#   diff2_pacf5 <dbl>, season_pacf <dbl>"
  },
  {
    "objectID": "sessions/day3/exercises.html",
    "href": "sessions/day3/exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Learn\nMitch’s section to learn the concepts on various datasets\n\n\nApply\nIn this exercise, we first use simple models to produce forecasts of future administered vaccine doses for the next 12 months. Following that, we use regression models to produce such a forecast.\n\n\n\n\n\n\nBasic of modelling/forecating\n\n\n\n\nSpecify and train three simple models including total average, naive and seasonal naive on administered vaccine doses.\nExamine the model table (mable) object and describe what each column and row represent.\nUse report(), tidy(), glance() and augment() to explore the trained model’s output.\nProduce forecasts for 12 months ahead including both point forecast and forecast distribution.\nExamine the forecast table (fable) object and explain what each column and row represent.\nVisualize the point forecasts alongside past values, as well as prediction interval for \\(90%\\) coverage.\nExtract prediction intervals for \\(90%\\) coverage.\nProduce probabilistic forecast using bootstrapping instead of assuming normal distribution. Generate 1000 possible future.\n\n\n\n\n\n\n\n\n\nForecating using regression\n\n\n\n\nExamine the association between dose_adminstrated and predictors\n\nAssess the association between dose_adminstrated and population_under1\nAssess the association between dose_adminstrated and strike\nExamine the association between leading predictors of population_under1 and dose_adminstrated\n\nSpecify and train the four different regression models with the following terms:\n\ntrensd and seasonality\ntrensd, seasonality, and population_under1\ntrensd, seasonality, population_under1, and strike\n\nExamine trained model output using report(), tidy(), and glance() and augment()\nProduce forecast\n\nUse new_data() to generate future months corresponding to forecast horizon\nAdd future values for the strike\nAdd future values for the population_under1\nGenerate forecasts for future periods\n\nVisualize forecasts"
  },
  {
    "objectID": "sessions/day4/exercises.html",
    "href": "sessions/day4/exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Learn\nMitch’s section to learn the concepts on various datasets\n\n\nApply\nIn this exercise, you use exponetial smoothing and ARIMA models to produce forecasts of future administered vaccine doses for the next 12 months. :::{.callout-caution} ## ETS 1. Specify and train automatic Exponential Smoothing model\n\nObserve the model table and explain what each row and column represent\nExtract ETS model’s output using report(), tidy(), glance()\nExtract the components of the selected ETS model\nGenerate forecasts using ETS and observe the forecast table\nVisualize forecasts\nUse the specific functions to determine manually the components and parameters of ETS models\n\n:::\n\n\n\n\n\n\nARIMA\n\n\n\n\nSpecify and train automatic ARIMA model\nObserve the model table and explain what each row and column represent\nExtract ARIMA model’s output using report(), tidy(), glance()\nGenerate forecasts using ARIMA\nVisualize forecasts\nDetermine model components manually"
  },
  {
    "objectID": "sessions/day5/exercises.html",
    "href": "sessions/day5/exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Learn\nMitch’s section to learn the concepts on various datasets\n\n\nApply\nIn this part, we evaluate the forecast accuracy of all models we have covered so far using a simple train/test split and time series cross validation.\n\n\n\n\n\n\nBasic of train/test forecast accuracy\n\n\n\n\nSplit the data into train and test ensuring the number of months in the test set equals the forecast horizon\nSpecify and train the following models on the train data:\n\nAverage\nNaive\nSeasonal Naive\nETS\nARIMA\nRegression with trend and seasonality\nRegression with trend, seasonality, and population_under1\nRegression with trend, seasonality, population_under1, and strike\nCombination of ETS and ARIMA and regression with population and strike\n\nProduce forecasts\n\nReplace the values of population in the test set with its estimation\nProduce forecasts for dose adminstrated\n\nCompute forecast accuracy including point forecast accuracy, prediction interval and probabilistic forecasts\nVisualise the forecasts\n\n\n\n\n\n\n\n\n\nAdvanced performance evaluation\n\n\n\n\nTime series cross validation\n\nSplit the data into test and train\n\nthe size of test set equals the \\(20% (30%)\\) of the length of the time series\nthe size of test set equals the \\(80% (70%)\\) of the length of the time series\n\nApply time series cross-validation technique to create different time series rolling origins for both the train and test set\nReplace the values of population in the cross-validated test set with its estimations\nSpecify and train the following models on the cross-validated train dataset:\n\nAverage\nNaive\nSeasonal Naive\nETS\nARIMA\nRegression with trend and seasonality\nRegression with trend, seasonality, and population_under1\nRegression with trend, seasonality, population_under1, and strike\nCombination of ETS and ARIMA and regression with population and strike\n\nProduce forecasts\nCompute forecast accuracy including point forecast accuracy, prediction interval and probabilistic forecasts\n\nCompute total average forecast accuracy across all orinigs and horizons\nCompute and visualise forecast accuracy across all horizons for each origin\nCompute average forecast accuracy across all orinigs for each horizon\n\nSpecify, train and forecast using the most accurate model and visualise forecast\n\n\n\nResidual diagnostics\n\nExtract residuals from the model table from the most accurate model\nProduce the time plot of residuals from the most accurate model\nCreate the histogram of residuals from the most accurate model\nProduce the ACF plot of of residuals from the most accurate model"
  },
  {
    "objectID": "sessions/template.html",
    "href": "sessions/template.html",
    "title": "{{title}}",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "sessions/template.html#lab-sessions",
    "href": "sessions/template.html#lab-sessions",
    "title": "{{title}}",
    "section": "Lab sessions",
    "text": "Lab sessions"
  }
]