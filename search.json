[
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "Tidy time series & forecasting in R",
    "section": "Course Overview",
    "text": "Course Overview\nForecasting is a valuable tool that allows organizations to make informed decisions about the future. Time series forecasting, in particular, uses historical data to predict future trends over time. This technique has extensive applications across a wide range of fields, including finance and economics, health and humanitarian operations, supply chain management, and more. By analyzing trends and patterns in data, time series forecasting can help decision-makers identify potential challenges and opportunities, and plan accordingly.\nIt is important for researchers in Low- and Middle-Income Countries (LMICs) to develop technical skill in data analysis and forecasting techniques, which are essential for accurate and reliable forecasting. By having these skills, researchers can analyze data, identify trends and patterns, and develop robust forecasting models to make informed decisions that can improve resource allocation and planning in LMICs. Additionally, researchers can collaborate with policy makers and stakeholders to ensure that the forecast results are integrated into decision-making processes, leading to more efficient and effective resource management strategies.\nThis workshop is part of the Forecasting for Social Good (F4SG) initiative, and will run online from the 23rd-27th October 2023."
  },
  {
    "objectID": "index.html#learning-objectives",
    "href": "index.html#learning-objectives",
    "title": "Tidy time series & forecasting in R",
    "section": "Learning objectives",
    "text": "Learning objectives\nDuring the training, participants will gain knowledge and skills in:\n\nPreparing time series data for analysis and exploration.\nExtracting and computing useful features from time series data and effectively visualizing it.\nIdentifying appropriate forecasting algorithms for time series and selecting the best approach for the data at hand."
  },
  {
    "objectID": "index.html#instructor",
    "href": "index.html#instructor",
    "title": "Tidy time series & forecasting in R",
    "section": "Instructor",
    "text": "Instructor\n Mitchell O’Hara-Wild (he/him) is a PhD student at Monash University, creating new techniques and tools for forecasting large collections of time series with Rob Hyndman and George Athanasopoulos. He is the lead developer of the tidy time-series forecasting tools fable and feasts, and has co-developed the widely used forecast package since 2015. Mitchell also operates a data consultancy, and has worked on many forecasting projects that have supported decision making and planning for businesses and governments. He is an award-winning educator, and has taught applied forecasting at Monash University and various forecasting workshops around the world."
  },
  {
    "objectID": "index.html#instructor-1",
    "href": "index.html#instructor-1",
    "title": "Tidy time series & forecasting in R",
    "section": "Instructor",
    "text": "Instructor\n\nBahman is a Reader (Associate Professor) in Data-Driven Decision Science at Cardiff Business School, Cardiff University, UK. He serves as the director of the Data Lab for Social Good Research Group at Cardiff University and is also the founder of the Forecasting for Social Good committee within the International Institute of Forecasters. Bahman specializes in the development and application of modelling, forecasting and management science tools and techniques providing informed insights for planning & decision-making processes in sectors contributing to social good, including healthcare operations, global health and humanitarian supply chains, agriculture and food, social sustainability, and governmental policy. His collaborative efforts have spanned a multitude of organisations, including notable bodies such as the National Health Service (NHS), Welsh Ambulance Service Trusts (WAST), United States Agency for International Developments (USAID), the International Committee of the Red Cross (ICRC), and John Snow Inc. (JSI). A remarkable highlight of his contributions is his pivotal role in disseminating forecasting knowledge especially in low and lower-middle income countries through the democratizing forecasting project sponsored by International Institute of Forecasters."
  },
  {
    "objectID": "index.html#mentors-for-the-cohort-2023",
    "href": "index.html#mentors-for-the-cohort-2023",
    "title": "Tidy time series & forecasting in R",
    "section": "Mentors for the cohort 2023",
    "text": "Mentors for the cohort 2023\nA committed team, comprising both PhD students and MSc. students, generously dedicates their time and expertise to offer valuable support to learners throughout the duration of the workshop to help learners with the excercises. The team includes:\n\nHarsha Halgamuwe Hewage\nJosephine Valensia\nKrisanat Anukarnsakulchularp\nLaiba Khan\nMandy Luon\nMingzhe Shi\nSneha Kharbanda\nZihao Wang"
  },
  {
    "objectID": "index.html#project-coordination",
    "href": "index.html#project-coordination",
    "title": "Tidy time series & forecasting in R",
    "section": "Project coordination",
    "text": "Project coordination\nThe coordination and administration of the project are overseen by a dedicated team from Jomo Kenyatta University of Agriculture and Technology. This includes tasks such as promoting the workshop across various countries, managing the intake of 138 applications from 13 different nations, conducting a thorough shortlisting process and selecting 62 participants, and maintaining effective communication with all attendees.\nJomo Kenyatta University team includes:\n\nHenry Kissinger Ochieng\nCaroline Mugo\nWinnie Chacha\nSamuel Mwalili"
  },
  {
    "objectID": "index.html#required-equipment",
    "href": "index.html#required-equipment",
    "title": "Tidy time series & forecasting in R",
    "section": "Required equipment",
    "text": "Required equipment\nPlease have your own laptop capable of running R."
  },
  {
    "objectID": "index.html#required-software",
    "href": "index.html#required-software",
    "title": "Tidy time series & forecasting in R",
    "section": "Required software",
    "text": "Required software\nTo be able to complete the exercises of this workshop, please install a suitable IDE (such as RStudio), a recent version of R (4.1+) and the following packages.\n\nTime series packages and extensions\n\nfpp3, sugrrants\n\ntidyverse packages and friends\n\ntidyverse, fpp3\n\n\nThe following code will install the main packages needed for the workshop.\ninstall.packages(c(\"tidyverse\",\"fpp3\", \"GGally\", \"sugrrants\", \"astsa\"))\nPlease have the required software installed and pre-work completed before attending the workshop."
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "This work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Time\n\n\nSession\n\n\n\n\n\n\n08:00-09:30\n\n\nBasics of time series and data structures\n\n\n\n\n09:45-11:15\n\n\nTime series patterns and basic graphics\n\n\n\n\nCheck with your mentor on Slack\n\n\nExercises\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "schedule.html#day-1",
    "href": "schedule.html#day-1",
    "title": "Schedule",
    "section": "",
    "text": "Time\n\n\nSession\n\n\n\n\n\n\n08:00-09:30\n\n\nBasics of time series and data structures\n\n\n\n\n09:45-11:15\n\n\nTime series patterns and basic graphics\n\n\n\n\nCheck with your mentor on Slack\n\n\nExercises\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "schedule.html#day-2",
    "href": "schedule.html#day-2",
    "title": "Schedule",
    "section": "Day 2",
    "text": "Day 2\n\n\n\n\n\n\nTime\n\n\nSession\n\n\n\n\n\n\n08:00-09:30\n\n\nTransforming / adjusting time series\n\n\n\n\n09:45-11:15\n\n\nComputing and visualizing features\n\n\n\n\nCheck with your mentor on Slack\n\n\nExercises\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "schedule.html#day-3",
    "href": "schedule.html#day-3",
    "title": "Schedule",
    "section": "Day 3",
    "text": "Day 3\n\n\n\n\n\n\nTime\n\n\nSession\n\n\n\n\n\n\n08:00-09:30\n\n\nBasic modeling / forecasting\n\n\n\n\n09:45-11:15\n\n\nForecasting with regression, how to represent temporal structure with regressors\n\n\n\n\nCheck with your mentor on Slack\n\n\nExercises\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "schedule.html#day-4",
    "href": "schedule.html#day-4",
    "title": "Schedule",
    "section": "Day 4",
    "text": "Day 4\n\n\n\n\n\n\nTime\n\n\nSession\n\n\n\n\n\n\n08:00-09:30\n\n\nARIMA\n\n\n\n\n09:45-11:15\n\n\nETS\n\n\n\n\nCheck with your mentor on Slack\n\n\nExercises\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "schedule.html#day-5",
    "href": "schedule.html#day-5",
    "title": "Schedule",
    "section": "Day 5",
    "text": "Day 5\n\n\n\n\n\n\nTime\n\n\nSession\n\n\n\n\n\n\n08:00-09:30\n\n\nBasic training and test accuracy\n\n\n\n\n09:45-11:15\n\n\nResidual diagnostics and cross validation\n\n\n\n\nCheck with your mentor on Slack\n\n\nExercises\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "sessions/break.html",
    "href": "sessions/break.html",
    "title": "Coffee Break",
    "section": "",
    "text": "Time for a coffee break!\nFeel free to ask me some questions, or simply enjoy the break."
  },
  {
    "objectID": "sessions/day1/exercises.html",
    "href": "sessions/day1/exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "We’ve prepared an exercises project with some starter code for each of the sessions. You can download and open this project using:\n\nusethis::use_course(\"https://workshop.mitchelloharawild.com/f4sg-africa/exercises.zip\")"
  },
  {
    "objectID": "sessions/day1/exercises.html#creating-a-time-series-tibble-a-tsibble",
    "href": "sessions/day1/exercises.html#creating-a-time-series-tibble-a-tsibble",
    "title": "Exercises",
    "section": "Creating a time series tibble (a tsibble!)",
    "text": "Creating a time series tibble (a tsibble!)\nA tsibble is a rectangular data frame that contains:\n\na time column: the index\nidentifying column(s): the key variables\nvalues (the measured variables)\n\nYou usually create a tsibble by converting an existing dataset (read from a file) with as_tsibble(). For example, let’s look at the production of rice in Guinea.\n\n# Read in the dataset using readr\nlibrary(readr)\nguinea_rice &lt;- read_csv(\"data/guinea_rice.csv\")\n\n# Convert the dataset to a tsibble\n# Here the index variable is 'Year', and there are no key variables.\n# The 'Production' variable is what we're interested in forecasting (the measured variable).\nlibrary(tsibble)\nguinea_rice &lt;- as_tsibble(guinea_rice, index = Year)\n\nA tsibble enables time-aware data manipulation, which makes it easy to work with time series. It also has extra checks to prevent common errors, while these can be frustrating at first they are important in correctly analysing your data.\nThere are two common mistakes when creating a tsibble, which we’ll see in the next example of Australian accommodation.\n\n# Read in the dataset using readr\naus_accommodation &lt;- read_csv(\"data/aus_accommodation.csv\")\n\n# Try to convert the dataset to a tsibble\naus_accommodation &lt;- as_tsibble(aus_accommodation, index = Date)\n\nError in `validate_tsibble()`:\n! A valid tsibble must have distinct rows identified by key and index.\nℹ Please use `duplicates()` to check the duplicated rows.\n\n\n\n\n\n\n\n\nThat didn’t work…\n\n\n\nReading the error says we have ‘duplicated rows’. What this means is that we have two or more rows in the dataset for the same point in time. In time series it isn’t possible to get two different values at the same time, but it is possible to measure several different things at the same time.\n\n\nWhen you get this error, consider if any of the dataset’s variables can identify individual series.\n\n\n\n\n\n\nTip\n\n\n\nThe identifying key variables of a time series are usually character variables, and the measured variables are almost always numeric.\n\n\n\naus_accommodation\n\n# A tibble: 592 × 5\n   Date       State                        Takings Occupancy   CPI\n   &lt;date&gt;     &lt;chr&gt;                          &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 1998-01-01 Australian Capital Territory    24.3      65    67  \n 2 1998-04-01 Australian Capital Territory    22.3      59    67.4\n 3 1998-07-01 Australian Capital Territory    22.5      58    67.5\n 4 1998-10-01 Australian Capital Territory    24.4      59    67.8\n 5 1999-01-01 Australian Capital Territory    23.7      58    67.8\n 6 1999-04-01 Australian Capital Territory    25.4      61    68.1\n 7 1999-07-01 Australian Capital Territory    28.2      66    68.7\n 8 1999-10-01 Australian Capital Territory    25.8      60    69.1\n 9 2000-01-01 Australian Capital Territory    27.3      60.9  69.7\n10 2000-04-01 Australian Capital Territory    30.1      64.7  70.2\n# ℹ 582 more rows\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhich of these variable(s) identifies each time series?\n\n\nIn this dataset we have accommodation data from all 8 states in Australia, and so we need to specify State as a key variable when creating our tsibble.\n\n# Try to convert the dataset to a tsibble\naus_accommodation &lt;- as_tsibble(aus_accommodation, index = Date, key = State)\naus_accommodation\n\n# A tsibble: 592 x 5 [1D]\n# Key:       State [8]\n   Date       State                        Takings Occupancy   CPI\n   &lt;date&gt;     &lt;chr&gt;                          &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 1998-01-01 Australian Capital Territory    24.3      65    67  \n 2 1998-04-01 Australian Capital Territory    22.3      59    67.4\n 3 1998-07-01 Australian Capital Territory    22.5      58    67.5\n 4 1998-10-01 Australian Capital Territory    24.4      59    67.8\n 5 1999-01-01 Australian Capital Territory    23.7      58    67.8\n 6 1999-04-01 Australian Capital Territory    25.4      61    68.1\n 7 1999-07-01 Australian Capital Territory    28.2      66    68.7\n 8 1999-10-01 Australian Capital Territory    25.8      60    69.1\n 9 2000-01-01 Australian Capital Territory    27.3      60.9  69.7\n10 2000-04-01 Australian Capital Territory    30.1      64.7  70.2\n# ℹ 582 more rows\n\n\nHurray, we have a tsibble! 🎉\n\n\n\n\n\n\nHowever there’s still one thing that isn’t right…\n\n\n\nIn the first row of the output we see [1D] - this means that the frequency of the data is daily.\n\n\nLooking at the index column (Date), we can see that each point in time is three months apart - or quarterly. This is another common mistake when working with time series, you need to set the appropriate temporal granularity.\n\n\n\n\n\n\nWhat is temporal granularity?\n\n\n\nTemporal granularity is the resolution in time. The time variable needs to match this resolution.\nIn this example, a date was used to represent quarters, but instead we must use yearquarter() to match the temporal granularity.\nHere’s a helpful list of common granularities:\n\nas.integer(): annual data (as above)\nyearquarter(): Quarterly data (shown here)\nyearmonth(): Monthly data\nyearweek(): Weekly data\nas.Date(): Daily data\nas.POSIXct(): Sub-daily data\n\n\n\nTo use the appropriate temporal granularity, we first must change our Date column before creating the tsibble.\n\n# Convert the `Date` column to quarterly with dplyr\nlibrary(dplyr)\naus_accommodation &lt;- aus_accommodation |&gt; \n  mutate(Date = yearquarter(Date)) |&gt; \n  as_tsibble(index = Date, key = State)\naus_accommodation\n\n# A tsibble: 592 x 5 [1Q]\n# Key:       State [8]\n      Date State                        Takings Occupancy   CPI\n     &lt;qtr&gt; &lt;chr&gt;                          &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 1998 Q1 Australian Capital Territory    24.3      65    67  \n 2 1998 Q2 Australian Capital Territory    22.3      59    67.4\n 3 1998 Q3 Australian Capital Territory    22.5      58    67.5\n 4 1998 Q4 Australian Capital Territory    24.4      59    67.8\n 5 1999 Q1 Australian Capital Territory    23.7      58    67.8\n 6 1999 Q2 Australian Capital Territory    25.4      61    68.1\n 7 1999 Q3 Australian Capital Territory    28.2      66    68.7\n 8 1999 Q4 Australian Capital Territory    25.8      60    69.1\n 9 2000 Q1 Australian Capital Territory    27.3      60.9  69.7\n10 2000 Q2 Australian Capital Territory    30.1      64.7  70.2\n# ℹ 582 more rows\n\n\nNow we have a tsibble that’s ready to use! In the first row of the output you should now see [1Q] indicating that the data is quarterly. You can also see the second row shows us our key variable, State. Next to this is [8], which tells us that this dataset contains 8 time series (one for each of Australia’s states).\n\n\n\n\n\n\nPipes\n\n\n\nWhen chaining together multiple functions, it’s helpful to use the pipe operator (|&gt;).\nThe pipe allows you to read the functions in the order that they are used - much like a sentence!\nMore information is here: https://r4ds.hadley.nz/workflow-style.html#sec-pipes\n\n\nThat’s all you need to know about creating a tidy time series tsibble 🌈.\n\n\n\n\n\n\nYour turn!\n\n\n\nCreate a tsibble for the number of tourists visiting Australia contained in data/tourism.csv.\nSome starter code has been provided for you in the day 1 exercises.\nHint: this dataset contains multiple key variables that need to be used together. You can specify multiple keys with as_tsibble(key = c(a, b, c))."
  },
  {
    "objectID": "sessions/day1/exercises.html#manipulating-time-series",
    "href": "sessions/day1/exercises.html#manipulating-time-series",
    "title": "Exercises",
    "section": "Manipulating time series",
    "text": "Manipulating time series\nOften you want to work with specific series, or perhaps the sum up the values across multiple series. We can use the same dplyr functions that are used in data analysis to explore our time series. Let’s focus on a single state from the Australian accommodation example - here we use filter() to keep only the Queensland data.\n\naus_accommodation |&gt; \n  filter(State == \"Queensland\")\n\n# A tsibble: 74 x 5 [1Q]\n# Key:       State [1]\n      Date State      Takings Occupancy   CPI\n     &lt;qtr&gt; &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 1998 Q1 Queensland    230.      54    67  \n 2 1998 Q2 Queensland    219.      54    67.4\n 3 1998 Q3 Queensland    268.      64    67.5\n 4 1998 Q4 Queensland    279.      61    67.8\n 5 1999 Q1 Queensland    241.      55    67.8\n 6 1999 Q2 Queensland    235.      56    68.1\n 7 1999 Q3 Queensland    286.      65    68.7\n 8 1999 Q4 Queensland    288.      61    69.1\n 9 2000 Q1 Queensland    253.      54.7  69.7\n10 2000 Q2 Queensland    253.      56.5  70.2\n# ℹ 64 more rows\n\n\nMaybe we wanted to focus on the more recent data, only keeping observations after 2010. Note that multiple conditions (both time and place) can be included inside a single filter() function.\n\naus_accommodation |&gt; \n  filter(State == \"Queensland\", Date &gt;= yearquarter(\"2010 Q1\"))\n\n# A tsibble: 26 x 5 [1Q]\n# Key:       State [1]\n      Date State      Takings Occupancy   CPI\n     &lt;qtr&gt; &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 2010 Q1 Queensland    464.      57.4  95.2\n 2 2010 Q2 Queensland    461.      58.5  95.8\n 3 2010 Q3 Queensland    573.      68.9  96.5\n 4 2010 Q4 Queensland    562.      64.8  96.9\n 5 2011 Q1 Queensland    471.      58.1  98.3\n 6 2011 Q2 Queensland    489.      61    99.2\n 7 2011 Q3 Queensland    592.      70.5  99.8\n 8 2011 Q4 Queensland    587.      66.9  99.8\n 9 2012 Q1 Queensland    530.      62.3  99.9\n10 2012 Q2 Queensland    519.      62.6 100. \n# ℹ 16 more rows\n\n\nLet’s try seeing the total accommodation Takings and Occupancy for all of Australia. For this, we can use the summarise() function to summarise information across multiple rows.\n\naus_accommodation |&gt; \n  summarise(Takings = sum(Takings), Occupancy = sum(Occupancy))\n\n# A tsibble: 74 x 3 [1Q]\n      Date Takings Occupancy\n     &lt;qtr&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1 1998 Q1    949.      469 \n 2 1998 Q2    875.      431 \n 3 1998 Q3    981.      458 \n 4 1998 Q4   1036.      468 \n 5 1999 Q1    997.      460 \n 6 1999 Q2    940.      447 \n 7 1999 Q3   1062.      481 \n 8 1999 Q4   1105.      474 \n 9 2000 Q1   1088.      465.\n10 2000 Q2   1039.      460.\n# ℹ 64 more rows\n\n\n\n\n\n\n\n\nThe index and summarise()\n\n\n\nWe still have our Date variable as it is automatically grouped when working with tsibble.\n\n\nWhat about calculating the annual takings, not quarterly? For this we use a special grouping function called index_by().\n\nlibrary(lubridate)\naus_accommodation |&gt; \n  index_by(Year = year(Date)) |&gt; \n  summarise(Takings = sum(Takings), Occupancy = sum(Occupancy))\n\n# A tsibble: 19 x 3 [1Y]\n    Year Takings Occupancy\n   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1  1998   3841.     1826 \n 2  1999   4104.     1862 \n 3  2000   4725.     1834.\n 4  2001   4766.     1819.\n 5  2002   4865.     1848 \n 6  2003   5277.     1887.\n 7  2004   5675.     1950.\n 8  2005   6189.     1996.\n 9  2006   6783.     2054.\n10  2007   7443.     2107.\n11  2008   7897.     2074.\n12  2009   7629.     2024.\n13  2010   8088.     2081 \n14  2011   8534.     2089.\n15  2012   8965.     2088 \n16  2013   8992.     2048.\n17  2014   9477.     2031.\n18  2015  10242.     2069.\n19  2016   5080.     1034.\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nUsing the tourism dataset, create an annual time series of the Purpose of travel for visitors to Australia (summing over State and Region)\nSome starter code has been provided for you in the day 1 exercises.\nHint: think about which key variables should be kept with group_by(), and how the index should be changed using index_by() then summarise().\n\n\nWhat if we didn’t want a time series at all? To calculate the total takings over all of time, we convert back to an ordinary data frame with as_tibble() and then summarise().\n\nlibrary(lubridate)\naus_accommodation |&gt; \n  as_tibble() |&gt; \n  summarise(Takings = sum(Takings), Occupancy = sum(Occupancy))\n\n# A tibble: 1 × 2\n  Takings Occupancy\n    &lt;dbl&gt;     &lt;dbl&gt;\n1 128571.    36720.\n\n\nWhich state has had the most accommodation takings in 2010? Let’s calculate total takings by state for 2010, and sort them with arrange().\n\naus_accommodation |&gt; \n  filter(year(Date) == 2010) |&gt; \n  as_tibble() |&gt; \n  group_by(State) |&gt; \n  summarise(Takings = sum(Takings), Occupancy = sum(Occupancy)) |&gt; \n  arrange(desc(Takings))\n\n# A tibble: 8 × 3\n  State                        Takings Occupancy\n  &lt;chr&gt;                          &lt;dbl&gt;     &lt;dbl&gt;\n1 New South Wales                2595.      259.\n2 Queensland                     2061.      250.\n3 Victoria                       1517.      258.\n4 Western Australia               849.      259.\n5 South Australia                 381.      252.\n6 Northern Territory              265.      262.\n7 Australian Capital Territory    227.      304.\n8 Tasmania                        193.      238.\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nUsing the tourism dataset, which Purpose of travel is most common in each state?\nSome starter code has been provided for you in the day 1 exercises.\nHint: since you no longer want to consider changes over time, you’ll need to convert the data back to a tibble."
  },
  {
    "objectID": "sessions/day1/exercises.html#visualising-time-series",
    "href": "sessions/day1/exercises.html#visualising-time-series",
    "title": "Exercises",
    "section": "Visualising time series",
    "text": "Visualising time series\nThere are a few common visualisation techniques specific to time series, however cross-sectional graphics also work well for time series data. The main difference is that we like to maintain the ordered and connected nature of time.\n\nTime plots\nThe simplest graphic for time series is the time series plot, which shows the variable of interest (on the y-axis) against time (on the x-axis). This plot can be created manually with ggplot2, or automatically plotted from the tsibble with autoplot().\n\nlibrary(fable)\nlibrary(ggplot2)\nguinea_rice |&gt; \n  autoplot(Production)\n\n\n\n\nIn this plot we can see that Production increases over time (known as trend). The increase is mostly smooth but there are a couple anomalies in 2001 and 2008.\n\n\n\n\n\n\nPlotting the time variable\n\n\n\nIn this plot, Production and Year are two continuous variables. We would often like to plot two continuous variables with a scatter plot, however in time-series we prefer to connect the observations from one year to the next to give this line chart.\n\n\nWe can also use autoplot() to produce a time plot of many series, but be careful not to plot too many lines at once!\n\naus_accommodation |&gt; \n  autoplot(Takings)\n\n\n\n\nIn this plot of Australian accommodation takings, we see that most states have increasing takings over time (upward trend). We can also notice a repeating up and down pattern, which upon closer inspection repeats every year. This repeating annual pattern is known as seasonality, and we can see that some states are more seasonal than others.\nLet’s focus on the sunny holiday destination of Queensland, and use different plots to better understand the seasonality.\n\naus_accommodation |&gt; \n  filter(State == \"Queensland\") |&gt; \n  autoplot(Takings)\n\n\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nUsing the tourism dataset, create time plots of the data. Which patterns can you observe?\nSome starter code has been provided for you in the day 1 exercises.\nHint: there are too many series to show in a single plot, so filter and summarise series of interest to you.\n\n\n\n\nSeasonal plots\nIt can be tricky to see which quarter has maximum accommodation takings from a time plot. Instead, it is better to use a seasonal plot with gg_season() from feasts.\n\nlibrary(feasts)\naus_accommodation |&gt; \n  filter(State == \"Queensland\") |&gt; \n  gg_season(Takings)\n\n\n\n\nHere we can see that the Q3 and Q4 takings are higher than Q1 and Q2, this is known as the seasonal peak and trough respectively.\n\n\n\n\n\n\nThe season plot\n\n\n\nThe seasonal plot is very similar to the time plot, but the x-axis now wraps over years. This allows us to more easily compare the years and find common patterns, like which month or quarter is biggest and smallest.\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nUsing the tourism dataset, create a seasonal plot for the total holiday travel to Australia over time. In which quarter is holiday travel highest and lowest?\nSome starter code has been provided for you in the day 1 exercises.\n\n\n\n\nSeasonal subseries plot\nAnother useful plot to understand the seasonal pattern of a time series is the subseries plot, it can be created with gg_subseries(). This plot is splits each month / quarter into separate facets (mini-plots), which shows how the values within each season change over time. The blue lines represent the average, which is a useful way to see the overall seasonality at a glance.\n\naus_accommodation |&gt; \n  filter(State == \"Queensland\") |&gt; \n  gg_subseries(Takings)\n\n\n\n\n\n\n\n\n\n\nSeasonal sub-series plots\n\n\n\nThe upward lines in each facet of this plot shows the trend of the data, however if the lines went in different directions that would imply the shape of the seasonality is changing over time.\nSeasonal plots work best after removing trend, which we will see how to do tomorrow!\n\n\nLet’s see this plot with a different dataset, recent beer production in Australia.\n\naus_beer &lt;- tsibbledata::aus_production |&gt; \n  filter(Quarter &gt;= yearquarter(\"1992 Q1\")) |&gt; \n  select(Beer)\naus_beer |&gt; \n  autoplot(Beer)\n\n\n\n\nAt a glance, this looks like the it is very seasonal and has a slight downward trend. However the seasonal subseries plot reveals that the trend is misleading!\n\naus_beer |&gt; \n  gg_subseries(Beer)\n\n\n\n\nHere we see that only Q4 (the peak) has a downward trend, while the other quarters are staying roughly the same. The seasonality is changing shape over time.\n\n\n\n\n\n\nChanging seasonality\n\n\n\nLook back at the time plot and focus only on the Q4 peaks, can you see these values decreasing over time? Now look at the Q1-Q3 throughs, how do they change over time?\nThis can be tricky to notice in the time plot, which is why seasonal subseries plots can be particularly helpful!\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nUsing the tourism dataset, create a seasonal subseries plot for the total business travel to Victoria over time. Does the seasonal pattern change over time?\nSome starter code has been provided for you in the day 1 exercises.\n\n\n\n\nACF plots\nThese plots may look a bit strange at first, but they are very useful for seeing all of the time series dynamics in a single plot. ACF is the ‘auto-correlation function’, essentially a measure of how similar a time series is to the lags of itself. Looking at these correlations can reveal trends, seasonality, cycles, and more subtle patterns. You can create an ACF plot using a combination of ACF() and autoplot().\n\nguinea_rice |&gt; \n  ACF(Production) |&gt; \n  autoplot()\n\n\n\n\nThe rice production of Guinea has an upward trend, which produces a gradual decay in the ACF.\n\naus_beer |&gt; \n  ACF(Beer) |&gt; \n  autoplot()\n\n\n\n\nThe recent beer production of Australia has lots of seasonality and no trend, which creates large peaks at the seasonal lags in the ACF. Every 4 quarters we see a large ACF spike.\n\naus_accommodation |&gt; \n  summarise(Occupancy = sum(Occupancy)) |&gt; \n  ACF(Occupancy) |&gt; \n  autoplot()\n\n\n\n\nThe total occupancy of Australia’s short-term accommodation is both trended and seasonal, which results in a slowly decaying ACF with peaks every seasonal lag (4, 8, 12, …).\nConsider the number of Snowshoe Hares which were traded by the Hudson Bay Company.\n\ntsibbledata::pelt |&gt; \n  autoplot(Hare)\n\n\n\n\nTo the untrained eye, this series has lots of up and down patterns - a bit like seasonality. However this pattern is cyclical, not seasonal. The ACF plot can help us distinguish cycles from seasonality.\n\n\n\n\n\n\nSeasonal or cyclic?\n\n\n\nSeasonality is a consistent repeating pattern, where the shape shape with similar peak and trough repeats at the same time interval.\nCyclical patterns are less consistent, with varying peaks and troughs that repeats over a varied time period.\n\n\nLet’s see the ACF for this dataset\n\ntsibbledata::pelt |&gt; \n  ACF(Hare) |&gt; \n  autoplot()\n\n\n\n\nNotice that the peak at lag 10 is less symmetric and ‘sharp’, this is because the pattern usually repeats every 10 years but sometimes 9 or 11. This is unlike seasonality, which has a sharper peak in the ACF due to the consistent time period between patterns.\n\n\n\n\n\n\nYour turn!\n\n\n\nIdentify which ACF matches the time plots in the following figures by identifying the patterns of trend, seasonality, and cycles in the ACF plots.\n\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nUsing the tourism dataset, create an ACF plot for the total travel to Australia over time. Can you identify patterns of trend and seasonality from this plot?\nSome starter code has been provided for you in the day 1 exercises.\n\n\n\n\n\n\n\n\nACF model evaluation\n\n\n\nImportantly, ACF plots can also tell us when there are no patterns/autocorrelations in the data (white noise).\nWe’ll be revisiting this plot to evaluate our models on day 5. We hope that a model uses all available information, and ACF plots can show if there is any patterns left over."
  },
  {
    "objectID": "sessions/day1/exercises.html#about-the-dataset",
    "href": "sessions/day1/exercises.html#about-the-dataset",
    "title": "Exercises",
    "section": "About the dataset",
    "text": "About the dataset\nIn this exercise, we use a dataset containing dose of BCG (Cacille Calmette-Guérin), vaccine administrated in 9 regions of an African country from January 2013 untill December 2021. BCG is a widely administered vaccine primarily used to protect against tuberculosis (TB), a serious infection that primarily affects the lungs but can also affect other parts of the body. BCG vaccination is recommended for newborn babies at risk of tuberculosis (TB) and is typically administered shortly after birth, usually within the first 28 days of life.\nIn addition to the administered dose, it also includes data on the population of children under one year old, whether a strike occurred in a specific month and region, and whether a region is affected by flooding events.\nIn this exercise, you will apply what you have learned so far on a dataset containing doses of administered vaccine in an African country."
  },
  {
    "objectID": "sessions/day1/exercises.html#about-the-dataset-1",
    "href": "sessions/day1/exercises.html#about-the-dataset-1",
    "title": "Exercises",
    "section": "About the dataset",
    "text": "About the dataset\nIn this exercise, we use a dataset containing dose of BCG (Cacille Calmette-Guérin), vaccine administrated in 9 regions of an African country from January 2013 until December 2021. BCG is a widely administered vaccine primarily used to protect against tuberculosis (TB), a serious infection that primarily affects the lungs but can also affect other parts of the body. BCG vaccination is recommended for newborn babies at risk of tuberculosis (TB) and is typically administered shortly after birth, usually within the first 28 days of life.\nIn addition to the administered dose, it also includes data on the population of children under one year old, and whether a strike occurred in a specific month and region.\nIn this exercise, you will apply what you have learned about different steps in the forecasting workflow on this dataset.\n\n\n\n\n\n\nYour turn!\n\n\n\n\nImport vaccine_adminstrated.csv data into R\n\nCheck and modify the data types of variables as needed\n\nPrepare your data\n\nCheck and fix missing values\nCheck duplications and fix it\nCreate tsibble\nCheck and fix temporal gaps\n\nManipulating time series\n\nCreate monthly time series of total doses adminstrated in the country\nCreate quarterly time series of doses adminstrated in each region\nCreate quarterly time series of total doses adminstrated in the country\n\nVisualizing time series\n\nUse time plots and describe what patterns you observe\nCreate plots to see if any consistent pattern exsists in monthly and quarterly of dose admisntrated\nCreate plots to see how dose admisntrated chnage over time for each month/quarter and how it differs across differnt month/quarter"
  },
  {
    "objectID": "sessions/day2/exercises.html",
    "href": "sessions/day2/exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Transformations provide useful simplifications of the patterns in a time series. Simplifying the patterns makes them easier to model, and so transforming the data is a common preliminary step in producing forecasts. Some transformations standardise values to be comparable between countries or other series in the dataset, while others can regularise the variation in the data.\nLet’s look at the turnover of print media in Australia.\n\nlibrary(tsibbledata)\nlibrary(fable)\n\nLoading required package: fabletools\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\naus_print &lt;- aus_retail |&gt; \n  filter(Industry == \"Newspaper and book retailing\") |&gt; \n  summarise(Turnover = sum(Turnover))\naus_print |&gt; \n  autoplot(Turnover)\n\n\n\n\nTurnover has increased until the end of 2010, after which it has steadily declined. When looking at monetary value it is common to consider price indices to ensure that turnover is comparable over time. This allows you to identify patterns and changes such as turning points in real monetary terms.\n\n\n\n\n\n\nData for transformations\n\n\n\nIt can be useful to use other datasets that contain information for the transformation. To do this we can merge the datasets in time using join operations.\n\n\n\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\naus_economy &lt;- global_economy |&gt; \n  filter(Country == \"Australia\")\naus_print |&gt; \n  mutate(Year = year(Month)) |&gt; \n  left_join(aus_economy, by = \"Year\") |&gt; \n  autoplot(Turnover/CPI)\n\nWarning: Removed 12 rows containing missing values (`geom_line()`).\n\n\n\n\n\nAfter taking into account CPI, the real monetary Turnover of the print media industry in Australia has been gradually declining since 1990-2000.\n\n\n\n\n\n\nYour turn!\n\n\n\nSelect a country of your choice from global_economy, then calculate and visualise the the GDP per capita over time (that is, the GDP scaled by the population).\n\n\n\n\n\n\n\n\nTricky to forecast\n\n\n\nWhile transformations help to make the patterns simpler to forecast, if additional information like CPI or Population are used then they will also need to be forecasted. While population is generally easy to forecast, CPI could be more complicated to forecast than the thing you’re originally forecasting!\n\n\nAnother useful transformation is calendar adjustments. This adjusts the observations in the time series to represent an equivalent time period, and can simplify seasonal patterns which result from these different lengths. This is particularly useful for monthly data, since the number of days in each month varies substantially.\n\nlibrary(feasts)\naus_print |&gt; \n  autoplot(Turnover / days_in_month(Month))\n\n\n\naus_print |&gt; \n  gg_subseries(Turnover / days_in_month(Month))\n\n\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nCalculate the monthly total Australian retail turnover from aus_retail and visualise the seasonal pattern. Then scale by the number of days in each month to calculate the daily average turnover and comparse the seasonal patterns.\n\n\nMathematical transformations are useful since they don’t require providing any future values to produce the forecasts. Log and power transformations (\\(y^k\\), for example square root, square, and inverse) are particularly helpful for regularising variation proportional to the level of the series.\n\naus_retail |&gt; \n  filter(State == \"Victoria\", Industry == \"Cafes, restaurants and catering services\") |&gt; \n  autoplot(Turnover)\n\n\n\n\nThis proportional variance is common in time series, in Victoria’s cafe and restaurant turnover you can see small changes when turnover is low (before 2000), and is much larger after 2010 when turnover is much larger.\n\naus_retail |&gt; \n  filter(State == \"Victoria\", Industry == \"Cafes, restaurants and catering services\") |&gt; \n  autoplot(log(Turnover))\n\n\n\n\nLog transforming the data changes this variation to be more consistent, where the variation before 2000 is now more similar to after 2010.\nThe box-cox transformation family parameterises the range of power transformations for more precise adjustments. The transformation parameter \\(\\lambda\\) controls the strength of the transformation, with \\(\\lambda=1\\) being no change in shape, \\(\\lambda = 0\\) being a log transformation and others being equivalent in shape to \\(y^\\lambda\\).\nThe log transformation above was a bit strong, so let’s try something slightly close to \\(\\lambda=1\\) - perhaps \\(\\lambda = 0.1\\)?\n\naus_retail |&gt; \n  filter(State == \"Victoria\", Industry == \"Cafes, restaurants and catering services\") |&gt; \n  autoplot(box_cox(Turnover, lambda = 0.1))\n\n\n\n\nThe variation is now consistent for the entire series, and the trend is linear.\n\n\n\n\n\n\nAutomatic box-cox transformations\n\n\n\nThe \\(\\lambda\\) parameter can be automatically computed using the guerrero() function.\n\n\nWe can calculate the optimal box-cox parameter using features() and guerrero():\n\naus_retail |&gt; \n  filter(State == \"Victoria\", Industry == \"Cafes, restaurants and catering services\") |&gt; \n  features(Turnover, features = guerrero)\n\n# A tibble: 1 × 3\n  State    Industry                                 lambda_guerrero\n  &lt;chr&gt;    &lt;chr&gt;                                              &lt;dbl&gt;\n1 Victoria Cafes, restaurants and catering services           0.173\n\n\nLooks like we were pretty close with \\(\\lambda = 0.1\\), let’s try using this more precise estimate:\n\naus_retail |&gt; \n  filter(State == \"Victoria\", Industry == \"Cafes, restaurants and catering services\") |&gt; \n  autoplot(box_cox(Turnover, lambda = guerrero(Turnover)))\n\n\n\n\nThe optimised box-cox transformation is very similar to \\(\\lambda = 0.1\\) - fortunately for us we don’t have to be precise since this transformation isn’t sensitive to your choice of parameter. So long as you are within \\(\\pm 0.1\\) the transformation should be okay. Additionally, if \\(\\lambda \\approx 0\\) then it is common to instead use the simpler log() transformation.\n\n\n\n\n\n\nYour turn!\n\n\n\nFind a suitable box-cox transformation for the monthly total Australian retail turnover, then compare your choice with the automatically selected parameter from the guerrero() feature.\n\n\n\n\n\nAnother commonly used transformation/adjustment requires a model to decompose the time series into its components. Seasonally adjusted time series are often used by analysts and policy makers to evaluate the underlying long term trends without the added complexity of seasonality. The STL decomposition is useful model which can isolate the seasonal pattern from the trend and remainder for many types of time series.\nThe STL decomposition separates time series into the form \\(Y = \\text{trend} + \\text{seasonality} + \\text{remainder}\\). Since this is an additive decomposition, we must first simplify any multiplicative patterns into additive ones using a suitable power transformation. Let’s try to remove the annual seasonality from Australia’s print media turnover.\n\naus_print |&gt; \n  autoplot(Turnover)\n\n\n\n\nThe seasonality is more varied when turnover increases, so we must transform the data before estimating the STL model.\n\naus_print |&gt; \n  autoplot(log(Turnover))\n\n\n\n\nThe log transformation (\\(\\lambda = 0\\)) does a great job at producing a consistent variation throughout the series. You could try to find a better transformation using the box-cox transformation family, however there is no need for it here.\nWe can estimate the STL model using STL() as follows:\n\nfit &lt;- aus_print |&gt; \n  model(STL(log(Turnover)))\nfit\n\n# A mable: 1 x 1\n  `STL(log(Turnover))`\n               &lt;model&gt;\n1                &lt;STL&gt;\n\n\nThe decomposition can be obtained from the model using components(), and then all of the components can be plotted with autoplot():\n\nfit |&gt; \n  components() \n\n# A dable: 441 x 7 [1M]\n# Key:     .model [1]\n# :        log(Turnover) = trend + season_year + remainder\n   .model        Month `log(Turnover)` trend season_year remainder season_adjust\n   &lt;chr&gt;         &lt;mth&gt;           &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n 1 STL(log(T… 1982 Apr            4.90  4.92     -0.0723   0.0528           4.97\n 2 STL(log(T… 1982 May            4.89  4.92     -0.0128  -0.0151           4.91\n 3 STL(log(T… 1982 Jun            4.84  4.93     -0.0942   0.00869          4.94\n 4 STL(log(T… 1982 Jul            4.86  4.93     -0.0480  -0.0303           4.90\n 5 STL(log(T… 1982 Aug            4.88  4.94     -0.0155  -0.0458           4.89\n 6 STL(log(T… 1982 Sep            4.90  4.94     -0.0453  -0.00317          4.94\n 7 STL(log(T… 1982 Oct            4.93  4.95     -0.0167   0.00114          4.95\n 8 STL(log(T… 1982 Nov            4.97  4.95      0.0181  -0.00245          4.95\n 9 STL(log(T… 1982 Dec            5.26  4.96      0.258    0.0398           5.00\n10 STL(log(T… 1983 Jan            4.92  4.97     -0.0180  -0.0249           4.94\n# ℹ 431 more rows\n\nfit |&gt; \n  components() |&gt; \n  autoplot()\n\n\n\n\nThe components are obtained using rolling estimation windows, which are the main way the decomposition is changed. A large window produces smooth components, and a small window produces flexible and quickly changing components.\n\naus_print |&gt; \n  model(STL(log(Turnover) ~ trend(window = 5) + season(window = Inf))) |&gt; \n  components() |&gt; \n  autoplot()\n\n\n\n\nThe infinite window for the seasonality results in a seasonal pattern that doesn’t change over time, while the small trend window allows the trend to change very quickly. The best choice of estimation window should produce components that match the patterns in the original data while being as smooth as possible.\n\nfit &lt;- aus_print |&gt; \n  model(STL(log(Turnover) ~ trend(window = 25) + season(window = Inf))) \nfit |&gt; \n  components() |&gt; \n  autoplot()\n\n\n\n\nA trend window of 25 for this dataset produces a mostly smooth trend component which can still react to brief decreases in turnover. The constant seasonal pattern (infinite window) is reasonable for this dataset since the seasonality doesn’t change much over time.\n\n\n\n\n\n\nSeasonal adjustment\n\n\n\nYou can find the de-seasonalised data in the season_adjust column of the components() output.\n\n\n\nfit |&gt; \n  components() |&gt; \n  autoplot(season_adjust)\n\n\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nFind a suitable STL decomposition for the total Australian retail turnover, then produce and visualise the seasonally adjusted time series.\nHint: don’t forget to use the suitable transformation found previously!\n\n\nSeasonal decomposition also makes it easier to take a look at the seasonality - we can use a combination of seasonal plots and decomposition to more easily see seasonal patterns.\n\nfit |&gt; \n  components() |&gt; \n  gg_season(season_year)\n\n\n\n\nIf the seasonal window allows the seasonal component to change over time, the gg_subseries() plot is especially useful for seeing how the pattern changes.\n\naus_print |&gt; \n  model(STL(log(Turnover) ~ trend(window = 25) + season(window = 9))) |&gt; \n  components() |&gt; \n  gg_subseries(season_year)\n\n\n\n\nJanuary and December seem to increase over time, while April, May and June are decreasing.\n\n\n\n\n\n\nYour turn!\n\n\n\nProduce appropriate seasonal plots of the seasonal component from your STL decomposition on Australian retail turnover.\n\n\n\n\n\nA useful technique for visualising large collections of time series is to produce summaries of their patterns known as features. Visualising many time series simultaneously is difficult since the scale and shape of patterns can vary substantially. The features() function will compute single value summaries over time such as the strength of trend or seasonality. There are many features available, but the features from STL decompositions are particularly interesting.\n\naus_retail |&gt; \n  features(Turnover, feat_stl)\n\n# A tibble: 152 × 11\n   State       Industry trend_strength seasonal_strength_year seasonal_peak_year\n   &lt;chr&gt;       &lt;chr&gt;             &lt;dbl&gt;                  &lt;dbl&gt;              &lt;dbl&gt;\n 1 Australian… Cafes, …          0.989                  0.562                  0\n 2 Australian… Cafes, …          0.993                  0.629                  0\n 3 Australian… Clothin…          0.991                  0.923                  9\n 4 Australian… Clothin…          0.993                  0.957                  9\n 5 Australian… Departm…          0.977                  0.980                  9\n 6 Australian… Electri…          0.992                  0.933                  9\n 7 Australian… Food re…          0.999                  0.890                  9\n 8 Australian… Footwea…          0.982                  0.944                  9\n 9 Australian… Furnitu…          0.981                  0.687                  9\n10 Australian… Hardwar…          0.992                  0.900                  9\n# ℹ 142 more rows\n# ℹ 6 more variables: seasonal_trough_year &lt;dbl&gt;, spikiness &lt;dbl&gt;,\n#   linearity &lt;dbl&gt;, curvature &lt;dbl&gt;, stl_e_acf1 &lt;dbl&gt;, stl_e_acf10 &lt;dbl&gt;\n\n\nIn particular, features from STL decompositions allow you to compare the strength of trend and seasonality between many time series.\n\nlibrary(ggplot2)\naus_retail |&gt; \n  features(Turnover, feat_stl) |&gt; \n  ggplot(aes(x = trend_strength, y = seasonal_strength_year)) + \n  geom_point()\n\n\n\n\nFrom this we can see that almost all time series have a strong trend, while the strength of seasonality is more varied - some series have strong seasonality while others have less seasonality.\n\n\n\n\n\n\nYour turn!\n\n\n\nCalculate the STL features for the time series in the tourism dataset. Try colouring the points in the scatterplot by the purpose of travel, are some reasons more trended or seasonal than others?\n\n\nThere are many other features that you can use - check out the documentation for ?features_by_pkg. You can produce a feature set of similar features using the feature_set() function.\n\naus_retail |&gt; \n  features(Turnover, feature_set(tags = \"autocorrelation\"))\n\n# A tibble: 152 × 13\n   State      Industry  acf1 acf10 diff1_acf1 diff1_acf10 diff2_acf1 diff2_acf10\n   &lt;chr&gt;      &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n 1 Australia… Cafes, … 0.973  8.59     -0.348       0.239     -0.572       0.502\n 2 Australia… Cafes, … 0.977  8.65     -0.327       0.259     -0.552       0.531\n 3 Australia… Clothin… 0.885  7.01     -0.276       0.251     -0.507       0.339\n 4 Australia… Clothin… 0.846  6.33     -0.303       0.201     -0.532       0.321\n 5 Australia… Departm… 0.500  1.60     -0.310       0.202     -0.540       0.315\n 6 Australia… Electri… 0.902  7.29     -0.247       0.324     -0.506       0.455\n 7 Australia… Food re… 0.984  9.13     -0.394       0.585     -0.611       1.12 \n 8 Australia… Footwea… 0.760  4.64     -0.325       0.155     -0.566       0.333\n 9 Australia… Furnitu… 0.952  7.67     -0.190       0.163     -0.530       0.394\n10 Australia… Hardwar… 0.957  7.67     -0.104       0.101     -0.497       0.311\n# ℹ 142 more rows\n# ℹ 5 more variables: season_acf1 &lt;dbl&gt;, pacf5 &lt;dbl&gt;, diff1_pacf5 &lt;dbl&gt;,\n#   diff2_pacf5 &lt;dbl&gt;, season_pacf &lt;dbl&gt;"
  },
  {
    "objectID": "sessions/day2/exercises.html#transformations",
    "href": "sessions/day2/exercises.html#transformations",
    "title": "Exercises",
    "section": "",
    "text": "Transformations provide useful simplifications of the patterns in a time series. Simplifying the patterns makes them easier to model, and so transforming the data is a common preliminary step in producing forecasts. Some transformations standardise values to be comparable between countries or other series in the dataset, while others can regularise the variation in the data.\nLet’s look at the turnover of print media in Australia.\n\nlibrary(tsibbledata)\nlibrary(fable)\n\nLoading required package: fabletools\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\naus_print &lt;- aus_retail |&gt; \n  filter(Industry == \"Newspaper and book retailing\") |&gt; \n  summarise(Turnover = sum(Turnover))\naus_print |&gt; \n  autoplot(Turnover)\n\n\n\n\nTurnover has increased until the end of 2010, after which it has steadily declined. When looking at monetary value it is common to consider price indices to ensure that turnover is comparable over time. This allows you to identify patterns and changes such as turning points in real monetary terms.\n\n\n\n\n\n\nData for transformations\n\n\n\nIt can be useful to use other datasets that contain information for the transformation. To do this we can merge the datasets in time using join operations.\n\n\n\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\naus_economy &lt;- global_economy |&gt; \n  filter(Country == \"Australia\")\naus_print |&gt; \n  mutate(Year = year(Month)) |&gt; \n  left_join(aus_economy, by = \"Year\") |&gt; \n  autoplot(Turnover/CPI)\n\nWarning: Removed 12 rows containing missing values (`geom_line()`).\n\n\n\n\n\nAfter taking into account CPI, the real monetary Turnover of the print media industry in Australia has been gradually declining since 1990-2000.\n\n\n\n\n\n\nYour turn!\n\n\n\nSelect a country of your choice from global_economy, then calculate and visualise the the GDP per capita over time (that is, the GDP scaled by the population).\n\n\n\n\n\n\n\n\nTricky to forecast\n\n\n\nWhile transformations help to make the patterns simpler to forecast, if additional information like CPI or Population are used then they will also need to be forecasted. While population is generally easy to forecast, CPI could be more complicated to forecast than the thing you’re originally forecasting!\n\n\nAnother useful transformation is calendar adjustments. This adjusts the observations in the time series to represent an equivalent time period, and can simplify seasonal patterns which result from these different lengths. This is particularly useful for monthly data, since the number of days in each month varies substantially.\n\nlibrary(feasts)\naus_print |&gt; \n  autoplot(Turnover / days_in_month(Month))\n\n\n\naus_print |&gt; \n  gg_subseries(Turnover / days_in_month(Month))\n\n\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nCalculate the monthly total Australian retail turnover from aus_retail and visualise the seasonal pattern. Then scale by the number of days in each month to calculate the daily average turnover and comparse the seasonal patterns.\n\n\nMathematical transformations are useful since they don’t require providing any future values to produce the forecasts. Log and power transformations (\\(y^k\\), for example square root, square, and inverse) are particularly helpful for regularising variation proportional to the level of the series.\n\naus_retail |&gt; \n  filter(State == \"Victoria\", Industry == \"Cafes, restaurants and catering services\") |&gt; \n  autoplot(Turnover)\n\n\n\n\nThis proportional variance is common in time series, in Victoria’s cafe and restaurant turnover you can see small changes when turnover is low (before 2000), and is much larger after 2010 when turnover is much larger.\n\naus_retail |&gt; \n  filter(State == \"Victoria\", Industry == \"Cafes, restaurants and catering services\") |&gt; \n  autoplot(log(Turnover))\n\n\n\n\nLog transforming the data changes this variation to be more consistent, where the variation before 2000 is now more similar to after 2010.\nThe box-cox transformation family parameterises the range of power transformations for more precise adjustments. The transformation parameter \\(\\lambda\\) controls the strength of the transformation, with \\(\\lambda=1\\) being no change in shape, \\(\\lambda = 0\\) being a log transformation and others being equivalent in shape to \\(y^\\lambda\\).\nThe log transformation above was a bit strong, so let’s try something slightly close to \\(\\lambda=1\\) - perhaps \\(\\lambda = 0.1\\)?\n\naus_retail |&gt; \n  filter(State == \"Victoria\", Industry == \"Cafes, restaurants and catering services\") |&gt; \n  autoplot(box_cox(Turnover, lambda = 0.1))\n\n\n\n\nThe variation is now consistent for the entire series, and the trend is linear.\n\n\n\n\n\n\nAutomatic box-cox transformations\n\n\n\nThe \\(\\lambda\\) parameter can be automatically computed using the guerrero() function.\n\n\nWe can calculate the optimal box-cox parameter using features() and guerrero():\n\naus_retail |&gt; \n  filter(State == \"Victoria\", Industry == \"Cafes, restaurants and catering services\") |&gt; \n  features(Turnover, features = guerrero)\n\n# A tibble: 1 × 3\n  State    Industry                                 lambda_guerrero\n  &lt;chr&gt;    &lt;chr&gt;                                              &lt;dbl&gt;\n1 Victoria Cafes, restaurants and catering services           0.173\n\n\nLooks like we were pretty close with \\(\\lambda = 0.1\\), let’s try using this more precise estimate:\n\naus_retail |&gt; \n  filter(State == \"Victoria\", Industry == \"Cafes, restaurants and catering services\") |&gt; \n  autoplot(box_cox(Turnover, lambda = guerrero(Turnover)))\n\n\n\n\nThe optimised box-cox transformation is very similar to \\(\\lambda = 0.1\\) - fortunately for us we don’t have to be precise since this transformation isn’t sensitive to your choice of parameter. So long as you are within \\(\\pm 0.1\\) the transformation should be okay. Additionally, if \\(\\lambda \\approx 0\\) then it is common to instead use the simpler log() transformation.\n\n\n\n\n\n\nYour turn!\n\n\n\nFind a suitable box-cox transformation for the monthly total Australian retail turnover, then compare your choice with the automatically selected parameter from the guerrero() feature."
  },
  {
    "objectID": "sessions/day2/exercises.html#decomposition",
    "href": "sessions/day2/exercises.html#decomposition",
    "title": "Exercises",
    "section": "",
    "text": "Another commonly used transformation/adjustment requires a model to decompose the time series into its components. Seasonally adjusted time series are often used by analysts and policy makers to evaluate the underlying long term trends without the added complexity of seasonality. The STL decomposition is useful model which can isolate the seasonal pattern from the trend and remainder for many types of time series.\nThe STL decomposition separates time series into the form \\(Y = \\text{trend} + \\text{seasonality} + \\text{remainder}\\). Since this is an additive decomposition, we must first simplify any multiplicative patterns into additive ones using a suitable power transformation. Let’s try to remove the annual seasonality from Australia’s print media turnover.\n\naus_print |&gt; \n  autoplot(Turnover)\n\n\n\n\nThe seasonality is more varied when turnover increases, so we must transform the data before estimating the STL model.\n\naus_print |&gt; \n  autoplot(log(Turnover))\n\n\n\n\nThe log transformation (\\(\\lambda = 0\\)) does a great job at producing a consistent variation throughout the series. You could try to find a better transformation using the box-cox transformation family, however there is no need for it here.\nWe can estimate the STL model using STL() as follows:\n\nfit &lt;- aus_print |&gt; \n  model(STL(log(Turnover)))\nfit\n\n# A mable: 1 x 1\n  `STL(log(Turnover))`\n               &lt;model&gt;\n1                &lt;STL&gt;\n\n\nThe decomposition can be obtained from the model using components(), and then all of the components can be plotted with autoplot():\n\nfit |&gt; \n  components() \n\n# A dable: 441 x 7 [1M]\n# Key:     .model [1]\n# :        log(Turnover) = trend + season_year + remainder\n   .model        Month `log(Turnover)` trend season_year remainder season_adjust\n   &lt;chr&gt;         &lt;mth&gt;           &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n 1 STL(log(T… 1982 Apr            4.90  4.92     -0.0723   0.0528           4.97\n 2 STL(log(T… 1982 May            4.89  4.92     -0.0128  -0.0151           4.91\n 3 STL(log(T… 1982 Jun            4.84  4.93     -0.0942   0.00869          4.94\n 4 STL(log(T… 1982 Jul            4.86  4.93     -0.0480  -0.0303           4.90\n 5 STL(log(T… 1982 Aug            4.88  4.94     -0.0155  -0.0458           4.89\n 6 STL(log(T… 1982 Sep            4.90  4.94     -0.0453  -0.00317          4.94\n 7 STL(log(T… 1982 Oct            4.93  4.95     -0.0167   0.00114          4.95\n 8 STL(log(T… 1982 Nov            4.97  4.95      0.0181  -0.00245          4.95\n 9 STL(log(T… 1982 Dec            5.26  4.96      0.258    0.0398           5.00\n10 STL(log(T… 1983 Jan            4.92  4.97     -0.0180  -0.0249           4.94\n# ℹ 431 more rows\n\nfit |&gt; \n  components() |&gt; \n  autoplot()\n\n\n\n\nThe components are obtained using rolling estimation windows, which are the main way the decomposition is changed. A large window produces smooth components, and a small window produces flexible and quickly changing components.\n\naus_print |&gt; \n  model(STL(log(Turnover) ~ trend(window = 5) + season(window = Inf))) |&gt; \n  components() |&gt; \n  autoplot()\n\n\n\n\nThe infinite window for the seasonality results in a seasonal pattern that doesn’t change over time, while the small trend window allows the trend to change very quickly. The best choice of estimation window should produce components that match the patterns in the original data while being as smooth as possible.\n\nfit &lt;- aus_print |&gt; \n  model(STL(log(Turnover) ~ trend(window = 25) + season(window = Inf))) \nfit |&gt; \n  components() |&gt; \n  autoplot()\n\n\n\n\nA trend window of 25 for this dataset produces a mostly smooth trend component which can still react to brief decreases in turnover. The constant seasonal pattern (infinite window) is reasonable for this dataset since the seasonality doesn’t change much over time.\n\n\n\n\n\n\nSeasonal adjustment\n\n\n\nYou can find the de-seasonalised data in the season_adjust column of the components() output.\n\n\n\nfit |&gt; \n  components() |&gt; \n  autoplot(season_adjust)\n\n\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nFind a suitable STL decomposition for the total Australian retail turnover, then produce and visualise the seasonally adjusted time series.\nHint: don’t forget to use the suitable transformation found previously!\n\n\nSeasonal decomposition also makes it easier to take a look at the seasonality - we can use a combination of seasonal plots and decomposition to more easily see seasonal patterns.\n\nfit |&gt; \n  components() |&gt; \n  gg_season(season_year)\n\n\n\n\nIf the seasonal window allows the seasonal component to change over time, the gg_subseries() plot is especially useful for seeing how the pattern changes.\n\naus_print |&gt; \n  model(STL(log(Turnover) ~ trend(window = 25) + season(window = 9))) |&gt; \n  components() |&gt; \n  gg_subseries(season_year)\n\n\n\n\nJanuary and December seem to increase over time, while April, May and June are decreasing.\n\n\n\n\n\n\nYour turn!\n\n\n\nProduce appropriate seasonal plots of the seasonal component from your STL decomposition on Australian retail turnover."
  },
  {
    "objectID": "sessions/day2/exercises.html#features",
    "href": "sessions/day2/exercises.html#features",
    "title": "Exercises",
    "section": "",
    "text": "A useful technique for visualising large collections of time series is to produce summaries of their patterns known as features. Visualising many time series simultaneously is difficult since the scale and shape of patterns can vary substantially. The features() function will compute single value summaries over time such as the strength of trend or seasonality. There are many features available, but the features from STL decompositions are particularly interesting.\n\naus_retail |&gt; \n  features(Turnover, feat_stl)\n\n# A tibble: 152 × 11\n   State       Industry trend_strength seasonal_strength_year seasonal_peak_year\n   &lt;chr&gt;       &lt;chr&gt;             &lt;dbl&gt;                  &lt;dbl&gt;              &lt;dbl&gt;\n 1 Australian… Cafes, …          0.989                  0.562                  0\n 2 Australian… Cafes, …          0.993                  0.629                  0\n 3 Australian… Clothin…          0.991                  0.923                  9\n 4 Australian… Clothin…          0.993                  0.957                  9\n 5 Australian… Departm…          0.977                  0.980                  9\n 6 Australian… Electri…          0.992                  0.933                  9\n 7 Australian… Food re…          0.999                  0.890                  9\n 8 Australian… Footwea…          0.982                  0.944                  9\n 9 Australian… Furnitu…          0.981                  0.687                  9\n10 Australian… Hardwar…          0.992                  0.900                  9\n# ℹ 142 more rows\n# ℹ 6 more variables: seasonal_trough_year &lt;dbl&gt;, spikiness &lt;dbl&gt;,\n#   linearity &lt;dbl&gt;, curvature &lt;dbl&gt;, stl_e_acf1 &lt;dbl&gt;, stl_e_acf10 &lt;dbl&gt;\n\n\nIn particular, features from STL decompositions allow you to compare the strength of trend and seasonality between many time series.\n\nlibrary(ggplot2)\naus_retail |&gt; \n  features(Turnover, feat_stl) |&gt; \n  ggplot(aes(x = trend_strength, y = seasonal_strength_year)) + \n  geom_point()\n\n\n\n\nFrom this we can see that almost all time series have a strong trend, while the strength of seasonality is more varied - some series have strong seasonality while others have less seasonality.\n\n\n\n\n\n\nYour turn!\n\n\n\nCalculate the STL features for the time series in the tourism dataset. Try colouring the points in the scatterplot by the purpose of travel, are some reasons more trended or seasonal than others?\n\n\nThere are many other features that you can use - check out the documentation for ?features_by_pkg. You can produce a feature set of similar features using the feature_set() function.\n\naus_retail |&gt; \n  features(Turnover, feature_set(tags = \"autocorrelation\"))\n\n# A tibble: 152 × 13\n   State      Industry  acf1 acf10 diff1_acf1 diff1_acf10 diff2_acf1 diff2_acf10\n   &lt;chr&gt;      &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n 1 Australia… Cafes, … 0.973  8.59     -0.348       0.239     -0.572       0.502\n 2 Australia… Cafes, … 0.977  8.65     -0.327       0.259     -0.552       0.531\n 3 Australia… Clothin… 0.885  7.01     -0.276       0.251     -0.507       0.339\n 4 Australia… Clothin… 0.846  6.33     -0.303       0.201     -0.532       0.321\n 5 Australia… Departm… 0.500  1.60     -0.310       0.202     -0.540       0.315\n 6 Australia… Electri… 0.902  7.29     -0.247       0.324     -0.506       0.455\n 7 Australia… Food re… 0.984  9.13     -0.394       0.585     -0.611       1.12 \n 8 Australia… Footwea… 0.760  4.64     -0.325       0.155     -0.566       0.333\n 9 Australia… Furnitu… 0.952  7.67     -0.190       0.163     -0.530       0.394\n10 Australia… Hardwar… 0.957  7.67     -0.104       0.101     -0.497       0.311\n# ℹ 142 more rows\n# ℹ 5 more variables: season_acf1 &lt;dbl&gt;, pacf5 &lt;dbl&gt;, diff1_pacf5 &lt;dbl&gt;,\n#   diff2_pacf5 &lt;dbl&gt;, season_pacf &lt;dbl&gt;"
  },
  {
    "objectID": "sessions/day3/exercises.html",
    "href": "sessions/day3/exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Learn\nMitch’s section to learn the concepts on various datasets\n\n\nApply\nIn this exercise, we first use simple models to produce forecasts of future administered vaccine doses for the next 12 months. Following that, we use regression models to produce such a forecast.\n\n\n\n\n\n\nBasic of modelling/forecating\n\n\n\n\nSpecify and train three simple models including total average, naive and seasonal naive on administered vaccine doses.\nExamine the model table (mable) object and describe what each column and row represent.\nUse report(), tidy(), glance() and augment() to explore the trained model’s output.\nProduce forecasts for 12 months ahead including both point forecast and forecast distribution.\nExamine the forecast table (fable) object and explain what each column and row represent.\nVisualize the point forecasts alongside past values, as well as prediction interval for \\(90%\\) coverage.\nExtract prediction intervals for \\(90%\\) coverage.\nProduce probabilistic forecast using bootstrapping instead of assuming normal distribution. Generate 1000 possible future.\n\n\n\n\n\n\n\n\n\nForecating using regression\n\n\n\n\nExamine the association between dose_adminstrated and predictors\n\nAssess the association between dose_adminstrated and population_under1\nAssess the association between dose_adminstrated and strike\nExamine the association between leading predictors of population_under1 and dose_adminstrated\n\nSpecify and train the four different regression models with the following terms:\n\ntrensd and seasonality\ntrensd, seasonality, and population_under1\ntrensd, seasonality, population_under1, and strike\n\nExamine trained model output using report(), tidy(), and glance() and augment()\nProduce forecast\n\nUse new_data() to generate future months corresponding to forecast horizon\nAdd future values for the strike\nAdd future values for the population_under1\nGenerate forecasts for future periods\n\nVisualize forecasts"
  },
  {
    "objectID": "sessions/day4/exercises.html",
    "href": "sessions/day4/exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Learn\nMitch’s section to learn the concepts on various datasets\n\n\nApply\nIn this exercise, you use exponetial smoothing and ARIMA models to produce forecasts of future administered vaccine doses for the next 12 months. :::{.callout-caution} ## ETS 1. Specify and train automatic Exponential Smoothing model\n\nObserve the model table and explain what each row and column represent\nExtract ETS model’s output using report(), tidy(), glance()\nExtract the components of the selected ETS model\nGenerate forecasts using ETS and observe the forecast table\nVisualize forecasts\nUse the specific functions to determine manually the components and parameters of ETS models\n\n:::\n\n\n\n\n\n\nARIMA\n\n\n\n\nSpecify and train automatic ARIMA model\nObserve the model table and explain what each row and column represent\nExtract ARIMA model’s output using report(), tidy(), glance()\nGenerate forecasts using ARIMA\nVisualize forecasts\nDetermine model components manually"
  },
  {
    "objectID": "sessions/day5/exercises.html",
    "href": "sessions/day5/exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Learn\nMitch’s section to learn the concepts on various datasets\n\n\nApply\nIn this part, we evaluate the forecast accuracy of all models we have covered so far using a simple train/test split and time series cross validation.\n\n\n\n\n\n\nBasic of train/test forecast accuracy\n\n\n\n\nSplit the data into train and test ensuring the number of months in the test set equals the forecast horizon\nSpecify and train the following models on the train data:\n\nAverage\nNaive\nSeasonal Naive\nETS\nARIMA\nRegression with trend and seasonality\nRegression with trend, seasonality, and population_under1\nRegression with trend, seasonality, population_under1, and strike\nCombination of ETS and ARIMA and regression with population and strike\n\nProduce forecasts\n\nReplace the values of population in the test set with its estimation\nProduce forecasts for dose adminstrated\n\nCompute forecast accuracy including point forecast accuracy, prediction interval and probabilistic forecasts\nVisualise the forecasts\n\n\n\n\n\n\n\n\n\nAdvanced performance evaluation\n\n\n\n\nTime series cross validation\n\nSplit the data into test and train\n\nthe size of test set equals the \\(20% (30%)\\) of the length of the time series\nthe size of test set equals the \\(80% (70%)\\) of the length of the time series\n\nApply time series cross-validation technique to create different time series rolling origins for both the train and test set\nReplace the values of population in the cross-validated test set with its estimations\nSpecify and train the following models on the cross-validated train dataset:\n\nAverage\nNaive\nSeasonal Naive\nETS\nARIMA\nRegression with trend and seasonality\nRegression with trend, seasonality, and population_under1\nRegression with trend, seasonality, population_under1, and strike\nCombination of ETS and ARIMA and regression with population and strike\n\nProduce forecasts\nCompute forecast accuracy including point forecast accuracy, prediction interval and probabilistic forecasts\n\nCompute total average forecast accuracy across all orinigs and horizons\nCompute and visualise forecast accuracy across all horizons for each origin\nCompute average forecast accuracy across all orinigs for each horizon\n\nSpecify, train and forecast using the most accurate model and visualise forecast\n\n\n\nResidual diagnostics\n\nExtract residuals from the model table from the most accurate model\nProduce the time plot of residuals from the most accurate model\nCreate the histogram of residuals from the most accurate model\nProduce the ACF plot of of residuals from the most accurate model"
  },
  {
    "objectID": "sessions/template.html",
    "href": "sessions/template.html",
    "title": "{{title}}",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "sessions/template.html#lab-sessions",
    "href": "sessions/template.html#lab-sessions",
    "title": "{{title}}",
    "section": "Lab sessions",
    "text": "Lab sessions"
  },
  {
    "objectID": "solutions/day1.html",
    "href": "solutions/day1.html",
    "title": "Lab exercise: day 1",
    "section": "",
    "text": "Let’s first import data and observe it. This dataset is located indata directory in the working directory:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tsibble)\n\n\nAttaching package: 'tsibble'\n\nThe following object is masked from 'package:lubridate':\n\n    interval\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, union\n\nlibrary(feasts)\n\nLoading required package: fabletools\n\nlibrary(lubridate)\nvaccine_administrated &lt;- read_csv(\"data/vaccine_adminstrated.csv\")\n\nRows: 972 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): month, region\ndbl (3): dose_adminstrated, population_under1, strike\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nvaccine_administrated\n\n# A tibble: 972 × 5\n   month    region dose_adminstrated population_under1 strike\n   &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;             &lt;dbl&gt;  &lt;dbl&gt;\n 1 2013 Jan A                  13328            183957      0\n 2 2013 Feb A                  12523            183957      0\n 3 2013 Mar A                  10051            183957      0\n 4 2013 Apr A                   9194            183957      0\n 5 2013 May A                  15985            183957      0\n 6 2013 Jun A                  13776            183957      0\n 7 2013 Jul A                  14258            183957      0\n 8 2013 Aug A                  13665            183957      0\n 9 2013 Sep A                  12909            183957      0\n10 2013 Oct A                  15010            183957      0\n# ℹ 962 more rows\n\n\nAfter importing your data, it’s important to double-check the structure of the data and the data types of the variables. The structure of the data you import into R must be a tibble/data frame.\nVerify the data type of each variable in vaccine_administered. We may need to modify the data type of month, and strike. Could you explain why?\n\nvaccine_administrated &lt;- vaccine_administrated |&gt; \n  mutate(month = yearmonth(month), strike = as_factor(strike))\nvaccine_administrated\n\n# A tibble: 972 × 5\n      month region dose_adminstrated population_under1 strike\n      &lt;mth&gt; &lt;chr&gt;              &lt;dbl&gt;             &lt;dbl&gt; &lt;fct&gt; \n 1 2013 Jan A                  13328            183957 0     \n 2 2013 Feb A                  12523            183957 0     \n 3 2013 Mar A                  10051            183957 0     \n 4 2013 Apr A                   9194            183957 0     \n 5 2013 May A                  15985            183957 0     \n 6 2013 Jun A                  13776            183957 0     \n 7 2013 Jul A                  14258            183957 0     \n 8 2013 Aug A                  13665            183957 0     \n 9 2013 Sep A                  12909            183957 0     \n10 2013 Oct A                  15010            183957 0     \n# ℹ 962 more rows\n\n\n\n\n\nPreparing your temporal data for time series analysis and forecasting may involve several steps, such as addressing data quality issues, handling missing values, fixing duplications, creating a tsibble, and checking/filling temporal gaps.\n\n\nIt’s essential to always check for any missing values and address them before creating a tsibble. The easiest way to know if there is a missing values is to use anyNA().\n\nanyNA(vaccine_administrated)\n\n[1] FALSE\n\n\nThe dataset you use may contain missing values (NA in R) and other data quality issues. While checking and fixing data quality issues are crucial for any project, addressing such issues goes beyond the scope of this training. For further information, you may read more about data quality issues here\n\n\n\nBefore you move forward, it’s important to consistently examine your data for any duplicated observations. If such duplications are identified, address them before moving forward.\nWrite R code to check for duplicate observations in the data.\n\nvaccine_administrated |&gt; duplicated() |&gt; sum()\n\n[1] 0\n\n\nHave you identified any duplicate observations? If your data contains no duplicates, you may proceed with creating a tsibble. Otherwise, you need to fix it.\nHow do you fix duplicated observations?\n\n\n\nYou can store your data in a tsibble format, which is suitable for time series analysis and forecasting. Most functions that you use for time series analysis and forecasting, requires your data to be in a tsibble format.\nComplete the following R chunk to create a tsibble.\n\nvaccine_administrated_tsb &lt;- vaccine_administrated |&gt;\n as_tsibble(index = month, key=region)\nvaccine_administrated_tsb\n\n# A tsibble: 972 x 5 [1M]\n# Key:       region [9]\n      month region dose_adminstrated population_under1 strike\n      &lt;mth&gt; &lt;chr&gt;              &lt;dbl&gt;             &lt;dbl&gt; &lt;fct&gt; \n 1 2013 Jan A                  13328            183957 0     \n 2 2013 Feb A                  12523            183957 0     \n 3 2013 Mar A                  10051            183957 0     \n 4 2013 Apr A                   9194            183957 0     \n 5 2013 May A                  15985            183957 0     \n 6 2013 Jun A                  13776            183957 0     \n 7 2013 Jul A                  14258            183957 0     \n 8 2013 Aug A                  13665            183957 0     \n 9 2013 Sep A                  12909            183957 0     \n10 2013 Oct A                  15010            183957 0     \n# ℹ 962 more rows\n\n\n\n\n\nAfter creating a tsibble, it’s important to check for any temporal gaps. If gaps are found, you could scan and count them. In this case, it’s crucial to fill them before proceeding. However, if you’re fortunate enough not to have any temporal gaps, no action is needed.\nComplete the following R chunk to check if there is any temporal gap in the data:\n\nhas_gaps(vaccine_administrated_tsb)#check gaps\n\n# A tibble: 9 × 2\n  region .gaps\n  &lt;chr&gt;  &lt;lgl&gt;\n1 A      FALSE\n2 B      FALSE\n3 C      FALSE\n4 D      FALSE\n5 E      FALSE\n6 G      FALSE\n7 H      FALSE\n8 I      FALSE\n9 J      FALSE\n\nscan_gaps(vaccine_administrated_tsb)# show me gaps\n\n# A tsibble: 0 x 2 [?]\n# Key:       region [0]\n# ℹ 2 variables: region &lt;chr&gt;, month &lt;mth&gt;\n\ncount_gaps(vaccine_administrated_tsb)# count gaps\n\n# A tibble: 0 × 4\n# ℹ 4 variables: region &lt;chr&gt;, .from &lt;mth&gt;, .to &lt;mth&gt;, .n &lt;int&gt;\n\n\n\n\n\n\nAt times, you may come across a dataset with temporal gaps, signifying intervals where no records exist in the temporal data. Prior to do any analysis, it’s important to identify and fill those gaps. You can use the below R code if such temporal gaps are present in the dataset.\n\n\n#If there is any gap, then fill it using fill_gaps.\n#vaccine_administrated_tsb &lt;- vaccine_administrated_tsb |&gt; fill_gaps(???)\n\n\n\n\n\nDepending on the decision your forecast will inform, you may need to manipulate your tsibble. For instance, if you’re forecasting the total dose administered in the country, or if you need to forecast quarterly doses administered for each region or for the entire country, you need to manipulate your time series frist.\n\nRemember, you can use index_by(), group_by() or group_by_key(), and summarise() to create different temporal granularity.\n\nComplete the following code to create total dose administrated in the country.\n\nvaccine_administrated_total &lt;- vaccine_administrated_tsb |&gt; \n  index_by(month) |&gt;\n  summarise(dose_adminstrated = sum(dose_adminstrated))\nvaccine_administrated_total\n\n# A tsibble: 108 x 2 [1M]\n      month dose_adminstrated\n      &lt;mth&gt;             &lt;dbl&gt;\n 1 2013 Jan            116829\n 2 2013 Feb            108938\n 3 2013 Mar             98702\n 4 2013 Apr             95553\n 5 2013 May            137861\n 6 2013 Jun            116245\n 7 2013 Jul            119031\n 8 2013 Aug            109351\n 9 2013 Sep            107001\n10 2013 Oct            115794\n# ℹ 98 more rows\n\n\nDepending on the forecasting task on hand, you may need to work with other time granularities such as quarterly time series.\nComplete the following code to create total dose administrated in the country.\n\nquarterly_vaccine_administrated &lt;- vaccine_administrated_tsb |&gt;\n  index_by(quarter = yearquarter(month)) |&gt;\n    summarise(dose_adminstrated = sum(dose_adminstrated))\nquarterly_vaccine_administrated\n\n# A tsibble: 36 x 2 [1Q]\n   quarter dose_adminstrated\n     &lt;qtr&gt;             &lt;dbl&gt;\n 1 2013 Q1            324469\n 2 2013 Q2            349659\n 3 2013 Q3            335383\n 4 2013 Q4            295517\n 5 2014 Q1            343574\n 6 2014 Q2            344160\n 7 2014 Q3            346011\n 8 2014 Q4            304819\n 9 2015 Q1            337089\n10 2015 Q2            340060\n# ℹ 26 more rows\n\n\nWhat if you need to create quarterly dose administrated in each region, write the R to achieve that.\n\nquarterly_vaccine_administrated_region &lt;- vaccine_administrated_tsb |&gt; group_by_key() |&gt; \n  index_by(quarter = yearquarter(month)) |&gt;\n    summarise(dose_adminstrated = sum(dose_adminstrated))\nquarterly_vaccine_administrated_region\n\n# A tsibble: 324 x 3 [1Q]\n# Key:       region [9]\n   region quarter dose_adminstrated\n   &lt;chr&gt;    &lt;qtr&gt;             &lt;dbl&gt;\n 1 A      2013 Q1             35902\n 2 A      2013 Q2             38955\n 3 A      2013 Q3             40832\n 4 A      2013 Q4             37056\n 5 A      2014 Q1             42289\n 6 A      2014 Q2             40084\n 7 A      2014 Q3             42577\n 8 A      2014 Q4             32617\n 9 A      2015 Q1             39745\n10 A      2015 Q2             41050\n# ℹ 314 more rows\n\n\n\n\n\n\n\nTo understand your data, you can start by producing time plot of dose administrated in 9 regions.\n\nvaccine_administrated_tsb |&gt; \n  autoplot(dose_adminstrated)\n\n\n\n\nYou can also focus on any region by filtering the region:\n\nvaccine_administrated_tsb |&gt; \n  filter(region == \"B\") |&gt; # change regions\n  autoplot(dose_adminstrated)\n\n\n\n\nDo you observe any systematic pattern in time series plots?\n\n\n\nIn time series analysis, we are looking for consistent pattern. Complete the following code to create seasonal plot to see if there is any obvious monthly consistent pattern.\n\nvaccine_administrated_tsb |&gt; \n  gg_season(dose_adminstrated)\n\n\n\n\nIt might not be easy to see the systematic pattern when you plot many time series together,instead you can first filter the time series of interest and then plot it:\n\nvaccine_administrated_tsb |&gt; \n  filter(region == \"B\") |&gt; # change regions\n  gg_season(dose_adminstrated)\n\n\n\n\nDo you observe any consistent pattern? How different the pattern is across region?\n\n\n\nYou might be also interested in observing how doses administrated within each month/quarter change over time, as well as understanding the dose administrated changes across different season. This could be plotted for each region separately . Complete the following code to create the plot for your region of interest:\n\nvaccine_administrated_tsb |&gt; \n  filter(region == \"G\") |&gt; # change regions\n  gg_subseries(dose_adminstrated)\n\n\n\n\nDo you see any pattern that has not been obvious with time plot and seasonal plot?\n\n\n\nIn forecasting, we would be interested in understanding how similar a time series is to the lags of itself. We often measure this similarity by calculating the correlation (i.e. the linear association) between a time series and its lags and then plot it.\n\nvaccine_administrated_tsb |&gt; \n  ACF(dose_adminstrated, lag_max = 12) |&gt; \n  autoplot()\n\n\n\n\nYou can also focus on any time series of interest by filtering the region:\n\nvaccine_administrated_tsb |&gt; \n  filter(region == \"B\") |&gt; # change regions\n  ACF(dose_adminstrated, lag_max = 12) |&gt; \n  autoplot()\n\n\n\n\nWhat can you say about the correlation between dose administrated with its lags?"
  },
  {
    "objectID": "solutions/day1.html#import-vaccine_adminstrated.csv-data-into-r",
    "href": "solutions/day1.html#import-vaccine_adminstrated.csv-data-into-r",
    "title": "Lab exercise: day 1",
    "section": "",
    "text": "Let’s first import data and observe it. This dataset is located indata directory in the working directory:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tsibble)\n\n\nAttaching package: 'tsibble'\n\nThe following object is masked from 'package:lubridate':\n\n    interval\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, union\n\nlibrary(feasts)\n\nLoading required package: fabletools\n\nlibrary(lubridate)\nvaccine_administrated &lt;- read_csv(\"data/vaccine_adminstrated.csv\")\n\nRows: 972 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): month, region\ndbl (3): dose_adminstrated, population_under1, strike\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nvaccine_administrated\n\n# A tibble: 972 × 5\n   month    region dose_adminstrated population_under1 strike\n   &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;             &lt;dbl&gt;  &lt;dbl&gt;\n 1 2013 Jan A                  13328            183957      0\n 2 2013 Feb A                  12523            183957      0\n 3 2013 Mar A                  10051            183957      0\n 4 2013 Apr A                   9194            183957      0\n 5 2013 May A                  15985            183957      0\n 6 2013 Jun A                  13776            183957      0\n 7 2013 Jul A                  14258            183957      0\n 8 2013 Aug A                  13665            183957      0\n 9 2013 Sep A                  12909            183957      0\n10 2013 Oct A                  15010            183957      0\n# ℹ 962 more rows\n\n\nAfter importing your data, it’s important to double-check the structure of the data and the data types of the variables. The structure of the data you import into R must be a tibble/data frame.\nVerify the data type of each variable in vaccine_administered. We may need to modify the data type of month, and strike. Could you explain why?\n\nvaccine_administrated &lt;- vaccine_administrated |&gt; \n  mutate(month = yearmonth(month), strike = as_factor(strike))\nvaccine_administrated\n\n# A tibble: 972 × 5\n      month region dose_adminstrated population_under1 strike\n      &lt;mth&gt; &lt;chr&gt;              &lt;dbl&gt;             &lt;dbl&gt; &lt;fct&gt; \n 1 2013 Jan A                  13328            183957 0     \n 2 2013 Feb A                  12523            183957 0     \n 3 2013 Mar A                  10051            183957 0     \n 4 2013 Apr A                   9194            183957 0     \n 5 2013 May A                  15985            183957 0     \n 6 2013 Jun A                  13776            183957 0     \n 7 2013 Jul A                  14258            183957 0     \n 8 2013 Aug A                  13665            183957 0     \n 9 2013 Sep A                  12909            183957 0     \n10 2013 Oct A                  15010            183957 0     \n# ℹ 962 more rows"
  },
  {
    "objectID": "solutions/day1.html#prepare-your-data",
    "href": "solutions/day1.html#prepare-your-data",
    "title": "Lab exercise: day 1",
    "section": "",
    "text": "Preparing your temporal data for time series analysis and forecasting may involve several steps, such as addressing data quality issues, handling missing values, fixing duplications, creating a tsibble, and checking/filling temporal gaps.\n\n\nIt’s essential to always check for any missing values and address them before creating a tsibble. The easiest way to know if there is a missing values is to use anyNA().\n\nanyNA(vaccine_administrated)\n\n[1] FALSE\n\n\nThe dataset you use may contain missing values (NA in R) and other data quality issues. While checking and fixing data quality issues are crucial for any project, addressing such issues goes beyond the scope of this training. For further information, you may read more about data quality issues here\n\n\n\nBefore you move forward, it’s important to consistently examine your data for any duplicated observations. If such duplications are identified, address them before moving forward.\nWrite R code to check for duplicate observations in the data.\n\nvaccine_administrated |&gt; duplicated() |&gt; sum()\n\n[1] 0\n\n\nHave you identified any duplicate observations? If your data contains no duplicates, you may proceed with creating a tsibble. Otherwise, you need to fix it.\nHow do you fix duplicated observations?\n\n\n\nYou can store your data in a tsibble format, which is suitable for time series analysis and forecasting. Most functions that you use for time series analysis and forecasting, requires your data to be in a tsibble format.\nComplete the following R chunk to create a tsibble.\n\nvaccine_administrated_tsb &lt;- vaccine_administrated |&gt;\n as_tsibble(index = month, key=region)\nvaccine_administrated_tsb\n\n# A tsibble: 972 x 5 [1M]\n# Key:       region [9]\n      month region dose_adminstrated population_under1 strike\n      &lt;mth&gt; &lt;chr&gt;              &lt;dbl&gt;             &lt;dbl&gt; &lt;fct&gt; \n 1 2013 Jan A                  13328            183957 0     \n 2 2013 Feb A                  12523            183957 0     \n 3 2013 Mar A                  10051            183957 0     \n 4 2013 Apr A                   9194            183957 0     \n 5 2013 May A                  15985            183957 0     \n 6 2013 Jun A                  13776            183957 0     \n 7 2013 Jul A                  14258            183957 0     \n 8 2013 Aug A                  13665            183957 0     \n 9 2013 Sep A                  12909            183957 0     \n10 2013 Oct A                  15010            183957 0     \n# ℹ 962 more rows\n\n\n\n\n\nAfter creating a tsibble, it’s important to check for any temporal gaps. If gaps are found, you could scan and count them. In this case, it’s crucial to fill them before proceeding. However, if you’re fortunate enough not to have any temporal gaps, no action is needed.\nComplete the following R chunk to check if there is any temporal gap in the data:\n\nhas_gaps(vaccine_administrated_tsb)#check gaps\n\n# A tibble: 9 × 2\n  region .gaps\n  &lt;chr&gt;  &lt;lgl&gt;\n1 A      FALSE\n2 B      FALSE\n3 C      FALSE\n4 D      FALSE\n5 E      FALSE\n6 G      FALSE\n7 H      FALSE\n8 I      FALSE\n9 J      FALSE\n\nscan_gaps(vaccine_administrated_tsb)# show me gaps\n\n# A tsibble: 0 x 2 [?]\n# Key:       region [0]\n# ℹ 2 variables: region &lt;chr&gt;, month &lt;mth&gt;\n\ncount_gaps(vaccine_administrated_tsb)# count gaps\n\n# A tibble: 0 × 4\n# ℹ 4 variables: region &lt;chr&gt;, .from &lt;mth&gt;, .to &lt;mth&gt;, .n &lt;int&gt;\n\n\n\n\n\n\nAt times, you may come across a dataset with temporal gaps, signifying intervals where no records exist in the temporal data. Prior to do any analysis, it’s important to identify and fill those gaps. You can use the below R code if such temporal gaps are present in the dataset.\n\n\n#If there is any gap, then fill it using fill_gaps.\n#vaccine_administrated_tsb &lt;- vaccine_administrated_tsb |&gt; fill_gaps(???)"
  },
  {
    "objectID": "solutions/day1.html#manipulating-time-series",
    "href": "solutions/day1.html#manipulating-time-series",
    "title": "Lab exercise: day 1",
    "section": "",
    "text": "Depending on the decision your forecast will inform, you may need to manipulate your tsibble. For instance, if you’re forecasting the total dose administered in the country, or if you need to forecast quarterly doses administered for each region or for the entire country, you need to manipulate your time series frist.\n\nRemember, you can use index_by(), group_by() or group_by_key(), and summarise() to create different temporal granularity.\n\nComplete the following code to create total dose administrated in the country.\n\nvaccine_administrated_total &lt;- vaccine_administrated_tsb |&gt; \n  index_by(month) |&gt;\n  summarise(dose_adminstrated = sum(dose_adminstrated))\nvaccine_administrated_total\n\n# A tsibble: 108 x 2 [1M]\n      month dose_adminstrated\n      &lt;mth&gt;             &lt;dbl&gt;\n 1 2013 Jan            116829\n 2 2013 Feb            108938\n 3 2013 Mar             98702\n 4 2013 Apr             95553\n 5 2013 May            137861\n 6 2013 Jun            116245\n 7 2013 Jul            119031\n 8 2013 Aug            109351\n 9 2013 Sep            107001\n10 2013 Oct            115794\n# ℹ 98 more rows\n\n\nDepending on the forecasting task on hand, you may need to work with other time granularities such as quarterly time series.\nComplete the following code to create total dose administrated in the country.\n\nquarterly_vaccine_administrated &lt;- vaccine_administrated_tsb |&gt;\n  index_by(quarter = yearquarter(month)) |&gt;\n    summarise(dose_adminstrated = sum(dose_adminstrated))\nquarterly_vaccine_administrated\n\n# A tsibble: 36 x 2 [1Q]\n   quarter dose_adminstrated\n     &lt;qtr&gt;             &lt;dbl&gt;\n 1 2013 Q1            324469\n 2 2013 Q2            349659\n 3 2013 Q3            335383\n 4 2013 Q4            295517\n 5 2014 Q1            343574\n 6 2014 Q2            344160\n 7 2014 Q3            346011\n 8 2014 Q4            304819\n 9 2015 Q1            337089\n10 2015 Q2            340060\n# ℹ 26 more rows\n\n\nWhat if you need to create quarterly dose administrated in each region, write the R to achieve that.\n\nquarterly_vaccine_administrated_region &lt;- vaccine_administrated_tsb |&gt; group_by_key() |&gt; \n  index_by(quarter = yearquarter(month)) |&gt;\n    summarise(dose_adminstrated = sum(dose_adminstrated))\nquarterly_vaccine_administrated_region\n\n# A tsibble: 324 x 3 [1Q]\n# Key:       region [9]\n   region quarter dose_adminstrated\n   &lt;chr&gt;    &lt;qtr&gt;             &lt;dbl&gt;\n 1 A      2013 Q1             35902\n 2 A      2013 Q2             38955\n 3 A      2013 Q3             40832\n 4 A      2013 Q4             37056\n 5 A      2014 Q1             42289\n 6 A      2014 Q2             40084\n 7 A      2014 Q3             42577\n 8 A      2014 Q4             32617\n 9 A      2015 Q1             39745\n10 A      2015 Q2             41050\n# ℹ 314 more rows"
  },
  {
    "objectID": "solutions/day1.html#visualising-time-series",
    "href": "solutions/day1.html#visualising-time-series",
    "title": "Lab exercise: day 1",
    "section": "",
    "text": "To understand your data, you can start by producing time plot of dose administrated in 9 regions.\n\nvaccine_administrated_tsb |&gt; \n  autoplot(dose_adminstrated)\n\n\n\n\nYou can also focus on any region by filtering the region:\n\nvaccine_administrated_tsb |&gt; \n  filter(region == \"B\") |&gt; # change regions\n  autoplot(dose_adminstrated)\n\n\n\n\nDo you observe any systematic pattern in time series plots?\n\n\n\nIn time series analysis, we are looking for consistent pattern. Complete the following code to create seasonal plot to see if there is any obvious monthly consistent pattern.\n\nvaccine_administrated_tsb |&gt; \n  gg_season(dose_adminstrated)\n\n\n\n\nIt might not be easy to see the systematic pattern when you plot many time series together,instead you can first filter the time series of interest and then plot it:\n\nvaccine_administrated_tsb |&gt; \n  filter(region == \"B\") |&gt; # change regions\n  gg_season(dose_adminstrated)\n\n\n\n\nDo you observe any consistent pattern? How different the pattern is across region?\n\n\n\nYou might be also interested in observing how doses administrated within each month/quarter change over time, as well as understanding the dose administrated changes across different season. This could be plotted for each region separately . Complete the following code to create the plot for your region of interest:\n\nvaccine_administrated_tsb |&gt; \n  filter(region == \"G\") |&gt; # change regions\n  gg_subseries(dose_adminstrated)\n\n\n\n\nDo you see any pattern that has not been obvious with time plot and seasonal plot?\n\n\n\nIn forecasting, we would be interested in understanding how similar a time series is to the lags of itself. We often measure this similarity by calculating the correlation (i.e. the linear association) between a time series and its lags and then plot it.\n\nvaccine_administrated_tsb |&gt; \n  ACF(dose_adminstrated, lag_max = 12) |&gt; \n  autoplot()\n\n\n\n\nYou can also focus on any time series of interest by filtering the region:\n\nvaccine_administrated_tsb |&gt; \n  filter(region == \"B\") |&gt; # change regions\n  ACF(dose_adminstrated, lag_max = 12) |&gt; \n  autoplot()\n\n\n\n\nWhat can you say about the correlation between dose administrated with its lags?"
  },
  {
    "objectID": "solutions/day2.html",
    "href": "solutions/day2.html",
    "title": "Lab exercise: day 2",
    "section": "",
    "text": "library(tsibble)\n\n\nAttaching package: 'tsibble'\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, union\n\nlibrary(feasts)\n\nLoading required package: fabletools\n\nlibrary(fabletools)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()       masks stats::filter()\n✖ lubridate::interval() masks tsibble::interval()\n✖ dplyr::lag()          masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nvaccine_administrated_tsb &lt;- read_rds(\"data/vaccine_administrated_tsb.rds\")\n\n\n\n\nstl_decom &lt;- vaccine_administrated_tsb |&gt;\n  filter(region == \"H\") |&gt; \n  model(\n    STL(dose_adminstrated ~ trend(window = 12) +\n                   season(window = \"periodic\")))\nstl_decom\n\n# A mable: 1 x 2\n# Key:     region [1]\n  region STL(dose_adminstrated ~ trend(window = 12) + season(window = \"periodi…¹\n  &lt;chr&gt;                                                                  &lt;model&gt;\n1 H                                                                        &lt;STL&gt;\n# ℹ abbreviated name:\n#   ¹​`STL(dose_adminstrated ~ trend(window = 12) + season(window = \"periodic\"))`\n\n\nComplete the code below to plot the components produced by STL:\n\nstl_decom |&gt; \n  components() |&gt;\n  autoplot()\n\n\n\n\nDescribe the result of the decomposition from the above plot.\n\nstl_decom |&gt; components() |&gt; \n  ggplot(aes(x = month)) +\n  geom_line(aes(y = dose_adminstrated, colour = \"Data\")) +\n  geom_line(aes(y = season_adjust,\n                colour = \"Seasonally Adjusted\")) +\n  geom_line(aes(y = trend, colour = \"Trend\")) +\n  labs(y = \"Month\",\n       title = \"Vaccine dose adminstrated\") +\n  scale_colour_manual(\n    values = c(\"gray\", \"#0072B2\", \"#D55E00\"),\n    breaks = c(\"Data\", \"Seasonally Adjusted\", \"Trend\")\n  )\n\n\n\n\n\n\n\nYou can start by calculating some simple features including average (mean) and standard deviation (sd). You also need to know how to calculate the coefficient of variation. Complete the following code to do that:\n\nvaccine_administrated_tsb |&gt;\n  features(dose_adminstrated, list(average = mean, standard_deviation = sd)) |&gt; mutate( coefficient_of_variation = standard_deviation/average)\n\n# A tibble: 9 × 4\n  region average standard_deviation coefficient_of_variation\n  &lt;chr&gt;    &lt;dbl&gt;              &lt;dbl&gt;                    &lt;dbl&gt;\n1 A       12875.              1791.                   0.139 \n2 B        4896.              1127.                   0.230 \n3 C       12860.              2192.                   0.170 \n4 D       15670.              2367.                   0.151 \n5 E        5339.               594.                   0.111 \n6 G       11153.              1987.                   0.178 \n7 H       30644.              2390.                   0.0780\n8 I       12735.              1501.                   0.118 \n9 J        4801.               369.                   0.0768\n\n\nYou can also use feasts package to include about 48 different features providing various numerical summaries of time series. Complete the following code to compute those features for the monthly vaccine dose adminstrated:\n\nvaccine_administrated_features &lt;- vaccine_administrated_tsb |&gt;\n  features(dose_adminstrated,feature_set(pkgs = \"feasts\"))\n\nWarning: `n_flat_spots()` was deprecated in feasts 0.1.5.\nℹ Please use `longest_flat_spot()` instead.\nℹ The deprecated feature was likely used in the fabletools package.\n  Please report the issue at &lt;https://github.com/tidyverts/fabletools/issues&gt;.\n\nvaccine_administrated_features\n\n# A tibble: 9 × 49\n  region trend_strength seasonal_strength_year seasonal_peak_year\n  &lt;chr&gt;           &lt;dbl&gt;                  &lt;dbl&gt;              &lt;dbl&gt;\n1 A               0.163                  0.493                  5\n2 B               0.842                  0.268                  3\n3 C               0.263                  0.366                  5\n4 D               0.249                  0.405                  5\n5 E               0.328                  0.371                  5\n6 G               0.248                  0.685                  3\n7 H               0.575                  0.445                  5\n8 I               0.119                  0.370                  5\n9 J               0.582                  0.489                  5\n# ℹ 45 more variables: seasonal_trough_year &lt;dbl&gt;, spikiness &lt;dbl&gt;,\n#   linearity &lt;dbl&gt;, curvature &lt;dbl&gt;, stl_e_acf1 &lt;dbl&gt;, stl_e_acf10 &lt;dbl&gt;,\n#   acf1 &lt;dbl&gt;, acf10 &lt;dbl&gt;, diff1_acf1 &lt;dbl&gt;, diff1_acf10 &lt;dbl&gt;,\n#   diff2_acf1 &lt;dbl&gt;, diff2_acf10 &lt;dbl&gt;, season_acf1 &lt;dbl&gt;, pacf5 &lt;dbl&gt;,\n#   diff1_pacf5 &lt;dbl&gt;, diff2_pacf5 &lt;dbl&gt;, season_pacf &lt;dbl&gt;,\n#   zero_run_mean &lt;dbl&gt;, nonzero_squared_cv &lt;dbl&gt;, zero_start_prop &lt;dbl&gt;,\n#   zero_end_prop &lt;dbl&gt;, lambda_guerrero &lt;dbl&gt;, kpss_stat &lt;dbl&gt;, …\n\n\n\nThe dataset of administered vaccine doses currently comprises only nine time series. Nevertheless, there are cases where datasets may includes hundreds or even thousands of time series. The method applied here can be replicated with datasets of varying sizes, including those with thousands of time series.\n\n\n\n\nCreate a scatterplot to show the strength of trend and seasonality features:\n\nggplot(data = vaccine_administrated_features, \n       mapping = aes(x = trend_strength, y = seasonal_strength_year)) +\n  geom_point()\n\n\n\n\nUsing a feature indicating the level of forecast difficulty or ease for a given time series, generate a histogram to visualize the distribution of forecastability within the dataset.\n\nggplot(data = vaccine_administrated_features, \n       mapping = aes(spectral_entropy)) +\n  geom_density(fill=\"lightblue\")\n\n\n\n\n\nThis distribution would make more sense when you deal with a dataset containing hundreds or thousands of time series."
  },
  {
    "objectID": "solutions/day2.html#decomposition",
    "href": "solutions/day2.html#decomposition",
    "title": "Lab exercise: day 2",
    "section": "",
    "text": "stl_decom &lt;- vaccine_administrated_tsb |&gt;\n  filter(region == \"H\") |&gt; \n  model(\n    STL(dose_adminstrated ~ trend(window = 12) +\n                   season(window = \"periodic\")))\nstl_decom\n\n# A mable: 1 x 2\n# Key:     region [1]\n  region STL(dose_adminstrated ~ trend(window = 12) + season(window = \"periodi…¹\n  &lt;chr&gt;                                                                  &lt;model&gt;\n1 H                                                                        &lt;STL&gt;\n# ℹ abbreviated name:\n#   ¹​`STL(dose_adminstrated ~ trend(window = 12) + season(window = \"periodic\"))`\n\n\nComplete the code below to plot the components produced by STL:\n\nstl_decom |&gt; \n  components() |&gt;\n  autoplot()\n\n\n\n\nDescribe the result of the decomposition from the above plot.\n\nstl_decom |&gt; components() |&gt; \n  ggplot(aes(x = month)) +\n  geom_line(aes(y = dose_adminstrated, colour = \"Data\")) +\n  geom_line(aes(y = season_adjust,\n                colour = \"Seasonally Adjusted\")) +\n  geom_line(aes(y = trend, colour = \"Trend\")) +\n  labs(y = \"Month\",\n       title = \"Vaccine dose adminstrated\") +\n  scale_colour_manual(\n    values = c(\"gray\", \"#0072B2\", \"#D55E00\"),\n    breaks = c(\"Data\", \"Seasonally Adjusted\", \"Trend\")\n  )"
  },
  {
    "objectID": "solutions/day2.html#computing-features",
    "href": "solutions/day2.html#computing-features",
    "title": "Lab exercise: day 2",
    "section": "",
    "text": "You can start by calculating some simple features including average (mean) and standard deviation (sd). You also need to know how to calculate the coefficient of variation. Complete the following code to do that:\n\nvaccine_administrated_tsb |&gt;\n  features(dose_adminstrated, list(average = mean, standard_deviation = sd)) |&gt; mutate( coefficient_of_variation = standard_deviation/average)\n\n# A tibble: 9 × 4\n  region average standard_deviation coefficient_of_variation\n  &lt;chr&gt;    &lt;dbl&gt;              &lt;dbl&gt;                    &lt;dbl&gt;\n1 A       12875.              1791.                   0.139 \n2 B        4896.              1127.                   0.230 \n3 C       12860.              2192.                   0.170 \n4 D       15670.              2367.                   0.151 \n5 E        5339.               594.                   0.111 \n6 G       11153.              1987.                   0.178 \n7 H       30644.              2390.                   0.0780\n8 I       12735.              1501.                   0.118 \n9 J        4801.               369.                   0.0768\n\n\nYou can also use feasts package to include about 48 different features providing various numerical summaries of time series. Complete the following code to compute those features for the monthly vaccine dose adminstrated:\n\nvaccine_administrated_features &lt;- vaccine_administrated_tsb |&gt;\n  features(dose_adminstrated,feature_set(pkgs = \"feasts\"))\n\nWarning: `n_flat_spots()` was deprecated in feasts 0.1.5.\nℹ Please use `longest_flat_spot()` instead.\nℹ The deprecated feature was likely used in the fabletools package.\n  Please report the issue at &lt;https://github.com/tidyverts/fabletools/issues&gt;.\n\nvaccine_administrated_features\n\n# A tibble: 9 × 49\n  region trend_strength seasonal_strength_year seasonal_peak_year\n  &lt;chr&gt;           &lt;dbl&gt;                  &lt;dbl&gt;              &lt;dbl&gt;\n1 A               0.163                  0.493                  5\n2 B               0.842                  0.268                  3\n3 C               0.263                  0.366                  5\n4 D               0.249                  0.405                  5\n5 E               0.328                  0.371                  5\n6 G               0.248                  0.685                  3\n7 H               0.575                  0.445                  5\n8 I               0.119                  0.370                  5\n9 J               0.582                  0.489                  5\n# ℹ 45 more variables: seasonal_trough_year &lt;dbl&gt;, spikiness &lt;dbl&gt;,\n#   linearity &lt;dbl&gt;, curvature &lt;dbl&gt;, stl_e_acf1 &lt;dbl&gt;, stl_e_acf10 &lt;dbl&gt;,\n#   acf1 &lt;dbl&gt;, acf10 &lt;dbl&gt;, diff1_acf1 &lt;dbl&gt;, diff1_acf10 &lt;dbl&gt;,\n#   diff2_acf1 &lt;dbl&gt;, diff2_acf10 &lt;dbl&gt;, season_acf1 &lt;dbl&gt;, pacf5 &lt;dbl&gt;,\n#   diff1_pacf5 &lt;dbl&gt;, diff2_pacf5 &lt;dbl&gt;, season_pacf &lt;dbl&gt;,\n#   zero_run_mean &lt;dbl&gt;, nonzero_squared_cv &lt;dbl&gt;, zero_start_prop &lt;dbl&gt;,\n#   zero_end_prop &lt;dbl&gt;, lambda_guerrero &lt;dbl&gt;, kpss_stat &lt;dbl&gt;, …\n\n\n\nThe dataset of administered vaccine doses currently comprises only nine time series. Nevertheless, there are cases where datasets may includes hundreds or even thousands of time series. The method applied here can be replicated with datasets of varying sizes, including those with thousands of time series."
  },
  {
    "objectID": "solutions/day2.html#visualizing-features",
    "href": "solutions/day2.html#visualizing-features",
    "title": "Lab exercise: day 2",
    "section": "",
    "text": "Create a scatterplot to show the strength of trend and seasonality features:\n\nggplot(data = vaccine_administrated_features, \n       mapping = aes(x = trend_strength, y = seasonal_strength_year)) +\n  geom_point()\n\n\n\n\nUsing a feature indicating the level of forecast difficulty or ease for a given time series, generate a histogram to visualize the distribution of forecastability within the dataset.\n\nggplot(data = vaccine_administrated_features, \n       mapping = aes(spectral_entropy)) +\n  geom_density(fill=\"lightblue\")\n\n\n\n\n\nThis distribution would make more sense when you deal with a dataset containing hundreds or thousands of time series."
  },
  {
    "objectID": "solutions/day3.html#basic-of-modellingforecasting",
    "href": "solutions/day3.html#basic-of-modellingforecasting",
    "title": "Lab exercise: day 3",
    "section": "Basic of modelling/forecasting",
    "text": "Basic of modelling/forecasting\n\nSpecify models and train models\nWe start with three simple benchmark models: i) total average, ii) naive, and iii) seasonal naive\n\nWe specify models using the function corresponding to the name of the forecasting model. We use a formula (response ~ terms) to specify models and train models (i.e. estimate parameters) using model(response ~ terms) function. If there is no term for the method, we ignore the ~ and terms, (e.g. MEAN(dose_adminstrated)):\n\nNow, complete the following R chunk to specify and train the there simple models on data:\n\nvaccine_fit &lt;- vaccine_administrated_tsb |&gt;\n  model(\n    mean = MEAN(dose_adminstrated),\n    naive = SNAIVE(dose_adminstrated),\n    snaive = SNAIVE(dose_adminstrated),\n  )\nvaccine_fit\n\n# A mable: 9 x 4\n# Key:     region [9]\n  region    mean    naive   snaive\n  &lt;chr&gt;  &lt;model&gt;  &lt;model&gt;  &lt;model&gt;\n1 A       &lt;MEAN&gt; &lt;SNAIVE&gt; &lt;SNAIVE&gt;\n2 B       &lt;MEAN&gt; &lt;SNAIVE&gt; &lt;SNAIVE&gt;\n3 C       &lt;MEAN&gt; &lt;SNAIVE&gt; &lt;SNAIVE&gt;\n4 D       &lt;MEAN&gt; &lt;SNAIVE&gt; &lt;SNAIVE&gt;\n5 E       &lt;MEAN&gt; &lt;SNAIVE&gt; &lt;SNAIVE&gt;\n6 G       &lt;MEAN&gt; &lt;SNAIVE&gt; &lt;SNAIVE&gt;\n7 H       &lt;MEAN&gt; &lt;SNAIVE&gt; &lt;SNAIVE&gt;\n8 I       &lt;MEAN&gt; &lt;SNAIVE&gt; &lt;SNAIVE&gt;\n9 J       &lt;MEAN&gt; &lt;SNAIVE&gt; &lt;SNAIVE&gt;\n\n\nObserve the vaccine_fit object.\nWhat type of data structure is it? How many rows and columns are present, and what do they represent?\n\n\nExtract fitted values and residuals\nYou can extract fitted values and residuals for each model. Complete the following code to extract those values for all models\n\nvaccine_fit |&gt; augment()\n\n# A tsibble: 2,916 x 7 [1M]\n# Key:       region, .model [27]\n   region .model    month dose_adminstrated .fitted  .resid  .innov\n   &lt;chr&gt;  &lt;chr&gt;     &lt;mth&gt;             &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 A      mean   2013 Jan             13328  12875.   453.    453. \n 2 A      mean   2013 Feb             12523  12875.  -352.   -352. \n 3 A      mean   2013 Mar             10051  12875. -2824.  -2824. \n 4 A      mean   2013 Apr              9194  12875. -3681.  -3681. \n 5 A      mean   2013 May             15985  12875.  3110.   3110. \n 6 A      mean   2013 Jun             13776  12875.   901.    901. \n 7 A      mean   2013 Jul             14258  12875.  1383.   1383. \n 8 A      mean   2013 Aug             13665  12875.   790.    790. \n 9 A      mean   2013 Sep             12909  12875.    33.9    33.9\n10 A      mean   2013 Oct             15010  12875.  2135.   2135. \n# ℹ 2,906 more rows\n\n\nYou can use filter() to extract these values for a specific model.\nComplete the following code to see only results for naive model:\n\nvaccine_fit |&gt; augment() |&gt; filter(.model==\"naive\")\n\n# A tsibble: 972 x 7 [1M]\n# Key:       region, .model [9]\n   region .model    month dose_adminstrated .fitted .resid .innov\n   &lt;chr&gt;  &lt;chr&gt;     &lt;mth&gt;             &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 A      naive  2013 Jan             13328      NA     NA     NA\n 2 A      naive  2013 Feb             12523      NA     NA     NA\n 3 A      naive  2013 Mar             10051      NA     NA     NA\n 4 A      naive  2013 Apr              9194      NA     NA     NA\n 5 A      naive  2013 May             15985      NA     NA     NA\n 6 A      naive  2013 Jun             13776      NA     NA     NA\n 7 A      naive  2013 Jul             14258      NA     NA     NA\n 8 A      naive  2013 Aug             13665      NA     NA     NA\n 9 A      naive  2013 Sep             12909      NA     NA     NA\n10 A      naive  2013 Oct             15010      NA     NA     NA\n# ℹ 962 more rows\n\n\nYou can use select() to get fitted values or residuals. Complete the R code to see residuals for the naive method:\n\nvaccine_fit |&gt; augment() |&gt; filter(.model==\"naive\") |&gt; select(.resid)\n\n# A tsibble: 972 x 4 [1M]\n# Key:       region, .model [9]\n   .resid    month region .model\n    &lt;dbl&gt;    &lt;mth&gt; &lt;chr&gt;  &lt;chr&gt; \n 1     NA 2013 Jan A      naive \n 2     NA 2013 Feb A      naive \n 3     NA 2013 Mar A      naive \n 4     NA 2013 Apr A      naive \n 5     NA 2013 May A      naive \n 6     NA 2013 Jun A      naive \n 7     NA 2013 Jul A      naive \n 8     NA 2013 Aug A      naive \n 9     NA 2013 Sep A      naive \n10     NA 2013 Oct A      naive \n# ℹ 962 more rows\n\n\n\nWe can look into more details of the trained models (mable) using tidy(), report(), glance() and extract information related to trained models. These function would be more useful with models like Regression, exponential smoothing (ETS) and ARIMA and we use them later once these models are introduced.\n\n\n\nProduce forecast\n\nIn order to produce forecasts, we pass the mable object, vaccine_fit, to the forecast() function and specify the forecast horizon required. This will generate both point forecast and a distribution of forecasts based on Normal distribution.\n\nComplete the following R code to produce forecasts of dose administrated for 12 months ahead:\n\nforecast_horizon &lt;- 12\nvaccine_forecast &lt;- vaccine_fit |&gt; forecast(h=forecast_horizon)\nvaccine_forecast\n\n# A fable: 324 x 5 [1M]\n# Key:     region, .model [27]\n   region .model    month dose_adminstrated  .mean\n   &lt;chr&gt;  &lt;chr&gt;     &lt;mth&gt;            &lt;dist&gt;  &lt;dbl&gt;\n 1 A      mean   2022 Jan N(12875, 3236838) 12875.\n 2 A      mean   2022 Feb N(12875, 3236838) 12875.\n 3 A      mean   2022 Mar N(12875, 3236838) 12875.\n 4 A      mean   2022 Apr N(12875, 3236838) 12875.\n 5 A      mean   2022 May N(12875, 3236838) 12875.\n 6 A      mean   2022 Jun N(12875, 3236838) 12875.\n 7 A      mean   2022 Jul N(12875, 3236838) 12875.\n 8 A      mean   2022 Aug N(12875, 3236838) 12875.\n 9 A      mean   2022 Sep N(12875, 3236838) 12875.\n10 A      mean   2022 Oct N(12875, 3236838) 12875.\n# ℹ 314 more rows\n\n\nObserve the vaccine_forecast object.\nWhat type of data structure is it? How many rows and columns are present, and what do they represent?\n\n\nVisualise foreacasts\nWe can also plot generated forecasts using autoplot(). Complete the following R code to plot the forecasts:\n\nvaccine_forecast |&gt; \n  autoplot()\n\n\n\n\nVisualizing forecasts alone might not be as informative, it is generally useful to plot it in conjunction with past data.\nComplete the following R code to include past administered dose data along with its forecast for the next 12 months:\n\nvaccine_forecast |&gt; \n  autoplot(vaccine_administrated_tsb, level=NULL)\n\n\n\n\nWhat the argument level=NULL does? and what happens if you remove it?\nIt might be hard to see the forecast lines in the above plot. To make forecasts more visible, we can plot a part of the time series data towards the end of the time series. You can usefilter_index() or tail() for that.\nComplete the following code to see past data from 2020 until the end and its forecasts:\n\nvaccine_forecast |&gt; autoplot(filter_index(vaccine_administrated_tsb,\"2020\"~.), level=NULL)\n\nWarning: There was 1 warning in `filter()`.\nℹ In argument: `time_in(month, ...)`.\nCaused by warning:\n! `yearmonth()` may yield unexpected results.\nℹ Please use arg `format` to supply formats.\n\n\n\n\n\n\n\nExtract prediction intervals\nYou may want to extract prediction intervals for any coverage probability you are interested in.\nComplete the R code to achieve that:\n\nvaccine_forecast_interval &lt;- vaccine_forecast |&gt; hilo(level = 90)\nvaccine_forecast_interval\n\n# A tsibble: 324 x 6 [1M]\n# Key:       region, .model [27]\n   region .model    month dose_adminstrated  .mean                  `90%`\n   &lt;chr&gt;  &lt;chr&gt;     &lt;mth&gt;            &lt;dist&gt;  &lt;dbl&gt;                 &lt;hilo&gt;\n 1 A      mean   2022 Jan N(12875, 3236838) 12875. [9915.857, 15834.44]90\n 2 A      mean   2022 Feb N(12875, 3236838) 12875. [9915.857, 15834.44]90\n 3 A      mean   2022 Mar N(12875, 3236838) 12875. [9915.857, 15834.44]90\n 4 A      mean   2022 Apr N(12875, 3236838) 12875. [9915.857, 15834.44]90\n 5 A      mean   2022 May N(12875, 3236838) 12875. [9915.857, 15834.44]90\n 6 A      mean   2022 Jun N(12875, 3236838) 12875. [9915.857, 15834.44]90\n 7 A      mean   2022 Jul N(12875, 3236838) 12875. [9915.857, 15834.44]90\n 8 A      mean   2022 Aug N(12875, 3236838) 12875. [9915.857, 15834.44]90\n 9 A      mean   2022 Sep N(12875, 3236838) 12875. [9915.857, 15834.44]90\n10 A      mean   2022 Oct N(12875, 3236838) 12875. [9915.857, 15834.44]90\n# ℹ 314 more rows\n\n\nTo be able to see values for lower bound and upper bound in separate columns, you need to unpack the prediction intervals extracted above.\n\nvaccine_forecast_interval |&gt; unpack_hilo(\"90%\")\n\n# A tsibble: 324 x 7 [1M]\n# Key:       region, .model [27]\n   region .model    month dose_adminstrated  .mean `90%_lower` `90%_upper`\n   &lt;chr&gt;  &lt;chr&gt;     &lt;mth&gt;            &lt;dist&gt;  &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 A      mean   2022 Jan N(12875, 3236838) 12875.       9916.      15834.\n 2 A      mean   2022 Feb N(12875, 3236838) 12875.       9916.      15834.\n 3 A      mean   2022 Mar N(12875, 3236838) 12875.       9916.      15834.\n 4 A      mean   2022 Apr N(12875, 3236838) 12875.       9916.      15834.\n 5 A      mean   2022 May N(12875, 3236838) 12875.       9916.      15834.\n 6 A      mean   2022 Jun N(12875, 3236838) 12875.       9916.      15834.\n 7 A      mean   2022 Jul N(12875, 3236838) 12875.       9916.      15834.\n 8 A      mean   2022 Aug N(12875, 3236838) 12875.       9916.      15834.\n 9 A      mean   2022 Sep N(12875, 3236838) 12875.       9916.      15834.\n10 A      mean   2022 Oct N(12875, 3236838) 12875.       9916.      15834.\n# ℹ 314 more rows\n\n\nYou may want to extract forecast you generated into Excel, it is easy to do it using write_csv() or\n\n\nProduce probabilistic forecast using bootstrapping\nMost time series models produce normally distributed forecasts, that is, we assume that the distribution of possible future values follows a normal distribution.\nWhen a normal distribution for the residuals is an unreasonable assumption, one alternative is to use bootstrapping, which only assumes that the residuals are uncorrelated with constant variance. You can also use forecast() directly to generate futures:\n\nAttention: Producing forecasts using bootstrapping may take time!! so, running time might be an issue if you have many time series.\n\n\nfc_bootstrap &lt;- vaccine_fit |&gt;\n  forecast(h = forecast_horizon, bootstrap = TRUE, times = 1000)\n\nYou can use generate() function to generate futures using bootstrapping. Here we do it for one model only:\n\nfit &lt;- vaccine_administrated_tsb |&gt;\n  model(naive=NAIVE(dose_adminstrated))\nsim_bootstrap &lt;- fit |&gt; generate(h = forecast_horizon, times = 1000, bootstrap = TRUE)\nsim_bootstrap\n\n# A tsibble: 108,000 x 6 [1M]\n# Key:       region, .model, .rep [9,000]\n   region .model .rep     month .innov   .sim\n   &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;    &lt;mth&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 A      naive  1     2022 Jan  3255. 16131.\n 2 A      naive  1     2022 Feb  1867. 17998.\n 3 A      naive  1     2022 Mar  4513. 22512.\n 4 A      naive  1     2022 Apr -1961. 20551.\n 5 A      naive  1     2022 May  3358. 23909.\n 6 A      naive  1     2022 Jun   135. 24044.\n 7 A      naive  1     2022 Jul  1956. 26001.\n 8 A      naive  1     2022 Aug  -975. 25026.\n 9 A      naive  1     2022 Sep -2000. 23026.\n10 A      naive  1     2022 Oct  -438. 22588.\n# ℹ 107,990 more rows\n\n\nCould you describe what columns and rows represent in sim_bootstrap?"
  },
  {
    "objectID": "solutions/day3.html#regression",
    "href": "solutions/day3.html#regression",
    "title": "Lab exercise: day 3",
    "section": "Regression",
    "text": "Regression\nWhen performing time series forecasting with fable package, it’s important to have a single tsibble that includes all the necessary variables for modeling. This includes the response variable (the variable you want to forecast) as well as all predictors.\nIn the vaccine_administrated_tsb, dose_adminstrated is the response variable and population_under1, and strike are predictors.\n\nYou may find your response variable and predictors in separate tsibbles. In such cases, it’s important to first join them before proceeding to the modeling stage.\n\n\nAssociation between dose_adminstrated and predictors\nWhen building regression models, domain knowledge may recommend some potential driving factors that can be useful t forecast the response variable. It is important to check whether these predictors are associated with response variable using scatter plot.\nComplete the following code to create a scatter plot showing a possible association between population under 1 and dose_adminstrated:\n\nggplot(vaccine_administrated_tsb, \n       aes(x= population_under1, y=dose_adminstrated))+\n  geom_point()+\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nDo you see any linear association between dose_adminstrated and population unde 1?\nComplete the following code to see if there are differences in dose_administered based on whether the months are associated with strike days or not.\n\nggplot(vaccine_administrated_tsb, \n       aes(x= strike, y = dose_adminstrated))+\n  geom_boxplot()\n\n\n\n\nDo you see any linear association between dose_adminstrated and strike?\n\n\nCross correlation and lagged/lead predictors\nYou may think that a predictor impacts the response variable, but with a delay. This is referred to as a leading predictor. For instance, you can investigate whether the population in previous months is associated with the dose administered in the future.\nComplete the following code to check the association between population in various lags and dose administrated:\n\n#lag2.plot from astsa package can be used to visualize leading predictors\nlag2.plot(vaccine_administrated_tsb$population_under1, vaccine_administrated_tsb$dose_adminstrated, max.lag = 12)\n\n\n\n\nInstead of scatter plots, you can also show the visualize the correlation between different lags of population with dose administrated using cross correlation function (cff).\nComplete the following code to show the ccf:\n\n# create a cross correlation plot\nccf(vaccine_administrated_tsb$population_under1, vaccine_administrated_tsb$dose_adminstrated)\n\n\n\n\nHow strong is the correlation between lagged population and dose administrated? How do you interpret the cross correlation function plot?\n\n\nSpecify and train time series regression model\nComplete the R code to specify and train the three regression models with different terms:\n\nfit_regression &lt;- vaccine_administrated_tsb |&gt;\n  model(\n  regression1 = TSLM(dose_adminstrated ~ trend() + season()),\n  regression_population = TSLM(dose_adminstrated ~ trend() + season() + population_under1),\n  regression_population_strike = TSLM(dose_adminstrated ~ trend() + season() + population_under1+strike))\n\nfit_regression\n\n# A mable: 9 x 4\n# Key:     region [9]\n  region regression1 regression_population regression_population_strike\n  &lt;chr&gt;      &lt;model&gt;               &lt;model&gt;                      &lt;model&gt;\n1 A           &lt;TSLM&gt;                &lt;TSLM&gt;                       &lt;TSLM&gt;\n2 B           &lt;TSLM&gt;                &lt;TSLM&gt;                       &lt;TSLM&gt;\n3 C           &lt;TSLM&gt;                &lt;TSLM&gt;                       &lt;TSLM&gt;\n4 D           &lt;TSLM&gt;                &lt;TSLM&gt;                       &lt;TSLM&gt;\n5 E           &lt;TSLM&gt;                &lt;TSLM&gt;                       &lt;TSLM&gt;\n6 G           &lt;TSLM&gt;                &lt;TSLM&gt;                       &lt;TSLM&gt;\n7 H           &lt;TSLM&gt;                &lt;TSLM&gt;                       &lt;TSLM&gt;\n8 I           &lt;TSLM&gt;                &lt;TSLM&gt;                       &lt;TSLM&gt;\n9 J           &lt;TSLM&gt;                &lt;TSLM&gt;                       &lt;TSLM&gt;\n\n\nHow many rows and columns are present, and what do they represent?\n\n\nCheck training model’s output\nYou can to get a summary of the trained regression models, this will tell you which variables are useful in explaining the variation in the vaccine dose administrated and also how much variation could be explained bu each model.\n\nfit_regression  |&gt; report()\n\nWarning in report.mdl_df(fit_regression): Model reporting is only supported for\nindividual models, so a glance will be shown. To see the report for a specific\nmodel, use `select()` and `filter()` to identify a single model.\n\n\n# A tibble: 27 × 16\n   region .model r_squared adj_r_squared sigma2 statistic  p_value    df log_lik\n   &lt;chr&gt;  &lt;chr&gt;      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n 1 A      regre…     0.346         0.264 2.36e6      4.19 2.84e- 5    13   -939.\n 2 A      regre…     0.349         0.259 2.38e6      3.87 5.35e- 5    14   -939.\n 3 A      regre…     0.466         0.386 1.97e6      5.80 5.40e- 8    15   -928.\n 4 B      regre…     0.726         0.692 3.91e5     21.0  1.04e-21    13   -842.\n 5 B      regre…     0.739         0.703 3.77e5     20.5  5.71e-22    14   -839.\n 6 B      regre…     0.874         0.855 1.84e5     46.1  1.42e-35    15   -800.\n 7 C      regre…     0.323         0.238 3.66e6      3.78 1.08e- 4    13   -962.\n 8 C      regre…     0.333         0.241 3.65e6      3.61 1.29e- 4    14   -962.\n 9 C      regre…     0.484         0.406 2.85e6      6.22 1.42e- 8    15   -948.\n10 D      regre…     0.275         0.184 4.57e6      3.01 1.32e- 3    13   -974.\n# ℹ 17 more rows\n# ℹ 7 more variables: AIC &lt;dbl&gt;, AICc &lt;dbl&gt;, BIC &lt;dbl&gt;, CV &lt;dbl&gt;,\n#   deviance &lt;dbl&gt;, df.residual &lt;int&gt;, rank &lt;int&gt;\n\n\nyou can select any model of interest and filter any region of focus if you like:\n\nfit_regression  |&gt; filter(region == \"B\") |&gt; select(regression_population_strike) |&gt; report()\n\nSeries: dose_adminstrated \nModel: TSLM \n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-906.38 -301.04  -11.11  259.63 1223.64 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        1.391e+03  7.893e+02   1.763  0.08122 .  \ntrend()            3.099e+01  1.394e+00  22.230  &lt; 2e-16 ***\nseason()year2      1.249e+02  2.023e+02   0.617  0.53857    \nseason()year3      5.354e+02  2.023e+02   2.646  0.00955 ** \nseason()year4      2.933e+02  2.023e+02   1.450  0.15056    \nseason()year5      5.657e+02  2.024e+02   2.796  0.00629 ** \nseason()year6      5.420e+02  2.039e+02   2.658  0.00925 ** \nseason()year7      4.402e+02  2.039e+02   2.159  0.03345 *  \nseason()year8      3.888e+02  2.040e+02   1.906  0.05973 .  \nseason()year9      3.493e+02  2.040e+02   1.712  0.09026 .  \nseason()year10     5.967e+02  2.041e+02   2.924  0.00434 ** \nseason()year11     1.335e+02  2.028e+02   0.658  0.51198    \nseason()year12    -3.180e+02  2.029e+02  -1.567  0.12041    \npopulation_under1  1.715e-02  7.983e-03   2.149  0.03426 *  \nstrike1           -2.321e+03  2.327e+02  -9.977 2.28e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 429.1 on 93 degrees of freedom\nMultiple R-squared: 0.874,  Adjusted R-squared: 0.8551\nF-statistic:  46.1 on 14 and 93 DF, p-value: &lt; 2.22e-16\n\n\nGet a summary of estimated parameters and corresponding statistics using tidy() and glance(). You can try these functions by completing the following code.\nuse tidy()\n\nfit_regression |&gt; tidy()\n\n# A tibble: 378 × 7\n   region .model      term          estimate std.error statistic  p.value\n   &lt;chr&gt;  &lt;chr&gt;       &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 A      regression1 (Intercept)   12892.      563.     22.9    1.90e-40\n 2 A      regression1 trend()           3.51      4.77    0.735  4.64e- 1\n 3 A      regression1 season()year2  -240.      724.     -0.331  7.41e- 1\n 4 A      regression1 season()year3   184.      724.      0.255  8.00e- 1\n 5 A      regression1 season()year4  -662.      725.     -0.913  3.63e- 1\n 6 A      regression1 season()year5  1232.      725.      1.70   9.24e- 2\n 7 A      regression1 season()year6  -166.      725.     -0.229  8.19e- 1\n 8 A      regression1 season()year7   683.      725.      0.943  3.48e- 1\n 9 A      regression1 season()year8    50.1     725.      0.0691 9.45e- 1\n10 A      regression1 season()year9   707.      725.      0.974  3.32e- 1\n# ℹ 368 more rows\n\n\nuse glance()\n\nfit_regression |&gt; glance()\n\n# A tibble: 27 × 16\n   region .model r_squared adj_r_squared sigma2 statistic  p_value    df log_lik\n   &lt;chr&gt;  &lt;chr&gt;      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n 1 A      regre…     0.346         0.264 2.36e6      4.19 2.84e- 5    13   -939.\n 2 A      regre…     0.349         0.259 2.38e6      3.87 5.35e- 5    14   -939.\n 3 A      regre…     0.466         0.386 1.97e6      5.80 5.40e- 8    15   -928.\n 4 B      regre…     0.726         0.692 3.91e5     21.0  1.04e-21    13   -842.\n 5 B      regre…     0.739         0.703 3.77e5     20.5  5.71e-22    14   -839.\n 6 B      regre…     0.874         0.855 1.84e5     46.1  1.42e-35    15   -800.\n 7 C      regre…     0.323         0.238 3.66e6      3.78 1.08e- 4    13   -962.\n 8 C      regre…     0.333         0.241 3.65e6      3.61 1.29e- 4    14   -962.\n 9 C      regre…     0.484         0.406 2.85e6      6.22 1.42e- 8    15   -948.\n10 D      regre…     0.275         0.184 4.57e6      3.01 1.32e- 3    13   -974.\n# ℹ 17 more rows\n# ℹ 7 more variables: AIC &lt;dbl&gt;, AICc &lt;dbl&gt;, BIC &lt;dbl&gt;, CV &lt;dbl&gt;,\n#   deviance &lt;dbl&gt;, df.residual &lt;int&gt;, rank &lt;int&gt;\n\n\nYou can augment() to see the fitted values and residuals:\n\nfit_regression |&gt; augment()\n\n# A tsibble: 2,916 x 7 [1M]\n# Key:       region, .model [27]\n   region .model         month dose_adminstrated .fitted .resid .innov\n   &lt;chr&gt;  &lt;chr&gt;          &lt;mth&gt;             &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 A      regression1 2013 Jan             13328  12895.   433.   433.\n 2 A      regression1 2013 Feb             12523  12658.  -135.  -135.\n 3 A      regression1 2013 Mar             10051  13086. -3035. -3035.\n 4 A      regression1 2013 Apr              9194  12244. -3050. -3050.\n 5 A      regression1 2013 May             15985  14141.  1844.  1844.\n 6 A      regression1 2013 Jun             13776  12746.  1030.  1030.\n 7 A      regression1 2013 Jul             14258  13599.   659.   659.\n 8 A      regression1 2013 Aug             13665  12970.   695.   695.\n 9 A      regression1 2013 Sep             12909  13630.  -721.  -721.\n10 A      regression1 2013 Oct             15010  12770.  2240.  2240.\n# ℹ 2,906 more rows\n\n\n\n\nProduce forecast using regression\nNow, let’s forecast using the regression models. Forecasting with the presence of predictors (such as population and strike) is slightly different than just using time series data.\nWe need to use the estimated values of predictors instead of actual that we have in test set. Because if we forecast for real future (e.g. next year), we don’t know true values of the population, we have to estimate it or use the estimation published by someone else such as Office of National Statistics.\nGiven that we use strike and population under a year in the model and we produce forecast for the next 12 month, we need to know the future values of these predictors. Strike is considered as a deterministic predictor, so as they are planned in advance. However, population is a stochastic predictor, as its future values are unknown and we need its estimation.\n\nThis might be different if you use leading predictors. Depending on how you include them in the model, you may not require to use their forecasts.\nIt is important to know the difference between to Ex-ante and Ex-post. Ex-ante forecasts are those that are made using only the information that is available in advance, which means the estimated values of predictors are used. Ex-post forecasts are those that are made using actual values of the predictors. Ex-post forecasts can show you how much error in the forecast could be related to the error of the estimation of predictors.\n\nLet’s forecast the population using regression as we don’t have the estimation from officials.\nYou first need to produce the future months. Complete the following code todo that:\n\nforecast_horizon &lt;- 12\nfuture_month &lt;- new_data(vaccine_administrated_tsb, n=forecast_horizon)\n\nGiven that we forecast for the next 12 months, we assume that the country will experience strikes in March next year. Add a new column, strike by completing the R code:\n\nfuture_month_strike &lt;- future_month |&gt; \n  mutate(strike = if_else(lubridate::month(month, label = TRUE) == \"Mar\", 1,0))\n\nAdd a new column, population_under1, to include the estimated population under 1, by completing the R code:\n\nforecast_population &lt;- vaccine_administrated_tsb |&gt; model(regression_population=TSLM(dose_adminstrated)) |&gt; forecast(h=forecast_horizon)\n\npopulation_point_forecast &lt;- forecast_population |&gt; as_tibble() |&gt; select(.mean)\n\n\ntest_future &lt;- bind_cols(future_month_strike, population_point_forecast) |&gt;\n  mutate(population_under1=.mean) |&gt; select(-.mean)\n\nNow, we can forecast using trained models. If you have predictors (e.g. population, strike, holiday, etc) you need to pass the future data instead of the h= using the new_data = argument: :\n\nfcst_regression &lt;-  fit_regression |&gt; \n  forecast(new_data = test_future)"
  },
  {
    "objectID": "solutions/day3.html#visualize-forecasts-by-regression",
    "href": "solutions/day3.html#visualize-forecasts-by-regression",
    "title": "Lab exercise: day 3",
    "section": "Visualize forecasts by regression",
    "text": "Visualize forecasts by regression\nComplete the following code to see the time series of dose adminstrated and its forecasts:\n\nfcst_regression |&gt; autoplot(vaccine_administrated_tsb, level=NULL)"
  },
  {
    "objectID": "solutions/day4.html#specify-and-traing-ets-models",
    "href": "solutions/day4.html#specify-and-traing-ets-models",
    "title": "Lab exercise: day 4",
    "section": "Specify and traing ETS models",
    "text": "Specify and traing ETS models\nWe now want to apply the Exponential Smoothing family of models to forecast dose administrated.\n\nRemember the following from ETS (E: Error, T:Trend, S: Seasonality) function:\n\nN: None (No trend, no seasonality)\nA: additive\nAd: additive damped\nM: multiplicative\n\n\nComplete the R code to train exponential smoothing models on the vaccine_administrated_tsb:\n\nfit_ets &lt;- vaccine_administrated_tsb |&gt;\n  model(\n    automatic_ets = ETS(dose_adminstrated)\n  )\n\n\nIf you don’t provide terms inside ETS(), then it is an automatic ETS! It will examine different models and return the one with lowest AICc."
  },
  {
    "objectID": "solutions/day4.html#observe-model-table-and-extract-models-output",
    "href": "solutions/day4.html#observe-model-table-and-extract-models-output",
    "title": "Lab exercise: day 4",
    "section": "Observe model table and extract model’s output",
    "text": "Observe model table and extract model’s output\nlet’s now observe the model table fit_ets. What type of data structure is fit_ets? How many rows and columns are present, and what do they represent?\nComplete the below R codes and run to understand model’s outputs:\nUse report():\n\nfit_ets |&gt; filter(region == \"A\") |&gt; report()\n\nSeries: dose_adminstrated \nModel: ETS(A,N,A) \n  Smoothing parameters:\n    alpha = 0.000100533 \n    gamma = 0.000100027 \n\n  Initial states:\n     l[0]      s[0]     s[-1]    s[-2]    s[-3]    s[-4]    s[-5]     s[-6]\n 12929.52 -3137.664 -832.7587 123.3305 858.3093 260.2248 1020.637 -96.87286\n    s[-7]     s[-8]    s[-9]    s[-10]    s[-11]\n 1223.083 -95.43912 739.7879 -40.85027 -21.78756\n\n  sigma^2:  2448776\n\n     AIC     AICc      BIC \n2109.475 2114.692 2149.706 \n\n\nWhich model is returned in, what are its components and parameter values?\nUse tidy():\n\nfit_ets |&gt; tidy()\n\n# A tibble: 98 × 4\n   region .model        term      estimate\n   &lt;chr&gt;  &lt;chr&gt;         &lt;chr&gt;        &lt;dbl&gt;\n 1 A      automatic_ets alpha     0.000101\n 2 A      automatic_ets gamma     0.000100\n 3 A      automatic_ets l[0]  12930.      \n 4 A      automatic_ets s[0]  -3138.      \n 5 A      automatic_ets s[-1]  -833.      \n 6 A      automatic_ets s[-2]   123.      \n 7 A      automatic_ets s[-3]   858.      \n 8 A      automatic_ets s[-4]   260.      \n 9 A      automatic_ets s[-5]  1021.      \n10 A      automatic_ets s[-6]   -96.9     \n# ℹ 88 more rows\n\n\nUse glance():\n\nfit_ets |&gt; glance()\n\n# A tibble: 9 × 10\n  region .model           sigma2 log_lik   AIC  AICc   BIC    MSE   AMSE     MAE\n  &lt;chr&gt;  &lt;chr&gt;             &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 A      automatic_ets   2.45e+6  -1040. 2109. 2115. 2150. 2.13e6 2.15e6 1.07e+3\n2 B      automatic_ets   3.02e+5   -933. 1872. 1873. 1880. 2.96e5 4.86e5 4.17e+2\n3 C      automatic_ets   2.59e-2  -1079. 2163. 2164. 2172. 4.47e6 4.59e6 1.10e-1\n4 D      automatic_ets   4.70e+6  -1075. 2180. 2185. 2220. 4.09e6 4.05e6 1.39e+3\n5 E      automatic_ets   2.69e+5   -921. 1871. 1876. 1911. 2.35e5 2.66e5 3.29e+2\n6 G      automatic_ets   1.84e+6  -1024. 2078. 2084. 2119. 1.60e6 1.73e6 8.83e+2\n7 H      automatic_ets   3.17e-3  -1050. 2133. 2140. 2179. 2.58e6 2.60e6 4.16e-2\n8 I      automatic_ets   2.28e+6  -1042. 2091. 2091. 2099. 2.23e6 2.24e6 1.13e+3\n9 J      automatic_ets   6.84e+4   -846. 1723. 1728. 1763. 5.95e4 6.40e4 1.89e+2\n\n\nYou can also observe the values corresponding to level, trend,and seasonal components in ETS framework. Each column corresponds to one components.\nComplete the R code and run to observe the selected model components:\n\nfit_ets |&gt; components()\n\n# A dable: 1,047 x 8 [1M]\n# Key:     region, .model [9]\n# :        dose_adminstrated = (lag(level, 1) + lag(slope, 1)) * lag(season,\n#   12) * (1 + remainder)\n   region .model           month dose_adminstrated level season remainder slope\n   &lt;chr&gt;  &lt;chr&gt;            &lt;mth&gt;             &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 A      automatic_ets 2012 Jan                NA    NA  -21.8        NA    NA\n 2 A      automatic_ets 2012 Feb                NA    NA  -40.9        NA    NA\n 3 A      automatic_ets 2012 Mar                NA    NA  740.         NA    NA\n 4 A      automatic_ets 2012 Apr                NA    NA  -95.4        NA    NA\n 5 A      automatic_ets 2012 May                NA    NA 1223.         NA    NA\n 6 A      automatic_ets 2012 Jun                NA    NA  -96.9        NA    NA\n 7 A      automatic_ets 2012 Jul                NA    NA 1021.         NA    NA\n 8 A      automatic_ets 2012 Aug                NA    NA  260.         NA    NA\n 9 A      automatic_ets 2012 Sep                NA    NA  858.         NA    NA\n10 A      automatic_ets 2012 Oct                NA    NA  123.         NA    NA\n# ℹ 1,037 more rows\n\n\nCould you describe what each row and column represent?"
  },
  {
    "objectID": "solutions/day4.html#generate-forecasts-using-ets",
    "href": "solutions/day4.html#generate-forecasts-using-ets",
    "title": "Lab exercise: day 4",
    "section": "Generate forecasts using ETS",
    "text": "Generate forecasts using ETS\nNow, we can forecast with the trained ETS model. Complete and run the following code to produce forecasts:\n\nforecast_horizon &lt;- 12\nforecast_ets &lt;- fit_ets |&gt;\n  forecast(h = forecast_horizon) \nforecast_ets\n\n# A fable: 108 x 5 [1M]\n# Key:     region, .model [9]\n   region .model           month dose_adminstrated  .mean\n   &lt;chr&gt;  &lt;chr&gt;            &lt;mth&gt;            &lt;dist&gt;  &lt;dbl&gt;\n 1 A      automatic_ets 2022 Jan N(12907, 2448776) 12907.\n 2 A      automatic_ets 2022 Feb N(12888, 2448776) 12888.\n 3 A      automatic_ets 2022 Mar N(13668, 2448776) 13668.\n 4 A      automatic_ets 2022 Apr N(12833, 2448776) 12833.\n 5 A      automatic_ets 2022 May N(14152, 2448776) 14152.\n 6 A      automatic_ets 2022 Jun N(12832, 2448776) 12832.\n 7 A      automatic_ets 2022 Jul N(13949, 2448776) 13949.\n 8 A      automatic_ets 2022 Aug N(13189, 2448776) 13189.\n 9 A      automatic_ets 2022 Sep N(13787, 2448776) 13787.\n10 A      automatic_ets 2022 Oct N(13052, 2448776) 13052.\n# ℹ 98 more rows\n\n\nWhat type of data structure is it? What each row and column represent?"
  },
  {
    "objectID": "solutions/day4.html#visualise-forecasts",
    "href": "solutions/day4.html#visualise-forecasts",
    "title": "Lab exercise: day 4",
    "section": "Visualise forecasts:",
    "text": "Visualise forecasts:\nYou can also visualize forecasts:\n\nforecast_ets |&gt;\n  autoplot(filter_index(vaccine_administrated_tsb, \"2020\" ~ .), level = NULL)\n\nWarning: There was 1 warning in `filter()`.\nℹ In argument: `time_in(month, ...)`.\nCaused by warning:\n! `yearmonth()` may yield unexpected results.\nℹ Please use arg `format` to supply formats."
  },
  {
    "objectID": "solutions/day4.html#detrmine-model-components-and-parameters-manually",
    "href": "solutions/day4.html#detrmine-model-components-and-parameters-manually",
    "title": "Lab exercise: day 4",
    "section": "Detrmine model components and parameters manually",
    "text": "Detrmine model components and parameters manually\nYou can use the following specific functions: error(), trend(), season() to manually specify the type of pattern (“N”, “A”, “Ad”, “M”) and also their corresponding parameters, if you wish.\nIn the following R-chunk, you can change parameters and the type of pattern to see their impact on fitting and forecast\n\nvaccine_ets &lt;- vaccine_administrated_tsb |&gt; filter(region == \"A\") |&gt; \n  model(\n    `alpha = 0.05` = ETS(dose_adminstrated ~ error(\"A\") + trend(\"A\", alpha = .05) + season(\"N\")),\n    `alpha = 0.15` = ETS(dose_adminstrated ~ error(\"A\") + trend(\"A\", alpha = .15) + season(\"N\")),\n    `alpha = 0.5` = ETS(dose_adminstrated ~ error(\"A\") + trend(\"A\", alpha = .5) + season(\"N\")),\n    `alpha = 0.9` = ETS(dose_adminstrated ~ error(\"A\") + trend(\"A\", alpha = .9) + season(\"N\"))\n  )\n\nvaccine_ets |&gt; augment() |&gt; filter_index(\"2020\" ~ .) |&gt; \n  ggplot(aes(x=month))+\n  geom_line(aes(y=dose_adminstrated, colour= \"Actual\"))+\n  geom_line(aes(y=.fitted, colour= factor(.model)))+\n  ggthemes::scale_color_colorblind()+\n  labs(colour =\"\")\n\nWarning: There was 1 warning in `filter()`.\nℹ In argument: `time_in(month, ...)`.\nCaused by warning:\n! `yearmonth()` may yield unexpected results.\nℹ Please use arg `format` to supply formats.\n\n\n\n\n\nCould you describe how the value of smoothing constant , alpha affects the forecast?"
  },
  {
    "objectID": "solutions/day4.html#arima",
    "href": "solutions/day4.html#arima",
    "title": "Lab exercise: day 4",
    "section": "ARIMA",
    "text": "ARIMA"
  },
  {
    "objectID": "solutions/day4.html#specify-and-train-arima-model",
    "href": "solutions/day4.html#specify-and-train-arima-model",
    "title": "Lab exercise: day 4",
    "section": "Specify and train ARIMA model",
    "text": "Specify and train ARIMA model\n\nIf you want this function automatically determines the order of autoregressive and moving average orders and their parameters, then you don’t need to provide arguments inside ARIMA(). The function will examine different models (combinations of p=0,1,2,.. and q =0,1,2,.., P=0,1,2,.. and Q =0,1,2,..) and return the one with lowest AICc.\n\nComplete the following code to train an automatic ARIMA model on the data:\n\nfit_arima &lt;- vaccine_administrated_tsb |&gt;\n  model(\n    automatic_arima = ARIMA(dose_adminstrated)\n  )\n\n\nIf we don’t provide terms inside ARIMA(), then it is an automatic model! It will examine different models and return the one with lowest AICc.\n\nlet’s now observe the fitted model, fit_arima:\n\nfit_arima\n\n# A mable: 9 x 2\n# Key:     region [9]\n  region                   automatic_arima\n  &lt;chr&gt;                            &lt;model&gt;\n1 A      &lt;ARIMA(0,0,1)(2,0,1)[12] w/ mean&gt;\n2 B                &lt;ARIMA(0,1,3) w/ drift&gt;\n3 C              &lt;ARIMA(1,1,1)(2,0,0)[12]&gt;\n4 D      &lt;ARIMA(1,0,0)(1,0,0)[12] w/ mean&gt;\n5 E              &lt;ARIMA(0,1,3)(1,0,1)[12]&gt;\n6 G              &lt;ARIMA(2,0,1)(2,1,0)[12]&gt;\n7 H              &lt;ARIMA(1,1,1)(2,0,0)[12]&gt;\n8 I                 &lt;ARIMA(0,0,0) w/ mean&gt;\n9 J              &lt;ARIMA(0,1,1)(0,0,2)[12]&gt;\n\n\nWhat type of data structure is it? What each row and column represent?"
  },
  {
    "objectID": "solutions/day4.html#extract-models-output",
    "href": "solutions/day4.html#extract-models-output",
    "title": "Lab exercise: day 4",
    "section": "Extract model’s output",
    "text": "Extract model’s output\nuse report()\n\nfit_arima |&gt; filter(region == \"A\") |&gt; \n  report()\n\nSeries: dose_adminstrated \nModel: ARIMA(0,0,1)(2,0,1)[12] w/ mean \n\nCoefficients:\n         ma1    sar1    sar2     sma1   constant\n      0.2600  0.7263  0.1264  -0.5431  1904.6765\ns.e.  0.0917  0.2415  0.1502   0.2448    57.9401\n\nsigma^2 estimated as 2529683:  log likelihood=-949.43\nAIC=1910.87   AICc=1911.7   BIC=1926.96\n\n\nUse tidy():\n\nfit_arima |&gt; tidy()\n\n# A tibble: 34 × 7\n   region .model          term     estimate std.error statistic  p.value\n   &lt;chr&gt;  &lt;chr&gt;           &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 A      automatic_arima ma1         0.260    0.0917     2.84  5.44e- 3\n 2 A      automatic_arima sar1        0.726    0.242      3.01  3.28e- 3\n 3 A      automatic_arima sar2        0.126    0.150      0.842 4.02e- 1\n 4 A      automatic_arima sma1       -0.543    0.245     -2.22  2.86e- 2\n 5 A      automatic_arima constant 1905.      57.9       32.9   4.53e-58\n 6 B      automatic_arima ma1        -0.403    0.121     -3.33  1.19e- 3\n 7 B      automatic_arima ma2        -0.193    0.136     -1.41  1.60e- 1\n 8 B      automatic_arima ma3        -0.339    0.128     -2.66  9.06e- 3\n 9 B      automatic_arima constant   29.8      4.48       6.65  1.29e- 9\n10 C      automatic_arima ar1         0.213    0.103      2.06  4.15e- 2\n# ℹ 24 more rows\n\n\nUse glance():\n\nfit_arima |&gt; glance()\n\n# A tibble: 9 × 9\n  region .model            sigma2 log_lik   AIC  AICc   BIC ar_roots   ma_roots \n  &lt;chr&gt;  &lt;chr&gt;              &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt;     &lt;list&gt;   \n1 A      automatic_arima 2529683.   -949. 1911. 1912. 1927. &lt;cpl [24]&gt; &lt;cpl&gt;    \n2 B      automatic_arima  265210.   -819. 1648. 1648. 1661. &lt;cpl [0]&gt;  &lt;cpl [3]&gt;\n3 C      automatic_arima 4158640.   -967. 1943. 1944. 1957. &lt;cpl [25]&gt; &lt;cpl [1]&gt;\n4 D      automatic_arima 4699765.   -982. 1972. 1973. 1983. &lt;cpl [13]&gt; &lt;cpl [0]&gt;\n5 E      automatic_arima  264671.   -819. 1650. 1651. 1666. &lt;cpl [12]&gt; &lt;cpl&gt;    \n6 G      automatic_arima 2314775.   -841. 1694. 1694. 1709. &lt;cpl [26]&gt; &lt;cpl [1]&gt;\n7 H      automatic_arima 3593702.   -959. 1929. 1929. 1942. &lt;cpl [25]&gt; &lt;cpl [1]&gt;\n8 I      automatic_arima 2254039.   -943. 1889. 1889. 1895. &lt;cpl [0]&gt;  &lt;cpl [0]&gt;\n9 J      automatic_arima   91859.   -763. 1534. 1534. 1545. &lt;cpl [0]&gt;  &lt;cpl&gt;"
  },
  {
    "objectID": "solutions/day4.html#generate-forecasts-using-arima",
    "href": "solutions/day4.html#generate-forecasts-using-arima",
    "title": "Lab exercise: day 4",
    "section": "Generate forecasts using ARIMA",
    "text": "Generate forecasts using ARIMA\nNow, we can forecast with the fitted ARIMA model:\n\nforecast_horizon &lt;- 12\nvaccine_fcst_arima &lt;- fit_arima |&gt; forecast(h = forecast_horizon)  \n\nLet’s now observe the forecast table, vaccine_fcst_arima:\n\nvaccine_fcst_arima\n\n# A fable: 108 x 5 [1M]\n# Key:     region, .model [9]\n   region .model             month dose_adminstrated  .mean\n   &lt;chr&gt;  &lt;chr&gt;              &lt;mth&gt;            &lt;dist&gt;  &lt;dbl&gt;\n 1 A      automatic_arima 2022 Jan N(13076, 2529701) 13076.\n 2 A      automatic_arima 2022 Feb N(12141, 2700761) 12141.\n 3 A      automatic_arima 2022 Mar N(13752, 2700761) 13752.\n 4 A      automatic_arima 2022 Apr N(13159, 2700761) 13159.\n 5 A      automatic_arima 2022 May N(13406, 2700761) 13406.\n 6 A      automatic_arima 2022 Jun N(13254, 2700761) 13254.\n 7 A      automatic_arima 2022 Jul N(13482, 2700761) 13482.\n 8 A      automatic_arima 2022 Aug N(13040, 2700761) 13040.\n 9 A      automatic_arima 2022 Sep N(13807, 2700761) 13807.\n10 A      automatic_arima 2022 Oct N(12653, 2700761) 12653.\n# ℹ 98 more rows\n\n\nWhat type of data structure is it? What each row and column represent?"
  },
  {
    "objectID": "solutions/day4.html#visualize-forecasts",
    "href": "solutions/day4.html#visualize-forecasts",
    "title": "Lab exercise: day 4",
    "section": "Visualize forecasts",
    "text": "Visualize forecasts\nYou can visualize your forecast:\n\nvaccine_fcst_arima |&gt; \n  autoplot(filter_index(vaccine_administrated_tsb, \"2020\" ~ .))\n\nWarning: There was 1 warning in `filter()`.\nℹ In argument: `time_in(month, ...)`.\nCaused by warning:\n! `yearmonth()` may yield unexpected results.\nℹ Please use arg `format` to supply formats."
  },
  {
    "objectID": "solutions/day4.html#determine-model-components-manually",
    "href": "solutions/day4.html#determine-model-components-manually",
    "title": "Lab exercise: day 4",
    "section": "Determine model components manually",
    "text": "Determine model components manually\nYou can also specify the order of p,q,P,Q using specific function pdq() and PDQ()manually:\n\nfit_arima_manual &lt;- vaccine_administrated_tsb |&gt;\n  model(\n    automatic_arima = ARIMA(dose_adminstrated),\n    arima_manual1=ARIMA(dose_adminstrated ~ 1+pdq(1,1,1)+PDQ(0,0,1)),\n    arima_manual2 = ARIMA(dose_adminstrated ~ 1+ pdq(3,1,0)+PDQ(1,0,0))\n  )\n\nCould you check AICc for the manual ARIMA models and compare it to the automatic ARIMA model?\n\nfit_arima_manual |&gt; glance() |&gt; select(region,.model, AICc) |&gt;  arrange(region,AICc)\n\n# A tibble: 27 × 3\n   region .model           AICc\n   &lt;chr&gt;  &lt;chr&gt;           &lt;dbl&gt;\n 1 A      arima_manual1   1910.\n 2 A      automatic_arima 1912.\n 3 A      arima_manual2   1925.\n 4 B      arima_manual1   1647.\n 5 B      automatic_arima 1648.\n 6 B      arima_manual2   1662.\n 7 C      automatic_arima 1944.\n 8 C      arima_manual1   1945.\n 9 C      arima_manual2   1966.\n10 D      arima_manual1   1962.\n# ℹ 17 more rows"
  },
  {
    "objectID": "solutions/day5.html#basic-of-traintest-forecast-accuracy",
    "href": "solutions/day5.html#basic-of-traintest-forecast-accuracy",
    "title": "Lab exercise: day 5",
    "section": "Basic of train/test forecast accuracy",
    "text": "Basic of train/test forecast accuracy\n\nSplit data\nWe tart by splitting data into two sets to describe the modeling process. We leave out 12 periods (equal to forecast horizon) as test set, and we pretend this is the future we want to forecast.\n\nforecast_horizon &lt;- 12# forecast horizon\ntest &lt;- vaccine_administrated_tsb |&gt; filter(month &gt;= (max(month)-forecast_horizon+1))\ntrain &lt;- vaccine_administrated_tsb |&gt; filter(month &lt; (max(month)-forecast_horizon+1))\n\n\n\nSpecify and train models\nWe specify four models with different components and train them on the train part:\n\nfit_vaccine &lt;- train |&gt;\n  model(\n    mean = MEAN(dose_adminstrated),\n    naive = NAIVE(dose_adminstrated),\n    snaive = SNAIVE(dose_adminstrated),\n    automatic_ets = ETS(dose_adminstrated),\n    automatic_arima = ARIMA(dose_adminstrated),\n  regression1 = TSLM(dose_adminstrated ~ trend() + season()),\n  regression_population = TSLM(dose_adminstrated ~ trend() + season() + population_under1),\n  regression_population_strike = TSLM(dose_adminstrated ~ trend() + season() + population_under1+strike)) |&gt; \n  mutate(combination = (automatic_ets+automatic_arima+regression_population_strike)/3\n)\n\n\n\nProduce forecasts\nHere we need to first prepare the future values of predictors corresponding to the test set:\n\nValues of population and strike in the test set\n\nforecast_horizon &lt;- 12\nfcs_ets_population &lt;- train |&gt; model(ets=ETS(population_under1)) |&gt; forecast(h=forecast_horizon)\n\npopulation_forecast &lt;- fcs_ets_population |&gt; as_tibble() |&gt; select(.mean)\n\ntest_future &lt;- bind_cols(test,population_forecast) |&gt;\n  mutate(dose_adminstrated=.mean) |&gt; \n  select(-.mean,-dose_adminstrated)\n\n\n\nProduce forecasts for dose adminstrated\n\nforecast_vaccine &lt;- fit_vaccine |&gt; \n  forecast(new_data = test_future)\n\nYou can visualise your forecast and see how the forecast looks like visually for the 12 month we predicted:\n\nforecast_vaccine |&gt; \n  autoplot(filter_index(train, \"2020\" ~ .), level=NULL)\n\nWarning: There was 1 warning in `filter()`.\nℹ In argument: `time_in(month, ...)`.\nCaused by warning:\n! `yearmonth()` may yield unexpected results.\nℹ Please use arg `format` to supply formats.\n\n\n\n\n\n\n\n\nCompute forecast accuracy\nLet’s compare the forecast accuracy of all models. Complete the R code to compute the point forecast accuracy, prediction interval accuracy and probabilistic distribution accuracy measures:\n\nforecast_vaccine |&gt; \n  accuracy(vaccine_administrated_tsb,\n           measures = list(point_accuracy_measures,\n                           interval_accuracy_measures, \n                           distribution_accuracy_measures)\n)\n\n# A tibble: 81 × 14\n   .model region .type    ME  RMSE   MAE   MPE  MAPE  MASE RMSSE    ACF1 winkler\n   &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 autom… A      Test  912.  1308. 1048.  6.23  7.34 0.635 0.608  0.212    6682.\n 2 autom… B      Test  758.   870.  772. 11.1  11.4  0.962 0.817  0.368    3124.\n 3 autom… C      Test  720.  1018.  856.  5.44  6.73 0.431 0.378  0.178    8864.\n 4 autom… D      Test  561.  1274. 1027.  2.95  6.36 0.493 0.440  0.202    9336.\n 5 autom… E      Test  608.   673.  608. 10.1  10.1  1.20  0.874  0.0861   2580.\n 6 autom… G      Test  854.  1625. 1199.  6.35 10.2  0.839 0.808  0.0713   7063.\n 7 autom… H      Test  931.  2028. 1846.  2.53  5.62 0.988 0.834  0.278    7852.\n 8 autom… I      Test  733.  1034.  916   5.18  6.68 0.591 0.504  0.380    6087.\n 9 autom… J      Test  -92.4  229.  194. -2.01  3.91 0.628 0.578  0.208    1327.\n10 autom… A      Test  767.  1620. 1232.  5.11  8.74 0.746 0.753 -0.147    6941.\n# ℹ 71 more rows\n# ℹ 2 more variables: percentile &lt;dbl&gt;, CRPS &lt;dbl&gt;\n\n\nWhich error metric do you use to evaluate the performance of models? Which model has the lowest error for each region? discuss your observation."
  },
  {
    "objectID": "solutions/day5.html#advanced-performance-evaluation",
    "href": "solutions/day5.html#advanced-performance-evaluation",
    "title": "Lab exercise: day 5",
    "section": "Advanced performance evaluation",
    "text": "Advanced performance evaluation\n\nTime series cross validation\n\nAttention: depedning on the umber of time series, the computation time might be big and cause compuationla time issues.\n\nThis is also called rolling forecast or rolling origin: You can also reflect on the following questions:\n\nWhy do we use TSCV? you can read more her\nHow do we do TSCV in R? Which steps to follow?\n\nsplit data using filter_index() or other functions\ncreate different time series origins\nmodel each origin,\nforecast each origin\n\n\nlet’s see how we do it in R:\n\nsplit data\nWe initially split the data into test and train. We defined a new variable, percentage_test. This will determine the percentage of the time series we use to evaluate the forecast accuracy using TSCV. As a general rule, we use 20%-30% of the length of time series as the test set. For instance, if we have 120 months of data, and use 20% as test set, that means we will have 24 months (120*0.2) in the test set:\n\nforecast_horizon &lt;- 12# forecast horizon\npercentage_test &lt;- 0.2 #20% of time series for test set\n\ntest &lt;- vaccine_administrated_tsb |&gt; \n  filter_index(as.character(max(vaccine_administrated_tsb$month)-round(percentage_test*length(unique(vaccine_administrated_tsb$month)))+1) ~ .)\n\ntrain &lt;- vaccine_administrated_tsb |&gt;\n  filter_index(. ~ as.character(max(vaccine_administrated_tsb$month)-(round(percentage_test*length(unique(vaccine_administrated_tsb$month))))))\n\n\n\nTime series cross validation\nBefore fitting the models, we need to create the time series origins in both train and test sets. We first apply time series cross validation on the train data. We start with an initial training, the length of the first origin (.init = ) and then increase the length of the previous origin by adding new observation (.step=), we continue creating these timeseries until the number of observation left at the end of timeseries equals to the forecast horizon, we stop there.\nNext, we apply time series cross validation on the test data. We create slides in the test set that corresponds to each origin created using train data, equal to the length of the forecast horizon.\n\ntrain_tscv &lt;- vaccine_administrated_tsb |&gt; \n  filter_index(. ~ as.character(max(vaccine_administrated_tsb$month)-(forecast_horizon))) |&gt;\n  stretch_tsibble(.init = length(unique(train$month)), .step = 1) # split data into different time series (i.e. origin or id) with increasing size\n\n# you need also to get future values that correspond to each .id, because you need them in the forecast model:\ntest_tscv &lt;- test |&gt; \n  slide_tsibble(.size = forecast_horizon, .step = 1, .id = \".id\") |&gt; select(-dose_adminstrated)\n\n\n.init is the size of first origin, .step is the increment step, this can correspond to the forecasting frequency, i.e. how often you generate the forecast. If .step = 1 in a monthly time series, it means we generate forecasts very month for the given forecast horizon.\n\n\n\nValues of population and strike in the test set\n\nIt is important to replace population_under1 values in the test_tscv with its estimation, otherwise we use perfect forecast for the population_under1 in the models using those predictors which can mislead us in choosing the most accurate model.\n\nWe don’t have access to these forecast, so here we forecast them using ETS. Complete the R code to produce the estimation of population_under1 abd replace it with actual values in test_tscv:\n\nfcs_ets_population_tscv &lt;- train_tscv |&gt; \n  model(ets=ETS(population_under1)) |&gt; \n  forecast(h=forecast_horizon)\n\npopulation_forecast_tscv &lt;- fcs_ets_population_tscv |&gt; as_tibble() |&gt; \n   select(-c(.model,population_under1))\n\ntest_future_tscv &lt;-inner_join(population_forecast_tscv,test_tscv) |&gt;\n  mutate(population_under1=.mean) |&gt; select(-.mean) |&gt; \n  as_tsibble(index = month, key = c(.id,region))\n\nJoining with `by = join_by(.id, region, month)`\n\npopulation_forecast_tscv &lt;- fcs_ets_population_tscv |&gt; \n  as_tibble() |&gt; select(.mean)\n\ntest_future_tscv &lt;- bind_cols(test_tscv,population_forecast_tscv) |&gt;\n  mutate(population_under1=.mean) |&gt; \n  select(-.mean) |&gt; tsibble::update_tsibble(key = c(.id, region))\n\n\n\nspecify and train models\nWe can train time series cross validation time series with regression models and any other models, this is exactly like what we have done before.\nComplete the R code to train all models on the TSCV data:\n\nfit_tscv &lt;-  train_tscv |&gt; \n  model(\n    mean = MEAN(dose_adminstrated),\n    naive = NAIVE(dose_adminstrated),\n    snaive = SNAIVE(dose_adminstrated),\n    automatic_ets = ETS(dose_adminstrated),\n    automatic_arima = ARIMA(dose_adminstrated),\n  regression1 = TSLM(dose_adminstrated ~ trend() + season()),\n  regression_population = TSLM(dose_adminstrated ~ trend() + season() + population_under1),\n  regression_population_strike = TSLM(dose_adminstrated ~ trend() + season() + population_under1+strike)) |&gt; \n  mutate(combination = (automatic_ets+automatic_arima+regression_population_strike)/3\n)\nfit_tscv\n\n# A mable: 99 x 11\n# Key:     .id, region [99]\n     .id region    mean   naive   snaive automatic_ets\n   &lt;int&gt; &lt;chr&gt;  &lt;model&gt; &lt;model&gt;  &lt;model&gt;       &lt;model&gt;\n 1     1 A       &lt;MEAN&gt; &lt;NAIVE&gt; &lt;SNAIVE&gt;  &lt;ETS(A,N,A)&gt;\n 2     1 B       &lt;MEAN&gt; &lt;NAIVE&gt; &lt;SNAIVE&gt;  &lt;ETS(A,N,N)&gt;\n 3     1 C       &lt;MEAN&gt; &lt;NAIVE&gt; &lt;SNAIVE&gt;  &lt;ETS(A,N,N)&gt;\n 4     1 D       &lt;MEAN&gt; &lt;NAIVE&gt; &lt;SNAIVE&gt;  &lt;ETS(A,N,A)&gt;\n 5     1 E       &lt;MEAN&gt; &lt;NAIVE&gt; &lt;SNAIVE&gt;  &lt;ETS(M,N,N)&gt;\n 6     1 G       &lt;MEAN&gt; &lt;NAIVE&gt; &lt;SNAIVE&gt;  &lt;ETS(M,N,A)&gt;\n 7     1 H       &lt;MEAN&gt; &lt;NAIVE&gt; &lt;SNAIVE&gt;  &lt;ETS(M,A,M)&gt;\n 8     1 I       &lt;MEAN&gt; &lt;NAIVE&gt; &lt;SNAIVE&gt;  &lt;ETS(A,N,N)&gt;\n 9     1 J       &lt;MEAN&gt; &lt;NAIVE&gt; &lt;SNAIVE&gt;  &lt;ETS(A,N,A)&gt;\n10     2 A       &lt;MEAN&gt; &lt;NAIVE&gt; &lt;SNAIVE&gt;  &lt;ETS(A,N,A)&gt;\n# ℹ 89 more rows\n# ℹ 5 more variables: automatic_arima &lt;model&gt;, regression1 &lt;model&gt;,\n#   regression_population &lt;model&gt;, regression_population_strike &lt;model&gt;,\n#   combination &lt;model&gt;\n\n\nObserve the fit_tscv object.\nWhat type of data structure is it? How many rows and columns are present, and what do they represent?\n\n\nproduce forecasts\nWe can forecast using trained models above. Complete the R code to produce forecasts for the TSCV:\n\nfcst_tscv &lt;- fit_tscv |&gt; \n  forecast(new_data = test_future_tscv)\nfcst_tscv\n\n# A fable: 10,692 x 8 [1M]\n# Key:     .id, region, .model [891]\n     .id region .model    month dose_adminstrated  .mean population_under1\n   &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;mth&gt;            &lt;dist&gt;  &lt;dbl&gt;             &lt;dbl&gt;\n 1     1 A      mean   2020 Mar N(12767, 3628753) 12767.           169023.\n 2     1 A      mean   2020 Apr N(12767, 3628753) 12767.           169023.\n 3     1 A      mean   2020 May N(12767, 3628753) 12767.           169023.\n 4     1 A      mean   2020 Jun N(12767, 3628753) 12767.           169023.\n 5     1 A      mean   2020 Jul N(12767, 3628753) 12767.           169023.\n 6     1 A      mean   2020 Aug N(12767, 3628753) 12767.           169023.\n 7     1 A      mean   2020 Sep N(12767, 3628753) 12767.           169023.\n 8     1 A      mean   2020 Oct N(12767, 3628753) 12767.           169023.\n 9     1 A      mean   2020 Nov N(12767, 3628753) 12767.           169023.\n10     1 A      mean   2020 Dec N(12767, 3628753) 12767.           169023.\n# ℹ 10,682 more rows\n# ℹ 1 more variable: strike &lt;fct&gt;\n\n\nObserve the fcst_tscv object.\nWhat type of data structure is it? How many rows and columns are present, and what do they represent?\n\n\nforecast accuracy\nLet’s compare the forecast accuracy of all models. Complete the R code to compute the point forecast accuracy, prediction interval accuracy and probabilistic distribution accuracy measures:\n\nfcst_accuracy &lt;- fcst_tscv |&gt; \n  accuracy(vaccine_administrated_tsb,\n           measures = list(point_accuracy_measures,\n                           interval_accuracy_measures, \n                           distribution_accuracy_measures)\n)\n\nfcst_accuracy |&gt; group_by(.model) |&gt; summarise(MASE=mean(MASE), winkler=mean(winkler), CRPS=mean(CRPS)) |&gt; arrange(MASE)\n\n# A tibble: 9 × 4\n  .model                        MASE winkler  CRPS\n  &lt;chr&gt;                        &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 combination                  0.635   5864.  619.\n2 regression_population_strike 0.660   6050.  655.\n3 regression1                  0.664   6293.  657.\n4 automatic_ets                0.690   6366.  679.\n5 regression_population        0.694   6261.  687.\n6 automatic_arima              0.699   6168.  670.\n7 snaive                       0.772   7362.  770.\n8 naive                        0.991  18125. 1356.\n9 mean                         1.09    7537.  929.\n\n\nObserve the fcst_accuracy object.\nWhat type of data structure is it? How many rows and columns are present, and what do they represent?\nYou may want to select a measure you focus on:\n\nfcst_accuracy |&gt; select(RMSE,MAE,MASE, winkler, CRPS)\n\n# A tibble: 81 × 5\n    RMSE   MAE  MASE winkler  CRPS\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 1279. 1026. 0.621   6826.  740.\n 2  683.  559. 0.698   3182.  388.\n 3 1084.  869. 0.438   8913.  726.\n 4 1632. 1243. 0.597   9473.  962.\n 5  593.  485. 0.953   2558.  338.\n 6 1924. 1500. 1.05    8965. 1103.\n 7 1841. 1647. 0.881   8030. 1070.\n 8  926.  787. 0.508   6223.  577.\n 9  206.  170. 0.550   1346.  127.\n10 1340. 1014. 0.614   6374.  755.\n# ℹ 71 more rows\n\n\nYou can calculate the overall accuracy across all regions:\n\nfcst_accuracy |&gt; group_by(.model) |&gt; summarise(MASE=mean(MASE), winkler=mean(winkler), CRPS=mean(CRPS)) |&gt; arrange(MASE)\n\n# A tibble: 9 × 4\n  .model                        MASE winkler  CRPS\n  &lt;chr&gt;                        &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 combination                  0.635   5864.  619.\n2 regression_population_strike 0.660   6050.  655.\n3 regression1                  0.664   6293.  657.\n4 automatic_ets                0.690   6366.  679.\n5 regression_population        0.694   6261.  687.\n6 automatic_arima              0.699   6168.  670.\n7 snaive                       0.772   7362.  770.\n8 naive                        0.991  18125. 1356.\n9 mean                         1.09    7537.  929.\n\n\nThis will provide an overall summary (i.e an average) of multiple accuracy measures across all origins and forecast horizon. The result is summarised automatically across all origins (.id) and horizon using a simple average.\nWhich method is the best method (i.e. lowest error metric)?\n\n\naccuracy per id\nNow let’s see how we can get the accuracy measure for each origin (i.e. .id) separately instead of averaging across all of them. To do this, you need to use an additional argument in accuracy(by=):\n\nfc_accuracy_by_id &lt;- fcst_tscv |&gt; \n  accuracy(vaccine_administrated_tsb, by = c(\".model\", \".id\",\"region\"))\n\nWe can now create some insightful visualisations. Complete the following code to generate a density plot and a box plot that highlights the distribution of the error metrics. You can choose any error metric:\n\n# Density plot\nfc_accuracy_by_id |&gt; \n  select(.id,.model,MASE) |&gt; \n  ggplot(aes(MASE))+\n    geom_density(aes(fill=factor(.model)), alpha=.5)\n\n\n\n\n\n#Boxplot\nfc_accuracy_by_id |&gt; \n  select(.id,.model,MASE) |&gt; \nggplot(aes(y= fct_reorder(.model,MASE), x=MASE))+\n    geom_boxplot()\n\n\n\n\nWhat insights do these plots provide?\n\n\naccuracy across horizon\nWhat if you want to show the accuracy measure for each model and each horizon (h=1, 2,…,12)?\nIn fable we don’t get automatically a column that corresponds to forecast horizon (h=1,2,3,…, 12). If this is something you are interested in, you can do it yourself, let’s first observe the first 24 observations to see the difference later:\n\nfcst_tscv[1:24,]\n\n# A fable: 24 x 8 [1M]\n# Key:     .id, region, .model [2]\n     .id region .model    month dose_adminstrated  .mean population_under1\n   &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;mth&gt;            &lt;dist&gt;  &lt;dbl&gt;             &lt;dbl&gt;\n 1     1 A      mean   2020 Mar N(12767, 3628753) 12767.           169023.\n 2     1 A      mean   2020 Apr N(12767, 3628753) 12767.           169023.\n 3     1 A      mean   2020 May N(12767, 3628753) 12767.           169023.\n 4     1 A      mean   2020 Jun N(12767, 3628753) 12767.           169023.\n 5     1 A      mean   2020 Jul N(12767, 3628753) 12767.           169023.\n 6     1 A      mean   2020 Aug N(12767, 3628753) 12767.           169023.\n 7     1 A      mean   2020 Sep N(12767, 3628753) 12767.           169023.\n 8     1 A      mean   2020 Oct N(12767, 3628753) 12767.           169023.\n 9     1 A      mean   2020 Nov N(12767, 3628753) 12767.           169023.\n10     1 A      mean   2020 Dec N(12767, 3628753) 12767.           169023.\n# ℹ 14 more rows\n# ℹ 1 more variable: strike &lt;fct&gt;\n\n#View(fcst_tscv[1:24,])\n\nWe first need to group by id and .model and then create a new variable called h and assign row_number() to it (you can type ?row_number in your Console to see what this function does, it simply returns the number of row):\n\nfc_h &lt;- fcst_tscv |&gt; \n  group_by(.id,.model, region) |&gt; \n  mutate(h=row_number()) |&gt; ungroup()\n#View(fc_h[1:24,])# view the first 24 rows of ae_fc and observe h\n\nNow check rows from 12 to 24 to see the difference.\nTo calculate the accuracy measures for each horizon and model, complete the following code :\n\nfc_accuracy_h &lt;- fc_h |&gt; \n  as_fable(response = \"dose_adminstrated\", distribution = \"dose_adminstrated\") |&gt; \naccuracy(vaccine_administrated_tsb, \n           measures = list(point_accuracy_measures, \n                           interval_accuracy_measures, \n                           distribution_accuracy_measures),\n           by = c(\"region\",\".model\",\"h\"))\n\nYou can now create a line chart to show how forecast accuracy may change over the forecast horizon. Please complete the R code for a metric of your preference. You can replicate this process by changing the chosen metric:\n\nggplot(data = fc_accuracy_h, \n       mapping = aes(x = h, y = MASE, color = .model))+\n         geom_point()+\n  geom_line()+\n  facet_wrap(vars(region), scales=\"free_y\")+\n  ggthemes::scale_color_colorblind()+\n  scale_x_continuous(breaks = 1:12)+\nggthemes::theme_clean()+\n  labs(x=\"Month\",y=\"Acuracy\")\n\nWarning: This manual palette can handle a maximum of 8 values. You have\nsupplied 9.\n\n\nWarning: Removed 108 rows containing missing values (`geom_point()`).\n\n\nWarning: Removed 108 rows containing missing values (`geom_line()`).\n\n\n\n\n\nWhat insights do these plots provide?\n\n\n\nForecast using best model for the future and visualise it\nNow, we need to generate forecast for the future of the time series using the best model identified above. In order to do that, we need to get the values of predictors in the future corresponding to the forecast horizon, we first need to use new_data() followed by some data manipulation to get the new data required for forecasting:\nYou first need to produce the future months. Complete the following code todo that:\n\nfuture_month &lt;- new_data(vaccine_administrated_tsb, n=forecast_horizon)\n\nWe assume that we know that in March the country will face strikes. Add a new column, strike by completing the R code:\n\nfuture_month_strike &lt;- future_month |&gt; \n  mutate(strike = if_else(lubridate::month(month, label = TRUE) == \"Mar\", 1,0))\n\nAdd a new column, population_under1, to include the estimated population under 1, by completing the R code:\n\nforecast_population &lt;- vaccine_administrated_tsb |&gt; model(regression_population=ETS(dose_adminstrated)) |&gt; forecast(h=forecast_horizon)\n\npopulation_point_forecast &lt;- forecast_population |&gt; as_tibble() |&gt; select(.mean)\n\n\ntest_future &lt;- bind_cols(future_month_strike, population_point_forecast) |&gt;\n  mutate(population_under1=.mean) |&gt; select(-.mean)\n\nTrain the combination approach on the entire time series data:\n\nfit_future &lt;- vaccine_administrated_tsb |&gt; \n  model(\n    automatic_ets = ETS(dose_adminstrated),\n    automatic_arima = ARIMA(dose_adminstrated),\n  regression_population_strike = TSLM(dose_adminstrated ~ trend() + season() + population_under1+strike)) |&gt; \n  mutate(combination = (automatic_ets+automatic_arima+regression_population_strike)/3\n) |&gt; select(combination)\n\nForecast using the combination approach for the future:\n\nfcst_future &lt;- fit_future |&gt; \n  forecast(new_data = test_future)\n\n\nfcst_future |&gt; \n  autoplot(filter_index(vaccine_administrated_tsb, \"2020\" ~ .))# visualise it\n\nWarning: There was 1 warning in `filter()`.\nℹ In argument: `time_in(month, ...)`.\nCaused by warning:\n! `yearmonth()` may yield unexpected results.\nℹ Please use arg `format` to supply formats.\n\n\n\n\n\n\n\nResidual diagnostics\nNow, let’s perform the residual diagnostic for the most accurate forecasts identified above though time series cross validation.\nPlot the residuals:\n\nfit_future |&gt; augment() |&gt; filter(region == \"A\") |&gt; \nautoplot(.resid) +\n  labs(title = \"Residuals from the from the most accurate model\")\n\n\n\n\nCreate the histogram of residuals:\n\nfit_future |&gt; augment() |&gt; filter(region == \"A\") |&gt;\n  ggplot(aes(x = .resid)) +\n  geom_histogram() +\n  labs(title = \"Histogram of residuals from the most accurate model\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nCreate the ACF plot of residuals:\n\nfit_future |&gt; augment() |&gt; filter(region == \"A\") |&gt;\n  ACF(.resid) |&gt;\n  autoplot() +\n  labs(title = \"Residuals from the most accurate model\")\n\n\n\n\nInstead, you could use a function that provides all three plots together:\n\nfit_future |&gt; filter(region == \"A\") |&gt;\n  gg_tsresiduals()\n\n\n\n\nWhat does the analysis of residuals reveal about the best model? Are there any systematic patterns left in the residuals?"
  }
]