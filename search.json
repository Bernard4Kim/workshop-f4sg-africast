[
  {
    "objectID": "sessions/day1/exercises.html",
    "href": "sessions/day1/exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Weâ€™ve prepared an exercises project with some starter code for each of the sessions. You can download and open this project using:\n\nusethis::use_course(\"https://workshop.f4sg.org/africast/exercises.zip\")"
  },
  {
    "objectID": "sessions/day1/exercises.html#creating-a-time-series-tibble-a-tsibble",
    "href": "sessions/day1/exercises.html#creating-a-time-series-tibble-a-tsibble",
    "title": "Exercises",
    "section": "Creating a time series tibble (a tsibble!)",
    "text": "Creating a time series tibble (a tsibble!)\nA tsibble is a rectangular data frame that contains:\n\na time column: the index\nidentifying column(s): the key variables\nvalues (the measured variables)\n\nYou usually create a tsibble by converting an existing dataset (read from a file) with as_tsibble(). For example, letâ€™s look at the production of rice in Guinea.\nA tsibble enables time-aware data manipulation, which makes it easy to work with time series. It also has extra checks to prevent common errors, while these can be frustrating at first they are important in correctly analysing your data.\nThere are two common mistakes when creating a tsibble, which weâ€™ll see in the next example of Australian accommodation.\n\n\nError in `validate_tsibble()`:\n! A valid tsibble must have distinct rows identified by key and index.\nâ„¹ Please use `duplicates()` to check the duplicated rows.\n\n\n\n\n\n\n\n\nThat didnâ€™t workâ€¦\n\n\n\nReading the error says we have â€˜duplicated rowsâ€™. What this means is that we have two or more rows in the dataset for the same point in time. In time series it isnâ€™t possible to get two different values at the same time, but it is possible to measure several different things at the same time.\n\n\nWhen you get this error, consider if any of the datasetâ€™s variables can identify individual series.\n\n\n\n\n\n\nTip\n\n\n\nThe identifying key variables of a time series are usually character variables, and the measured variables are almost always numeric.\n\n\n\n\n# A tibble: 592 Ã— 5\n   Date       State                        Takings Occupancy   CPI\n   &lt;date&gt;     &lt;chr&gt;                          &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 1998-01-01 Australian Capital Territory    24.3      65    67  \n 2 1998-04-01 Australian Capital Territory    22.3      59    67.4\n 3 1998-07-01 Australian Capital Territory    22.5      58    67.5\n 4 1998-10-01 Australian Capital Territory    24.4      59    67.8\n 5 1999-01-01 Australian Capital Territory    23.7      58    67.8\n 6 1999-04-01 Australian Capital Territory    25.4      61    68.1\n 7 1999-07-01 Australian Capital Territory    28.2      66    68.7\n 8 1999-10-01 Australian Capital Territory    25.8      60    69.1\n 9 2000-01-01 Australian Capital Territory    27.3      60.9  69.7\n10 2000-04-01 Australian Capital Territory    30.1      64.7  70.2\n# â„¹ 582 more rows\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhich of these variable(s) identifies each time series?\n\n\nIn this dataset we have accommodation data from all 8 states in Australia, and so we need to specify State as a key variable when creating our tsibble.\n\n\n# A tsibble: 592 x 5 [1D]\n# Key:       State [8]\n   Date       State                        Takings Occupancy   CPI\n   &lt;date&gt;     &lt;chr&gt;                          &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 1998-01-01 Australian Capital Territory    24.3      65    67  \n 2 1998-04-01 Australian Capital Territory    22.3      59    67.4\n 3 1998-07-01 Australian Capital Territory    22.5      58    67.5\n 4 1998-10-01 Australian Capital Territory    24.4      59    67.8\n 5 1999-01-01 Australian Capital Territory    23.7      58    67.8\n 6 1999-04-01 Australian Capital Territory    25.4      61    68.1\n 7 1999-07-01 Australian Capital Territory    28.2      66    68.7\n 8 1999-10-01 Australian Capital Territory    25.8      60    69.1\n 9 2000-01-01 Australian Capital Territory    27.3      60.9  69.7\n10 2000-04-01 Australian Capital Territory    30.1      64.7  70.2\n# â„¹ 582 more rows\n\n\nHurray, we have a tsibble! ðŸŽ‰\n\n\n\n\n\n\nHowever thereâ€™s still one thing that isnâ€™t rightâ€¦\n\n\n\nIn the first row of the output we see [1D] - this means that the frequency of the data is daily.\n\n\nLooking at the index column (Date), we can see that each point in time is three months apart - or quarterly. This is another common mistake when working with time series, you need to set the appropriate temporal granularity.\n\n\n\n\n\n\nWhat is temporal granularity?\n\n\n\nTemporal granularity is the resolution in time. The time variable needs to match this resolution.\nIn this example, a date was used to represent quarters, but instead we must use yearquarter() to match the temporal granularity.\nHereâ€™s a helpful list of common granularities:\n\nas.integer(): annual data (as above)\nyearquarter(): Quarterly data (shown here)\nyearmonth(): Monthly data\nyearweek(): Weekly data\nas.Date(): Daily data\nas.POSIXct(): Sub-daily data\n\n\n\nTo use the appropriate temporal granularity, we first must change our Date column before creating the tsibble.\n\n\n# A tsibble: 592 x 5 [1Q]\n# Key:       State [8]\n      Date State                        Takings Occupancy   CPI\n     &lt;qtr&gt; &lt;chr&gt;                          &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 1998 Q1 Australian Capital Territory    24.3      65    67  \n 2 1998 Q2 Australian Capital Territory    22.3      59    67.4\n 3 1998 Q3 Australian Capital Territory    22.5      58    67.5\n 4 1998 Q4 Australian Capital Territory    24.4      59    67.8\n 5 1999 Q1 Australian Capital Territory    23.7      58    67.8\n 6 1999 Q2 Australian Capital Territory    25.4      61    68.1\n 7 1999 Q3 Australian Capital Territory    28.2      66    68.7\n 8 1999 Q4 Australian Capital Territory    25.8      60    69.1\n 9 2000 Q1 Australian Capital Territory    27.3      60.9  69.7\n10 2000 Q2 Australian Capital Territory    30.1      64.7  70.2\n# â„¹ 582 more rows\n\n\nNow we have a tsibble thatâ€™s ready to use! In the first row of the output you should now see [1Q] indicating that the data is quarterly. You can also see the second row shows us our key variable, State. Next to this is [8], which tells us that this dataset contains 8 time series (one for each of Australiaâ€™s states).\n\n\n\n\n\n\nPipes\n\n\n\nWhen chaining together multiple functions, itâ€™s helpful to use the pipe operator (|&gt;).\nThe pipe allows you to read the functions in the order that they are used - much like a sentence!\nMore information is here: https://r4ds.hadley.nz/workflow-style.html#sec-pipes\n\n\nThatâ€™s all you need to know about creating a tidy time series tsibble ðŸŒˆ.\n\n\n\n\n\n\nYour turn!\n\n\n\nCreate a tsibble for the number of tourists visiting Australia contained in data/tourism.csv.\nSome starter code has been provided for you in the day 1 exercises.\nHint: this dataset contains multiple key variables that need to be used together. You can specify multiple keys with as_tsibble(key = c(a, b, c))."
  },
  {
    "objectID": "sessions/day1/exercises.html#manipulating-time-series",
    "href": "sessions/day1/exercises.html#manipulating-time-series",
    "title": "Exercises",
    "section": "Manipulating time series",
    "text": "Manipulating time series\nOften you want to work with specific series, or perhaps the sum up the values across multiple series. We can use the same dplyr functions that are used in data analysis to explore our time series. Letâ€™s focus on a single state from the Australian accommodation example - here we use filter() to keep only the Queensland data.\n\n\n# A tsibble: 74 x 5 [1Q]\n# Key:       State [1]\n      Date State      Takings Occupancy   CPI\n     &lt;qtr&gt; &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 1998 Q1 Queensland    230.      54    67  \n 2 1998 Q2 Queensland    219.      54    67.4\n 3 1998 Q3 Queensland    268.      64    67.5\n 4 1998 Q4 Queensland    279.      61    67.8\n 5 1999 Q1 Queensland    241.      55    67.8\n 6 1999 Q2 Queensland    235.      56    68.1\n 7 1999 Q3 Queensland    286.      65    68.7\n 8 1999 Q4 Queensland    288.      61    69.1\n 9 2000 Q1 Queensland    253.      54.7  69.7\n10 2000 Q2 Queensland    253.      56.5  70.2\n# â„¹ 64 more rows\n\n\nMaybe we wanted to focus on the more recent data, only keeping observations after 2010. Note that multiple conditions (both time and place) can be included inside a single filter() function.\n\n\n# A tsibble: 26 x 5 [1Q]\n# Key:       State [1]\n      Date State      Takings Occupancy   CPI\n     &lt;qtr&gt; &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 2010 Q1 Queensland    464.      57.4  95.2\n 2 2010 Q2 Queensland    461.      58.5  95.8\n 3 2010 Q3 Queensland    573.      68.9  96.5\n 4 2010 Q4 Queensland    562.      64.8  96.9\n 5 2011 Q1 Queensland    471.      58.1  98.3\n 6 2011 Q2 Queensland    489.      61    99.2\n 7 2011 Q3 Queensland    592.      70.5  99.8\n 8 2011 Q4 Queensland    587.      66.9  99.8\n 9 2012 Q1 Queensland    530.      62.3  99.9\n10 2012 Q2 Queensland    519.      62.6 100. \n# â„¹ 16 more rows\n\n\nLetâ€™s try seeing the total accommodation Takings and Occupancy for all of Australia. For this, we can use the summarise() function to summarise information across multiple rows.\n\n\n# A tsibble: 74 x 3 [1Q]\n      Date Takings Occupancy\n     &lt;qtr&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1 1998 Q1    949.      469 \n 2 1998 Q2    875.      431 \n 3 1998 Q3    981.      458 \n 4 1998 Q4   1036.      468 \n 5 1999 Q1    997.      460 \n 6 1999 Q2    940.      447 \n 7 1999 Q3   1062.      481 \n 8 1999 Q4   1105.      474 \n 9 2000 Q1   1088.      465.\n10 2000 Q2   1039.      460.\n# â„¹ 64 more rows\n\n\n\n\n\n\n\n\nThe index and summarise()\n\n\n\nWe still have our Date variable as it is automatically grouped when working with tsibble.\n\n\nWhat about calculating the annual takings, not quarterly? For this we use a special grouping function called index_by().\n\n\n# A tsibble: 19 x 3 [1Y]\n    Year Takings Occupancy\n   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1  1998   3841.     1826 \n 2  1999   4104.     1862 \n 3  2000   4725.     1834.\n 4  2001   4766.     1819.\n 5  2002   4865.     1848 \n 6  2003   5277.     1887.\n 7  2004   5675.     1950.\n 8  2005   6189.     1996.\n 9  2006   6783.     2054.\n10  2007   7443.     2107.\n11  2008   7897.     2074.\n12  2009   7629.     2024.\n13  2010   8088.     2081 \n14  2011   8534.     2089.\n15  2012   8965.     2088 \n16  2013   8992.     2048.\n17  2014   9477.     2031.\n18  2015  10242.     2069.\n19  2016   5080.     1034.\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nUsing the tourism dataset, create an annual time series of the Purpose of travel for visitors to Australia (summing over State and Region)\nSome starter code has been provided for you in the day 1 exercises.\nHint: think about which key variables should be kept with group_by(), and how the index should be changed using index_by() then summarise().\n\n\nWhat if we didnâ€™t want a time series at all? To calculate the total takings over all of time, we convert back to an ordinary data frame with as_tibble() and then summarise().\n\n\n# A tibble: 1 Ã— 2\n  Takings Occupancy\n    &lt;dbl&gt;     &lt;dbl&gt;\n1 128571.    36720.\n\n\nWhich state has had the most accommodation takings in 2010? Letâ€™s calculate total takings by state for 2010, and sort them with arrange().\n\n\n# A tibble: 8 Ã— 3\n  State                        Takings Occupancy\n  &lt;chr&gt;                          &lt;dbl&gt;     &lt;dbl&gt;\n1 New South Wales                2595.      259.\n2 Queensland                     2061.      250.\n3 Victoria                       1517.      258.\n4 Western Australia               849.      259.\n5 South Australia                 381.      252.\n6 Northern Territory              265.      262.\n7 Australian Capital Territory    227.      304.\n8 Tasmania                        193.      238.\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nUsing the tourism dataset, which Purpose of travel is most common in each state?\nSome starter code has been provided for you in the day 1 exercises.\nHint: since you no longer want to consider changes over time, youâ€™ll need to convert the data back to a tibble."
  },
  {
    "objectID": "sessions/day1/exercises.html#visualising-time-series",
    "href": "sessions/day1/exercises.html#visualising-time-series",
    "title": "Exercises",
    "section": "Visualising time series",
    "text": "Visualising time series\nThere are a few common visualisation techniques specific to time series, however cross-sectional graphics also work well for time series data. The main difference is that we like to maintain the ordered and connected nature of time.\n\nTime plots\nThe simplest graphic for time series is the time series plot, which shows the variable of interest (on the y-axis) against time (on the x-axis). This plot can be created manually with ggplot2, or automatically plotted from the tsibble with autoplot().\n\n\n\n\n\n\n\n\n\nIn this plot we can see that Production increases over time (known as trend). The increase is mostly smooth but there are a couple anomalies in 2001 and 2008.\n\n\n\n\n\n\nPlotting the time variable\n\n\n\nIn this plot, Production and Year are two continuous variables. We would often like to plot two continuous variables with a scatter plot, however in time-series we prefer to connect the observations from one year to the next to give this line chart.\n\n\nWe can also use autoplot() to produce a time plot of many series, but be careful not to plot too many lines at once!\n\n\n\n\n\n\n\n\n\nIn this plot of Australian accommodation takings, we see that most states have increasing takings over time (upward trend). We can also notice a repeating up and down pattern, which upon closer inspection repeats every year. This repeating annual pattern is known as seasonality, and we can see that some states are more seasonal than others.\nLetâ€™s focus on the sunny holiday destination of Queensland, and use different plots to better understand the seasonality.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nUsing the tourism dataset, create time plots of the data. Which patterns can you observe?\nSome starter code has been provided for you in the day 1 exercises.\nHint: there are too many series to show in a single plot, so filter and summarise series of interest to you.\n\n\n\n\nSeasonal plots\nIt can be tricky to see which quarter has maximum accommodation takings from a time plot. Instead, it is better to use a seasonal plot with gg_season() from feasts.\n\n\n\n\n\n\n\n\n\nHere we can see that the Q3 and Q4 takings are higher than Q1 and Q2, this is known as the seasonal peak and trough respectively.\n\n\n\n\n\n\nThe season plot\n\n\n\nThe seasonal plot is very similar to the time plot, but the x-axis now wraps over years. This allows us to more easily compare the years and find common patterns, like which month or quarter is biggest and smallest.\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nUsing the tourism dataset, create a seasonal plot for the total holiday travel to Australia over time. In which quarter is holiday travel highest and lowest?\nSome starter code has been provided for you in the day 1 exercises.\n\n\n\n\nSeasonal subseries plot\nAnother useful plot to understand the seasonal pattern of a time series is the subseries plot, it can be created with gg_subseries(). This plot is splits each month / quarter into separate facets (mini-plots), which shows how the values within each season change over time. The blue lines represent the average, which is a useful way to see the overall seasonality at a glance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeasonal sub-series plots\n\n\n\nThe upward lines in each facet of this plot shows the trend of the data, however if the lines went in different directions that would imply the shape of the seasonality is changing over time.\nSeasonal plots work best after removing trend, which we will see how to do tomorrow!\n\n\nLetâ€™s see this plot with a different dataset, recent beer production in Australia.\n\n\n\n\n\n\n\n\n\nAt a glance, this looks like the it is very seasonal and has a slight downward trend. However the seasonal subseries plot reveals that the trend is misleading!\n\n\n\n\n\n\n\n\n\nHere we see that only Q4 (the peak) has a downward trend, while the other quarters are staying roughly the same. The seasonality is changing shape over time.\n\n\n\n\n\n\nChanging seasonality\n\n\n\nLook back at the time plot and focus only on the Q4 peaks, can you see these values decreasing over time? Now look at the Q1-Q3 throughs, how do they change over time?\nThis can be tricky to notice in the time plot, which is why seasonal subseries plots can be particularly helpful!\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nUsing the tourism dataset, create a seasonal subseries plot for the total business travel to Victoria over time. Does the seasonal pattern change over time?\nSome starter code has been provided for you in the day 1 exercises.\n\n\n\n\nACF plots\nThese plots may look a bit strange at first, but they are very useful for seeing all of the time series dynamics in a single plot. ACF is the â€˜auto-correlation functionâ€™, essentially a measure of how similar a time series is to the lags of itself. Looking at these correlations can reveal trends, seasonality, cycles, and more subtle patterns. You can create an ACF plot using a combination of ACF() and autoplot().\n\n\n\n\n\n\n\n\n\nThe rice production of Guinea has an upward trend, which produces a gradual decay in the ACF.\n\n\n\n\n\n\n\n\n\nThe recent beer production of Australia has lots of seasonality and no trend, which creates large peaks at the seasonal lags in the ACF. Every 4 quarters we see a large ACF spike.\n\n\n\n\n\n\n\n\n\nThe total occupancy of Australiaâ€™s short-term accommodation is both trended and seasonal, which results in a slowly decaying ACF with peaks every seasonal lag (4, 8, 12, â€¦).\nConsider the number of Snowshoe Hares which were traded by the Hudson Bay Company.\n\n\n\n\n\n\n\n\n\nTo the untrained eye, this series has lots of up and down patterns - a bit like seasonality. However this pattern is cyclical, not seasonal. The ACF plot can help us distinguish cycles from seasonality.\n\n\n\n\n\n\nSeasonal or cyclic?\n\n\n\nSeasonality is a consistent repeating pattern, where the shape shape with similar peak and trough repeats at the same time interval.\nCyclical patterns are less consistent, with varying peaks and troughs that repeats over a varied time period.\n\n\nLetâ€™s see the ACF for this dataset\n\n\n\n\n\n\n\n\n\nNotice that the peak at lag 10 is less symmetric and â€˜sharpâ€™, this is because the pattern usually repeats every 10 years but sometimes 9 or 11. This is unlike seasonality, which has a sharper peak in the ACF due to the consistent time period between patterns.\n\n\n\n\n\n\nYour turn!\n\n\n\nIdentify which ACF matches the time plots in the following figures by identifying the patterns of trend, seasonality, and cycles in the ACF plots.\n\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nUsing the tourism dataset, create an ACF plot for the total travel to Australia over time. Can you identify patterns of trend and seasonality from this plot?\nSome starter code has been provided for you in the day 1 exercises.\n\n\n\n\n\n\n\n\nACF model evaluation\n\n\n\nImportantly, ACF plots can also tell us when there are no patterns/autocorrelations in the data (white noise).\nWeâ€™ll be revisiting this plot to evaluate our models on day 5. We hope that a model uses all available information, and ACF plots can show if there is any patterns left over."
  },
  {
    "objectID": "sessions/day1/exercises.html#about-the-dataset",
    "href": "sessions/day1/exercises.html#about-the-dataset",
    "title": "Exercises",
    "section": "About the dataset",
    "text": "About the dataset\nIn this exercise, we use a dataset containing dose of BCG (Cacille Calmette-GuÃ©rin), vaccine administrated in 9 regions of an African country from January 2013 until December 2021. BCG is a widely administered vaccine primarily used to protect against tuberculosis (TB), a serious infection that primarily affects the lungs but can also affect other parts of the body. BCG vaccination is recommended for newborn babies at risk of tuberculosis (TB) and is typically administered shortly after birth, usually within the first 28 days of life.\nIn addition to the administered dose, it also includes data on the population of children under one year old, and whether a strike occurred in a specific month and region.\nIn this exercise, you will apply what you have learned about different steps in the forecasting workflow on this dataset.\n\n\n\n\n\n\nYour turn!\n\n\n\n\nImport vaccine_adminstrated.csv data into R\n\nCheck and modify the data types of variables as needed\n\nPrepare your data\n\nCheck and fix missing values\nCheck duplications and fix it\nCreate tsibble\nCheck and fix temporal gaps\n\nManipulating time series\n\nCreate monthly time series of total doses adminstrated in the country\nCreate quarterly time series of doses adminstrated in each region\nCreate quarterly time series of total doses adminstrated in the country\n\nVisualizing time series\n\nUse time plots and describe what patterns you observe\nCreate plots to see if any consistent pattern exsists in monthly and quarterly of dose admisntrated\nCreate plots to see how dose admisntrated chnage over time for each month/quarter and how it differs across differnt month/quarter"
  },
  {
    "objectID": "sessions/day2/exercises.html",
    "href": "sessions/day2/exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Transformations provide useful simplifications of the patterns in a time series. Simplifying the patterns makes them easier to model, and so transforming the data is a common preliminary step in producing forecasts. Some transformations standardise values to be comparable between countries or other series in the dataset, while others can regularise the variation in the data.\nLetâ€™s look at the turnover of print media in Australia.\n\n\n\n\n\n\n\n\n\nTurnover has increased until the end of 2010, after which it has steadily declined. When looking at monetary value it is common to consider price indices to ensure that turnover is comparable over time. This allows you to identify patterns and changes such as turning points in real monetary terms.\n\n\n\n\n\n\nData for transformations\n\n\n\nIt can be useful to use other datasets that contain information for the transformation. To do this we can merge the datasets in time using join operations.\n\n\n\n\n\n\n\n\n\n\n\nAfter taking into account CPI, the real monetary Turnover of the print media industry in Australia has been gradually declining since 1990-2000.\n\n\n\n\n\n\nYour turn!\n\n\n\nSelect a country of your choice from global_economy, then calculate and visualise the the GDP per capita over time (that is, the GDP scaled by the population).\n\n\n\n\n\n\n\n\nTricky to forecast\n\n\n\nWhile transformations help to make the patterns simpler to forecast, if additional information like CPI or Population are used then they will also need to be forecasted. While population is generally easy to forecast, CPI could be more complicated to forecast than the thing youâ€™re originally forecasting!\n\n\nAnother useful transformation is calendar adjustments. This adjusts the observations in the time series to represent an equivalent time period, and can simplify seasonal patterns which result from these different lengths. This is particularly useful for monthly data, since the number of days in each month varies substantially.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nCalculate the monthly total Australian retail turnover from aus_retail and visualise the seasonal pattern. Then scale by the number of days in each month to calculate the daily average turnover and comparse the seasonal patterns.\n\n\nMathematical transformations are useful since they donâ€™t require providing any future values to produce the forecasts. Log and power transformations (\\(y^k\\), for example square root, square, and inverse) are particularly helpful for regularising variation proportional to the level of the series.\n\n\n\n\n\n\n\n\n\nThis proportional variance is common in time series, in Victoriaâ€™s cafe and restaurant turnover you can see small changes when turnover is low (before 2000), and is much larger after 2010 when turnover is much larger.\n\n\n\n\n\n\n\n\n\nLog transforming the data changes this variation to be more consistent, where the variation before 2000 is now more similar to after 2010.\nThe box-cox transformation family parameterises the range of power transformations for more precise adjustments. The transformation parameter \\(\\lambda\\) controls the strength of the transformation, with \\(\\lambda=1\\) being no change in shape, \\(\\lambda = 0\\) being a log transformation and others being equivalent in shape to \\(y^\\lambda\\).\nThe log transformation above was a bit strong, so letâ€™s try something slightly close to \\(\\lambda=1\\) - perhaps \\(\\lambda = 0.1\\)?\n\n\n\n\n\n\n\n\n\nThe variation is now consistent for the entire series, and the trend is linear.\n\n\n\n\n\n\nAutomatic box-cox transformations\n\n\n\nThe \\(\\lambda\\) parameter can be automatically computed using the guerrero() function.\n\n\nWe can calculate the optimal box-cox parameter using features() and guerrero():\n\n\n# A tibble: 1 Ã— 3\n  State    Industry                                 lambda_guerrero\n  &lt;chr&gt;    &lt;chr&gt;                                              &lt;dbl&gt;\n1 Victoria Cafes, restaurants and catering services           0.173\n\n\nLooks like we were pretty close with \\(\\lambda = 0.1\\), letâ€™s try using this more precise estimate:\n\n\n\n\n\n\n\n\n\nThe optimised box-cox transformation is very similar to \\(\\lambda = 0.1\\) - fortunately for us we donâ€™t have to be precise since this transformation isnâ€™t sensitive to your choice of parameter. So long as you are within \\(\\pm 0.1\\) the transformation should be okay. Additionally, if \\(\\lambda \\approx 0\\) then it is common to instead use the simpler log() transformation.\n\n\n\n\n\n\nYour turn!\n\n\n\nFind a suitable box-cox transformation for the monthly total Australian retail turnover, then compare your choice with the automatically selected parameter from the guerrero() feature.\n\n\n\n\n\nAnother commonly used transformation/adjustment requires a model to decompose the time series into its components. Seasonally adjusted time series are often used by analysts and policy makers to evaluate the underlying long term trends without the added complexity of seasonality. The STL decomposition is useful model which can isolate the seasonal pattern from the trend and remainder for many types of time series.\nThe STL decomposition separates time series into the form \\(Y = \\text{trend} + \\text{seasonality} + \\text{remainder}\\). Since this is an additive decomposition, we must first simplify any multiplicative patterns into additive ones using a suitable power transformation. Letâ€™s try to remove the annual seasonality from Australiaâ€™s print media turnover.\n\n\n\n\n\n\n\n\n\nThe seasonality is more varied when turnover increases, so we must transform the data before estimating the STL model.\n\n\n\n\n\n\n\n\n\nThe log transformation (\\(\\lambda = 0\\)) does a great job at producing a consistent variation throughout the series. You could try to find a better transformation using the box-cox transformation family, however there is no need for it here.\nWe can estimate the STL model using STL() as follows:\n\n\n# A mable: 1 x 1\n  `STL(log(Turnover))`\n               &lt;model&gt;\n1                &lt;STL&gt;\n\n\nThe decomposition can be obtained from the model using components(), and then all of the components can be plotted with autoplot():\n\n\n# A dable: 441 x 7 [1M]\n# Key:     .model [1]\n# :        log(Turnover) = trend + season_year + remainder\n   .model        Month `log(Turnover)` trend season_year remainder season_adjust\n   &lt;chr&gt;         &lt;mth&gt;           &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n 1 STL(log(Tâ€¦ 1982 Apr            4.90  4.92     -0.0723   0.0528           4.97\n 2 STL(log(Tâ€¦ 1982 May            4.89  4.92     -0.0128  -0.0151           4.91\n 3 STL(log(Tâ€¦ 1982 Jun            4.84  4.93     -0.0942   0.00869          4.94\n 4 STL(log(Tâ€¦ 1982 Jul            4.86  4.93     -0.0480  -0.0303           4.90\n 5 STL(log(Tâ€¦ 1982 Aug            4.88  4.94     -0.0155  -0.0458           4.89\n 6 STL(log(Tâ€¦ 1982 Sep            4.90  4.94     -0.0453  -0.00317          4.94\n 7 STL(log(Tâ€¦ 1982 Oct            4.93  4.95     -0.0167   0.00114          4.95\n 8 STL(log(Tâ€¦ 1982 Nov            4.97  4.95      0.0181  -0.00245          4.95\n 9 STL(log(Tâ€¦ 1982 Dec            5.26  4.96      0.258    0.0398           5.00\n10 STL(log(Tâ€¦ 1983 Jan            4.92  4.97     -0.0180  -0.0249           4.94\n# â„¹ 431 more rows\n\n\n\n\n\n\n\n\n\nThe components are obtained using rolling estimation windows, which are the main way the decomposition is changed. A large window produces smooth components, and a small window produces flexible and quickly changing components.\n\n\n\n\n\n\n\n\n\nThe infinite window for the seasonality results in a seasonal pattern that doesnâ€™t change over time, while the small trend window allows the trend to change very quickly. The best choice of estimation window should produce components that match the patterns in the original data while being as smooth as possible.\n\n\n\n\n\n\n\n\n\nA trend window of 25 for this dataset produces a mostly smooth trend component which can still react to brief decreases in turnover. The constant seasonal pattern (infinite window) is reasonable for this dataset since the seasonality doesnâ€™t change much over time.\n\n\n\n\n\n\nSeasonal adjustment\n\n\n\nYou can find the de-seasonalised data in the season_adjust column of the components() output.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nFind a suitable STL decomposition for the total Australian retail turnover, then produce and visualise the seasonally adjusted time series.\nHint: donâ€™t forget to use the suitable transformation found previously!\n\n\nSeasonal decomposition also makes it easier to take a look at the seasonality - we can use a combination of seasonal plots and decomposition to more easily see seasonal patterns.\n\n\n\n\n\n\n\n\n\nIf the seasonal window allows the seasonal component to change over time, the gg_subseries() plot is especially useful for seeing how the pattern changes.\n\n\n\n\n\n\n\n\n\nJanuary and December seem to increase over time, while April, May and June are decreasing.\n\n\n\n\n\n\nYour turn!\n\n\n\nProduce appropriate seasonal plots of the seasonal component from your STL decomposition on Australian retail turnover.\n\n\n\n\n\nA useful technique for visualising large collections of time series is to produce summaries of their patterns known as features. Visualising many time series simultaneously is difficult since the scale and shape of patterns can vary substantially. The features() function will compute single value summaries over time such as the strength of trend or seasonality. There are many features available, but the features from STL decompositions are particularly interesting.\n\n\n# A tibble: 152 Ã— 11\n   State       Industry trend_strength seasonal_strength_year seasonal_peak_year\n   &lt;chr&gt;       &lt;chr&gt;             &lt;dbl&gt;                  &lt;dbl&gt;              &lt;dbl&gt;\n 1 Australianâ€¦ Cafes, â€¦          0.989                  0.562                  0\n 2 Australianâ€¦ Cafes, â€¦          0.993                  0.629                  0\n 3 Australianâ€¦ Clothinâ€¦          0.991                  0.923                  9\n 4 Australianâ€¦ Clothinâ€¦          0.993                  0.957                  9\n 5 Australianâ€¦ Departmâ€¦          0.977                  0.980                  9\n 6 Australianâ€¦ Electriâ€¦          0.992                  0.933                  9\n 7 Australianâ€¦ Food reâ€¦          0.999                  0.890                  9\n 8 Australianâ€¦ Footweaâ€¦          0.982                  0.944                  9\n 9 Australianâ€¦ Furnituâ€¦          0.981                  0.687                  9\n10 Australianâ€¦ Hardwarâ€¦          0.992                  0.900                  9\n# â„¹ 142 more rows\n# â„¹ 6 more variables: seasonal_trough_year &lt;dbl&gt;, spikiness &lt;dbl&gt;,\n#   linearity &lt;dbl&gt;, curvature &lt;dbl&gt;, stl_e_acf1 &lt;dbl&gt;, stl_e_acf10 &lt;dbl&gt;\n\n\nIn particular, features from STL decompositions allow you to compare the strength of trend and seasonality between many time series.\n\n\n\n\n\n\n\n\n\nFrom this we can see that almost all time series have a strong trend, while the strength of seasonality is more varied - some series have strong seasonality while others have less seasonality.\n\n\n\n\n\n\nYour turn!\n\n\n\nCalculate the STL features for the time series in the tourism dataset. Try colouring the points in the scatterplot by the purpose of travel, are some reasons more trended or seasonal than others?\n\n\nThere are many other features that you can use - check out the documentation for ?features_by_pkg. You can produce a feature set of similar features using the feature_set() function.\n\n\n# A tibble: 152 Ã— 13\n   State      Industry  acf1 acf10 diff1_acf1 diff1_acf10 diff2_acf1 diff2_acf10\n   &lt;chr&gt;      &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n 1 Australiaâ€¦ Cafes, â€¦ 0.973  8.59     -0.348       0.239     -0.572       0.502\n 2 Australiaâ€¦ Cafes, â€¦ 0.977  8.65     -0.327       0.259     -0.552       0.531\n 3 Australiaâ€¦ Clothinâ€¦ 0.885  7.01     -0.276       0.251     -0.507       0.339\n 4 Australiaâ€¦ Clothinâ€¦ 0.846  6.33     -0.303       0.201     -0.532       0.321\n 5 Australiaâ€¦ Departmâ€¦ 0.500  1.60     -0.310       0.202     -0.540       0.315\n 6 Australiaâ€¦ Electriâ€¦ 0.902  7.29     -0.247       0.324     -0.506       0.455\n 7 Australiaâ€¦ Food reâ€¦ 0.984  9.13     -0.394       0.585     -0.611       1.12 \n 8 Australiaâ€¦ Footweaâ€¦ 0.760  4.64     -0.325       0.155     -0.566       0.333\n 9 Australiaâ€¦ Furnituâ€¦ 0.952  7.67     -0.190       0.163     -0.530       0.394\n10 Australiaâ€¦ Hardwarâ€¦ 0.957  7.67     -0.104       0.101     -0.497       0.311\n# â„¹ 142 more rows\n# â„¹ 5 more variables: season_acf1 &lt;dbl&gt;, pacf5 &lt;dbl&gt;, diff1_pacf5 &lt;dbl&gt;,\n#   diff2_pacf5 &lt;dbl&gt;, season_pacf &lt;dbl&gt;"
  },
  {
    "objectID": "sessions/day2/exercises.html#transformations",
    "href": "sessions/day2/exercises.html#transformations",
    "title": "Exercises",
    "section": "",
    "text": "Transformations provide useful simplifications of the patterns in a time series. Simplifying the patterns makes them easier to model, and so transforming the data is a common preliminary step in producing forecasts. Some transformations standardise values to be comparable between countries or other series in the dataset, while others can regularise the variation in the data.\nLetâ€™s look at the turnover of print media in Australia.\n\n\n\n\n\n\n\n\n\nTurnover has increased until the end of 2010, after which it has steadily declined. When looking at monetary value it is common to consider price indices to ensure that turnover is comparable over time. This allows you to identify patterns and changes such as turning points in real monetary terms.\n\n\n\n\n\n\nData for transformations\n\n\n\nIt can be useful to use other datasets that contain information for the transformation. To do this we can merge the datasets in time using join operations.\n\n\n\n\n\n\n\n\n\n\n\nAfter taking into account CPI, the real monetary Turnover of the print media industry in Australia has been gradually declining since 1990-2000.\n\n\n\n\n\n\nYour turn!\n\n\n\nSelect a country of your choice from global_economy, then calculate and visualise the the GDP per capita over time (that is, the GDP scaled by the population).\n\n\n\n\n\n\n\n\nTricky to forecast\n\n\n\nWhile transformations help to make the patterns simpler to forecast, if additional information like CPI or Population are used then they will also need to be forecasted. While population is generally easy to forecast, CPI could be more complicated to forecast than the thing youâ€™re originally forecasting!\n\n\nAnother useful transformation is calendar adjustments. This adjusts the observations in the time series to represent an equivalent time period, and can simplify seasonal patterns which result from these different lengths. This is particularly useful for monthly data, since the number of days in each month varies substantially.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nCalculate the monthly total Australian retail turnover from aus_retail and visualise the seasonal pattern. Then scale by the number of days in each month to calculate the daily average turnover and comparse the seasonal patterns.\n\n\nMathematical transformations are useful since they donâ€™t require providing any future values to produce the forecasts. Log and power transformations (\\(y^k\\), for example square root, square, and inverse) are particularly helpful for regularising variation proportional to the level of the series.\n\n\n\n\n\n\n\n\n\nThis proportional variance is common in time series, in Victoriaâ€™s cafe and restaurant turnover you can see small changes when turnover is low (before 2000), and is much larger after 2010 when turnover is much larger.\n\n\n\n\n\n\n\n\n\nLog transforming the data changes this variation to be more consistent, where the variation before 2000 is now more similar to after 2010.\nThe box-cox transformation family parameterises the range of power transformations for more precise adjustments. The transformation parameter \\(\\lambda\\) controls the strength of the transformation, with \\(\\lambda=1\\) being no change in shape, \\(\\lambda = 0\\) being a log transformation and others being equivalent in shape to \\(y^\\lambda\\).\nThe log transformation above was a bit strong, so letâ€™s try something slightly close to \\(\\lambda=1\\) - perhaps \\(\\lambda = 0.1\\)?\n\n\n\n\n\n\n\n\n\nThe variation is now consistent for the entire series, and the trend is linear.\n\n\n\n\n\n\nAutomatic box-cox transformations\n\n\n\nThe \\(\\lambda\\) parameter can be automatically computed using the guerrero() function.\n\n\nWe can calculate the optimal box-cox parameter using features() and guerrero():\n\n\n# A tibble: 1 Ã— 3\n  State    Industry                                 lambda_guerrero\n  &lt;chr&gt;    &lt;chr&gt;                                              &lt;dbl&gt;\n1 Victoria Cafes, restaurants and catering services           0.173\n\n\nLooks like we were pretty close with \\(\\lambda = 0.1\\), letâ€™s try using this more precise estimate:\n\n\n\n\n\n\n\n\n\nThe optimised box-cox transformation is very similar to \\(\\lambda = 0.1\\) - fortunately for us we donâ€™t have to be precise since this transformation isnâ€™t sensitive to your choice of parameter. So long as you are within \\(\\pm 0.1\\) the transformation should be okay. Additionally, if \\(\\lambda \\approx 0\\) then it is common to instead use the simpler log() transformation.\n\n\n\n\n\n\nYour turn!\n\n\n\nFind a suitable box-cox transformation for the monthly total Australian retail turnover, then compare your choice with the automatically selected parameter from the guerrero() feature."
  },
  {
    "objectID": "sessions/day2/exercises.html#decomposition",
    "href": "sessions/day2/exercises.html#decomposition",
    "title": "Exercises",
    "section": "",
    "text": "Another commonly used transformation/adjustment requires a model to decompose the time series into its components. Seasonally adjusted time series are often used by analysts and policy makers to evaluate the underlying long term trends without the added complexity of seasonality. The STL decomposition is useful model which can isolate the seasonal pattern from the trend and remainder for many types of time series.\nThe STL decomposition separates time series into the form \\(Y = \\text{trend} + \\text{seasonality} + \\text{remainder}\\). Since this is an additive decomposition, we must first simplify any multiplicative patterns into additive ones using a suitable power transformation. Letâ€™s try to remove the annual seasonality from Australiaâ€™s print media turnover.\n\n\n\n\n\n\n\n\n\nThe seasonality is more varied when turnover increases, so we must transform the data before estimating the STL model.\n\n\n\n\n\n\n\n\n\nThe log transformation (\\(\\lambda = 0\\)) does a great job at producing a consistent variation throughout the series. You could try to find a better transformation using the box-cox transformation family, however there is no need for it here.\nWe can estimate the STL model using STL() as follows:\n\n\n# A mable: 1 x 1\n  `STL(log(Turnover))`\n               &lt;model&gt;\n1                &lt;STL&gt;\n\n\nThe decomposition can be obtained from the model using components(), and then all of the components can be plotted with autoplot():\n\n\n# A dable: 441 x 7 [1M]\n# Key:     .model [1]\n# :        log(Turnover) = trend + season_year + remainder\n   .model        Month `log(Turnover)` trend season_year remainder season_adjust\n   &lt;chr&gt;         &lt;mth&gt;           &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n 1 STL(log(Tâ€¦ 1982 Apr            4.90  4.92     -0.0723   0.0528           4.97\n 2 STL(log(Tâ€¦ 1982 May            4.89  4.92     -0.0128  -0.0151           4.91\n 3 STL(log(Tâ€¦ 1982 Jun            4.84  4.93     -0.0942   0.00869          4.94\n 4 STL(log(Tâ€¦ 1982 Jul            4.86  4.93     -0.0480  -0.0303           4.90\n 5 STL(log(Tâ€¦ 1982 Aug            4.88  4.94     -0.0155  -0.0458           4.89\n 6 STL(log(Tâ€¦ 1982 Sep            4.90  4.94     -0.0453  -0.00317          4.94\n 7 STL(log(Tâ€¦ 1982 Oct            4.93  4.95     -0.0167   0.00114          4.95\n 8 STL(log(Tâ€¦ 1982 Nov            4.97  4.95      0.0181  -0.00245          4.95\n 9 STL(log(Tâ€¦ 1982 Dec            5.26  4.96      0.258    0.0398           5.00\n10 STL(log(Tâ€¦ 1983 Jan            4.92  4.97     -0.0180  -0.0249           4.94\n# â„¹ 431 more rows\n\n\n\n\n\n\n\n\n\nThe components are obtained using rolling estimation windows, which are the main way the decomposition is changed. A large window produces smooth components, and a small window produces flexible and quickly changing components.\n\n\n\n\n\n\n\n\n\nThe infinite window for the seasonality results in a seasonal pattern that doesnâ€™t change over time, while the small trend window allows the trend to change very quickly. The best choice of estimation window should produce components that match the patterns in the original data while being as smooth as possible.\n\n\n\n\n\n\n\n\n\nA trend window of 25 for this dataset produces a mostly smooth trend component which can still react to brief decreases in turnover. The constant seasonal pattern (infinite window) is reasonable for this dataset since the seasonality doesnâ€™t change much over time.\n\n\n\n\n\n\nSeasonal adjustment\n\n\n\nYou can find the de-seasonalised data in the season_adjust column of the components() output.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nFind a suitable STL decomposition for the total Australian retail turnover, then produce and visualise the seasonally adjusted time series.\nHint: donâ€™t forget to use the suitable transformation found previously!\n\n\nSeasonal decomposition also makes it easier to take a look at the seasonality - we can use a combination of seasonal plots and decomposition to more easily see seasonal patterns.\n\n\n\n\n\n\n\n\n\nIf the seasonal window allows the seasonal component to change over time, the gg_subseries() plot is especially useful for seeing how the pattern changes.\n\n\n\n\n\n\n\n\n\nJanuary and December seem to increase over time, while April, May and June are decreasing.\n\n\n\n\n\n\nYour turn!\n\n\n\nProduce appropriate seasonal plots of the seasonal component from your STL decomposition on Australian retail turnover."
  },
  {
    "objectID": "sessions/day2/exercises.html#features",
    "href": "sessions/day2/exercises.html#features",
    "title": "Exercises",
    "section": "",
    "text": "A useful technique for visualising large collections of time series is to produce summaries of their patterns known as features. Visualising many time series simultaneously is difficult since the scale and shape of patterns can vary substantially. The features() function will compute single value summaries over time such as the strength of trend or seasonality. There are many features available, but the features from STL decompositions are particularly interesting.\n\n\n# A tibble: 152 Ã— 11\n   State       Industry trend_strength seasonal_strength_year seasonal_peak_year\n   &lt;chr&gt;       &lt;chr&gt;             &lt;dbl&gt;                  &lt;dbl&gt;              &lt;dbl&gt;\n 1 Australianâ€¦ Cafes, â€¦          0.989                  0.562                  0\n 2 Australianâ€¦ Cafes, â€¦          0.993                  0.629                  0\n 3 Australianâ€¦ Clothinâ€¦          0.991                  0.923                  9\n 4 Australianâ€¦ Clothinâ€¦          0.993                  0.957                  9\n 5 Australianâ€¦ Departmâ€¦          0.977                  0.980                  9\n 6 Australianâ€¦ Electriâ€¦          0.992                  0.933                  9\n 7 Australianâ€¦ Food reâ€¦          0.999                  0.890                  9\n 8 Australianâ€¦ Footweaâ€¦          0.982                  0.944                  9\n 9 Australianâ€¦ Furnituâ€¦          0.981                  0.687                  9\n10 Australianâ€¦ Hardwarâ€¦          0.992                  0.900                  9\n# â„¹ 142 more rows\n# â„¹ 6 more variables: seasonal_trough_year &lt;dbl&gt;, spikiness &lt;dbl&gt;,\n#   linearity &lt;dbl&gt;, curvature &lt;dbl&gt;, stl_e_acf1 &lt;dbl&gt;, stl_e_acf10 &lt;dbl&gt;\n\n\nIn particular, features from STL decompositions allow you to compare the strength of trend and seasonality between many time series.\n\n\n\n\n\n\n\n\n\nFrom this we can see that almost all time series have a strong trend, while the strength of seasonality is more varied - some series have strong seasonality while others have less seasonality.\n\n\n\n\n\n\nYour turn!\n\n\n\nCalculate the STL features for the time series in the tourism dataset. Try colouring the points in the scatterplot by the purpose of travel, are some reasons more trended or seasonal than others?\n\n\nThere are many other features that you can use - check out the documentation for ?features_by_pkg. You can produce a feature set of similar features using the feature_set() function.\n\n\n# A tibble: 152 Ã— 13\n   State      Industry  acf1 acf10 diff1_acf1 diff1_acf10 diff2_acf1 diff2_acf10\n   &lt;chr&gt;      &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n 1 Australiaâ€¦ Cafes, â€¦ 0.973  8.59     -0.348       0.239     -0.572       0.502\n 2 Australiaâ€¦ Cafes, â€¦ 0.977  8.65     -0.327       0.259     -0.552       0.531\n 3 Australiaâ€¦ Clothinâ€¦ 0.885  7.01     -0.276       0.251     -0.507       0.339\n 4 Australiaâ€¦ Clothinâ€¦ 0.846  6.33     -0.303       0.201     -0.532       0.321\n 5 Australiaâ€¦ Departmâ€¦ 0.500  1.60     -0.310       0.202     -0.540       0.315\n 6 Australiaâ€¦ Electriâ€¦ 0.902  7.29     -0.247       0.324     -0.506       0.455\n 7 Australiaâ€¦ Food reâ€¦ 0.984  9.13     -0.394       0.585     -0.611       1.12 \n 8 Australiaâ€¦ Footweaâ€¦ 0.760  4.64     -0.325       0.155     -0.566       0.333\n 9 Australiaâ€¦ Furnituâ€¦ 0.952  7.67     -0.190       0.163     -0.530       0.394\n10 Australiaâ€¦ Hardwarâ€¦ 0.957  7.67     -0.104       0.101     -0.497       0.311\n# â„¹ 142 more rows\n# â„¹ 5 more variables: season_acf1 &lt;dbl&gt;, pacf5 &lt;dbl&gt;, diff1_pacf5 &lt;dbl&gt;,\n#   diff2_pacf5 &lt;dbl&gt;, season_pacf &lt;dbl&gt;"
  },
  {
    "objectID": "sessions/day4/exercises.html",
    "href": "sessions/day4/exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Exponential smoothing and ARIMA models allow more dynamic patterns to be modelled for forecasting. This allows the model to learn and adapt to changes in the trend, seasonality, and other subtle patterns to produce forecasts that accurately reflect the most recent data.\n\n\nExponential smoothing models can handle both additive and multiplicative patterns, and models the series based on three primary components: the error, trend, and seasonality (ETS). Since multiplicative patterns can be directly modelled with ETS, there is no need to transform the data.\nETS models are specified using the ETS() function.\n\n\n\n\n\n\nETS specials\n\n\n\nThe specials for the ETS model describe the structure of the three components.\n\nerror(): The structure of the error, either additive (\"A\") or multiplicative (\"M\")\ntrend(): The structure of the error, either none (\"N\"), additive (\"A\") or multiplicative (\"M\"). Adding \"d\" afterwards (\"Ad\" or \"Md\") will dampen the trend, flattening it out into the future.\nseason(): The structure of the seasonality, either none (\"N\"), additive (\"A\") or multiplicative (\"M\").\n\nMore information is available in the ?ETS help file.\n\n\nAdditive components have a constant variation with the level of the series, while multiplicative components have a proportional variation to the level of the series.\nFor trend, an additive trend is mostly straight (linear) while a multiplicative trend curves up or down (exponential).\n\n\n\n\n\n\n\n\n\nAustraliaâ€™s population has a linear (additive) trend, while the GDP has an exponential (multiplicative) trend.\nYou can also notice that the error (randomness) is additive for population, but multiplicative for GDP since the series is more variable at larger GDP timepoints.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeasonality is multiplicative if its shape gets larger as the series increases, which is the case for Australian retail turnover. Australian holiday tourism is more additive, since the size of the seasonality does not change much as the number of trips increases or decreases.\nETS models without specials will automatically select the most appropriate model for the data.\n\n\n# A mable: 1 x 1\n  `ETS(Turnover)`\n          &lt;model&gt;\n1    &lt;ETS(M,A,M)&gt;\n\n\nThe automatically selected ETS(M,A,M) model matches the patterns in the Australian retail turnover data. The trend is a straight line (additive), while the error and seasonality grows proportionately to the amount of turnover (multiplicative).\nAs was done with the benchmark and regression models, we can:\n\nlook at the model with tidy(), glance(), and augment(),\nproduce forecasts with forecast().\n\nLetâ€™s see how the ETS forecasts look.\n\n\n\n\n\n\n\n\n\nVery nice! The trend matches the history, and the seasonality grows in size much like the historical data. You can also see the forecasts are very confident (small intervals) for the first two years, but get less confident (larger intervals) as we forecast further into the future.\n\n\n\n\n\n\nYour turn!\n\n\n\nProduce forecasts from an automatically selected ETS model for Australiaâ€™s print media turnover. Does the chosen ETS model align with the patterns you see in the data?\n\n\n\n\n\nARIMA models can also capture trends and seasonality, but it is achieved in a very different way. Rather than directly representing the structure of the trend and seasonal components like ETS, it uses a combination of data transformation, past values and past errors to produce forecasts.\n\n\n\n\n\n\nTransformations needed!\n\n\n\nUnlike ETS models, an ARIMA model must have additive patterns in the data to work well. So if the data has multiplicative seasonality, use an appropriate transformation to simplify the patterns before estimating an ARIMA model.\n\n\nARIMA models are specified using the ARIMA() function.\n\n\n\n\n\n\nARIMA specials\n\n\n\nThe specials for the ARIMA model describe the number of lags and differences in the model.\n\npdq(): The non-seasonal parameters\nPDQ(): The seasonal parameters\n\nThe number of autoregressive (AR) lags are set by p and P, while the number of moving average (MA) lags are set by q and Q.\nThe long term forecasting behaviour (trends and seasonality) are controlled by the differences d and D, where a seasonal difference imparts seasonality and two or more differences (or constants) produces a trend.\nMore information is available in the ?ARIMA help file.\n\n\nMuch like ETS models, an automatically selected ARIMA will be estimated if the specials arenâ€™t specified.\n\n\n\n\n\n\n\n\n\nA box-cox transformation with \\(\\lambda = 0.2\\) seems to convert the multiplicative patterns in the data into additive ones. We need to use this transformation when estimating an ARIMA model.\n\n\n# A mable: 1 x 1\n  `ARIMA(box_cox(Turnover, 0.2))`\n                          &lt;model&gt;\n1       &lt;ARIMA(3,1,0)(2,1,1)[12]&gt;\n\n\nThe chosen model contains a seasonal difference and a non-seasonal difference, with 2 seasonal AR and 3 non-seasonal AR lags and 1 seasonal MA term.\n\n\n\n\n\n\n\n\n\nThe forecasts from this model also look good!\n\n\n\n\n\n\nYour turn!\n\n\n\nProduce forecasts from an automatically selected ARIMA model for Australiaâ€™s print media turnover. Donâ€™t forget to find and use a suitable transformation to simplify the multiplicative patterns in the data.\n\n\n\n\n\nBoth ETS and ARIMA models work well for time series that contain trends and seasonality, so which works better? ETS models are well suited to time series with multiplicative patterns since they can be directly modelled without transformations. ETS is also more explainable, since the common temporal patterns of trend and seasonality are directly modelled. ARIMA models however are more capable at incorporating subtle patterns like short-term correlations into the forecasts, and are capable of modelling cyclical data. Often pracitioners use an average of the forecasts from both models to produce a more accurate forecast (ensembling).\nIn the day 5 materials we will learn about accuracy measures, which will allow us to more precisely identify which model works best on specific datasets."
  },
  {
    "objectID": "sessions/day4/exercises.html#expontential-smoothing",
    "href": "sessions/day4/exercises.html#expontential-smoothing",
    "title": "Exercises",
    "section": "",
    "text": "Exponential smoothing models can handle both additive and multiplicative patterns, and models the series based on three primary components: the error, trend, and seasonality (ETS). Since multiplicative patterns can be directly modelled with ETS, there is no need to transform the data.\nETS models are specified using the ETS() function.\n\n\n\n\n\n\nETS specials\n\n\n\nThe specials for the ETS model describe the structure of the three components.\n\nerror(): The structure of the error, either additive (\"A\") or multiplicative (\"M\")\ntrend(): The structure of the error, either none (\"N\"), additive (\"A\") or multiplicative (\"M\"). Adding \"d\" afterwards (\"Ad\" or \"Md\") will dampen the trend, flattening it out into the future.\nseason(): The structure of the seasonality, either none (\"N\"), additive (\"A\") or multiplicative (\"M\").\n\nMore information is available in the ?ETS help file.\n\n\nAdditive components have a constant variation with the level of the series, while multiplicative components have a proportional variation to the level of the series.\nFor trend, an additive trend is mostly straight (linear) while a multiplicative trend curves up or down (exponential).\n\n\n\n\n\n\n\n\n\nAustraliaâ€™s population has a linear (additive) trend, while the GDP has an exponential (multiplicative) trend.\nYou can also notice that the error (randomness) is additive for population, but multiplicative for GDP since the series is more variable at larger GDP timepoints.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeasonality is multiplicative if its shape gets larger as the series increases, which is the case for Australian retail turnover. Australian holiday tourism is more additive, since the size of the seasonality does not change much as the number of trips increases or decreases.\nETS models without specials will automatically select the most appropriate model for the data.\n\n\n# A mable: 1 x 1\n  `ETS(Turnover)`\n          &lt;model&gt;\n1    &lt;ETS(M,A,M)&gt;\n\n\nThe automatically selected ETS(M,A,M) model matches the patterns in the Australian retail turnover data. The trend is a straight line (additive), while the error and seasonality grows proportionately to the amount of turnover (multiplicative).\nAs was done with the benchmark and regression models, we can:\n\nlook at the model with tidy(), glance(), and augment(),\nproduce forecasts with forecast().\n\nLetâ€™s see how the ETS forecasts look.\n\n\n\n\n\n\n\n\n\nVery nice! The trend matches the history, and the seasonality grows in size much like the historical data. You can also see the forecasts are very confident (small intervals) for the first two years, but get less confident (larger intervals) as we forecast further into the future.\n\n\n\n\n\n\nYour turn!\n\n\n\nProduce forecasts from an automatically selected ETS model for Australiaâ€™s print media turnover. Does the chosen ETS model align with the patterns you see in the data?"
  },
  {
    "objectID": "sessions/day4/exercises.html#arima",
    "href": "sessions/day4/exercises.html#arima",
    "title": "Exercises",
    "section": "",
    "text": "ARIMA models can also capture trends and seasonality, but it is achieved in a very different way. Rather than directly representing the structure of the trend and seasonal components like ETS, it uses a combination of data transformation, past values and past errors to produce forecasts.\n\n\n\n\n\n\nTransformations needed!\n\n\n\nUnlike ETS models, an ARIMA model must have additive patterns in the data to work well. So if the data has multiplicative seasonality, use an appropriate transformation to simplify the patterns before estimating an ARIMA model.\n\n\nARIMA models are specified using the ARIMA() function.\n\n\n\n\n\n\nARIMA specials\n\n\n\nThe specials for the ARIMA model describe the number of lags and differences in the model.\n\npdq(): The non-seasonal parameters\nPDQ(): The seasonal parameters\n\nThe number of autoregressive (AR) lags are set by p and P, while the number of moving average (MA) lags are set by q and Q.\nThe long term forecasting behaviour (trends and seasonality) are controlled by the differences d and D, where a seasonal difference imparts seasonality and two or more differences (or constants) produces a trend.\nMore information is available in the ?ARIMA help file.\n\n\nMuch like ETS models, an automatically selected ARIMA will be estimated if the specials arenâ€™t specified.\n\n\n\n\n\n\n\n\n\nA box-cox transformation with \\(\\lambda = 0.2\\) seems to convert the multiplicative patterns in the data into additive ones. We need to use this transformation when estimating an ARIMA model.\n\n\n# A mable: 1 x 1\n  `ARIMA(box_cox(Turnover, 0.2))`\n                          &lt;model&gt;\n1       &lt;ARIMA(3,1,0)(2,1,1)[12]&gt;\n\n\nThe chosen model contains a seasonal difference and a non-seasonal difference, with 2 seasonal AR and 3 non-seasonal AR lags and 1 seasonal MA term.\n\n\n\n\n\n\n\n\n\nThe forecasts from this model also look good!\n\n\n\n\n\n\nYour turn!\n\n\n\nProduce forecasts from an automatically selected ARIMA model for Australiaâ€™s print media turnover. Donâ€™t forget to find and use a suitable transformation to simplify the multiplicative patterns in the data."
  },
  {
    "objectID": "sessions/day4/exercises.html#ets-or-arima",
    "href": "sessions/day4/exercises.html#ets-or-arima",
    "title": "Exercises",
    "section": "",
    "text": "Both ETS and ARIMA models work well for time series that contain trends and seasonality, so which works better? ETS models are well suited to time series with multiplicative patterns since they can be directly modelled without transformations. ETS is also more explainable, since the common temporal patterns of trend and seasonality are directly modelled. ARIMA models however are more capable at incorporating subtle patterns like short-term correlations into the forecasts, and are capable of modelling cyclical data. Often pracitioners use an average of the forecasts from both models to produce a more accurate forecast (ensembling).\nIn the day 5 materials we will learn about accuracy measures, which will allow us to more precisely identify which model works best on specific datasets."
  },
  {
    "objectID": "sessions/day5/diagnostics.html",
    "href": "sessions/day5/diagnostics.html",
    "title": "Residual diagnostics and cross validation",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "sessions/day4/ets.html",
    "href": "sessions/day4/ets.html",
    "title": "ETS",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "sessions/day3/regressors.html",
    "href": "sessions/day3/regressors.html",
    "title": "Forecasting with regression, how to represent temporal structure with regressors",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "sessions/day2/features.html",
    "href": "sessions/day2/features.html",
    "title": "Computing and visualizing features",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "sessions/day1/graphics.html",
    "href": "sessions/day1/graphics.html",
    "title": "Time series patterns and basic graphics",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "sessions/day4/resources.html#code-demonstrated-in-workshop-sessions",
    "href": "sessions/day4/resources.html#code-demonstrated-in-workshop-sessions",
    "title": "Resources (recording and code)",
    "section": "Code demonstrated in workshop sessions",
    "text": "Code demonstrated in workshop sessions\nClick here to download the demo code from this session"
  },
  {
    "objectID": "sessions/day2/resources.html",
    "href": "sessions/day2/resources.html",
    "title": "Resources (recording and code)",
    "section": "",
    "text": "Recordings from previous years\n\n\n\n\n\n2023"
  },
  {
    "objectID": "sessions/day2/resources.html#recording",
    "href": "sessions/day2/resources.html#recording",
    "title": "Resources (recording and code)",
    "section": "",
    "text": "Recordings from previous years\n\n\n\n\n\n2023"
  },
  {
    "objectID": "sessions/day2/resources.html#code-demonstrated-in-workshop-sessions",
    "href": "sessions/day2/resources.html#code-demonstrated-in-workshop-sessions",
    "title": "Resources (recording and code)",
    "section": "Code demonstrated in workshop sessions",
    "text": "Code demonstrated in workshop sessions\nClick here to download the demo code from this session"
  },
  {
    "objectID": "sessions/break.html",
    "href": "sessions/break.html",
    "title": "Coffee Break",
    "section": "",
    "text": "Time for a coffee break!\nFeel free to ask me some questions, or simply enjoy the break."
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "This work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "Tidy time series & forecasting in R",
    "section": "Course Overview",
    "text": "Course Overview\nForecasting is a valuable tool that allows organizations to make informed decisions about the future. Time series forecasting, in particular, uses historical data to predict future trends over time. This technique has extensive applications across a wide range of fields, including finance and economics, health and humanitarian operations, supply chain management, and more. By analyzing trends and patterns in data, time series forecasting can help decision-makers identify potential challenges and opportunities, and plan accordingly.\nIt is important for researchers in Low- and Middle-Income Countries (LMICs) to develop technical skill in data analysis and forecasting techniques, which are essential for accurate and reliable forecasting. By having these skills, researchers can analyze data, identify trends and patterns, and develop robust forecasting models to make informed decisions that can improve resource allocation and planning in LMICs. Additionally, researchers can collaborate with policy makers and stakeholders to ensure that the forecast results are integrated into decision-making processes, leading to more efficient and effective resource management strategies.\nThis workshop is part of the Forecasting for Social Good (F4SG) initiative, and will run online from the 23rd-27th September 2024."
  },
  {
    "objectID": "index.html#learning-objectives",
    "href": "index.html#learning-objectives",
    "title": "Tidy time series & forecasting in R",
    "section": "Learning objectives",
    "text": "Learning objectives\nDuring the training, participants will gain knowledge and skills in:\n\nPreparing time series data for analysis and exploration.\nExtracting and computing useful features from time series data and effectively visualizing it.\nIdentifying appropriate forecasting algorithms for time series and selecting the best approach for the data at hand."
  },
  {
    "objectID": "index.html#instructor",
    "href": "index.html#instructor",
    "title": "Tidy time series & forecasting in R",
    "section": "Instructor",
    "text": "Instructor\n Mitchell Oâ€™Hara-Wild (he/him) is a PhD candidate at Monash University, creating new techniques and tools for forecasting large collections of time series with Rob Hyndman and George Athanasopoulos. He is the lead developer of the tidy time-series forecasting tools fable and feasts, and has co-developed the widely used forecast package since 2015. Mitchell also operates a data consultancy, and has worked on many forecasting projects that have supported decision making and planning for businesses and governments. He is an award-winning educator, and has taught applied forecasting at Monash University and various forecasting workshops around the world."
  },
  {
    "objectID": "index.html#instructor-1",
    "href": "index.html#instructor-1",
    "title": "Tidy time series & forecasting in R",
    "section": "Instructor",
    "text": "Instructor\n\nBahman is a Reader (Associate Professor) in Data-Driven Decision Science at Cardiff Business School, Cardiff University, UK. He serves as the director of the Data Lab for Social Good Research Group at Cardiff University and is also the founder of the Forecasting for Social Good committee within the International Institute of Forecasters. Bahman specializes in the development and application of modelling, forecasting and management science tools and techniques providing informed insights for planning & decision-making processes in sectors contributing to social good, including healthcare operations, global health and humanitarian supply chains, agriculture and food, social sustainability, and governmental policy. His collaborative efforts have spanned a multitude of organisations, including notable bodies such as the National Health Service (NHS), Welsh Ambulance Service Trusts (WAST), United States Agency for International Developments (USAID), the International Committee of the Red Cross (ICRC), and John Snow Inc.Â (JSI). A remarkable highlight of his contributions is his pivotal role in disseminating forecasting knowledge especially in low and lower-middle income countries through the democratizing forecasting project sponsored by International Institute of Forecasters."
  },
  {
    "objectID": "index.html#mentors-for-the-cohort-2024",
    "href": "index.html#mentors-for-the-cohort-2024",
    "title": "Tidy time series & forecasting in R",
    "section": "Mentors for the cohort 2024",
    "text": "Mentors for the cohort 2024\nA committed team, comprising both PhD students and MSc. students, generously dedicates their time and expertise to offer valuable support to learners throughout the duration of the workshop to help learners with the exercises.\nThe team includes:\n\nAkar Stephen Eghelakpo\nAgnes Njambi Wanjau\nCaroline Mugo\nDebra Ukamaka Okeh\nNjeri Kennedy Mungâ€™are\nElizabeth Anyango Magero\nIvy Lucy Owuor\nLumumba Wandera Victor\nMatabel Odin\nVerrah Otiende\nWinnie Chacha\nAbel Mokua Nyabera\nHerbert Imboga\nHarsha Charma\nMustafa Aslan\nMingshe Zhi\nUdeshi Salgado"
  },
  {
    "objectID": "index.html#project-coordination",
    "href": "index.html#project-coordination",
    "title": "Tidy time series & forecasting in R",
    "section": "Project coordination",
    "text": "Project coordination\nThe coordination and administration of the project are overseen by a dedicated team from Jomo Kenyatta University of Agriculture and Technology.\n\nJomo Kenyatta University team includes:\n\nHenry Kissinger Ochieng\nCaroline Mugo\nWinnie Chacha\nSamuel Mwalili"
  },
  {
    "objectID": "index.html#required-equipment",
    "href": "index.html#required-equipment",
    "title": "Tidy time series & forecasting in R",
    "section": "Required equipment",
    "text": "Required equipment\nPlease have your own laptop capable of running R."
  },
  {
    "objectID": "index.html#required-software",
    "href": "index.html#required-software",
    "title": "Tidy time series & forecasting in R",
    "section": "Required software",
    "text": "Required software\nTo be able to complete the exercises of this workshop, please install a suitable IDE (such as RStudio), a recent version of R (4.1+) and the following packages.\n\nTime series packages and extensions\n\nfpp3, sugrrants\n\ntidyverse packages and friends\n\ntidyverse, fpp3\n\n\nThe following code will install the main packages needed for the workshop.\ninstall.packages(c(\"tidyverse\",\"fpp3\", \"GGally\", \"sugrrants\", \"astsa\"))\nPlease have the required software installed and pre-work completed before attending the workshop."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "All times are listed in UK time (UTC+1)."
  },
  {
    "objectID": "schedule.html#day-1",
    "href": "schedule.html#day-1",
    "title": "Schedule",
    "section": "Day 1",
    "text": "Day 1\n\n\n\n\n\nTime\n\n\nSession\n\n\n\n\n\n\n08:00-09:30\n\n\nBasics of time series and data structures\n\n\n\n\n09:45-11:15\n\n\nTime series patterns and basic graphics\n\n\n\n\nCheck with your mentor on Slack\n\n\nExercises\n\n\n\n\nÂ \n\n\nResources (recording and code)\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "schedule.html#day-2",
    "href": "schedule.html#day-2",
    "title": "Schedule",
    "section": "Day 2",
    "text": "Day 2\n\n\n\n\n\nTime\n\n\nSession\n\n\n\n\n\n\n08:00-09:30\n\n\nTransforming / adjusting time series\n\n\n\n\n09:45-11:15\n\n\nComputing and visualizing features\n\n\n\n\nCheck with your mentor on Slack\n\n\nExercises\n\n\n\n\nÂ \n\n\nResources (recording and code)\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "schedule.html#day-3",
    "href": "schedule.html#day-3",
    "title": "Schedule",
    "section": "Day 3",
    "text": "Day 3\n\n\n\n\n\nTime\n\n\nSession\n\n\n\n\n\n\n08:00-09:30\n\n\nBasic modeling / forecasting\n\n\n\n\n09:45-11:15\n\n\nForecasting with regression, how to represent temporal structure with regressors\n\n\n\n\nCheck with your mentor on Slack\n\n\nExercises\n\n\n\n\nÂ \n\n\nResources (recording and code)\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "schedule.html#day-4",
    "href": "schedule.html#day-4",
    "title": "Schedule",
    "section": "Day 4",
    "text": "Day 4\n\n\n\n\n\nTime\n\n\nSession\n\n\n\n\n\n\n08:00-09:30\n\n\nARIMA\n\n\n\n\n09:45-11:15\n\n\nETS\n\n\n\n\nCheck with your mentor on Slack\n\n\nExercises\n\n\n\n\nÂ \n\n\nResources (recording and code)\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "schedule.html#day-5",
    "href": "schedule.html#day-5",
    "title": "Schedule",
    "section": "Day 5",
    "text": "Day 5\n\n\n\n\n\nTime\n\n\nSession\n\n\n\n\n\n\n08:00-09:30\n\n\nBasic training and test accuracy\n\n\n\n\n09:45-11:15\n\n\nResidual diagnostics and cross validation\n\n\n\n\nCheck with your mentor on Slack\n\n\nExercises\n\n\n\n\nÂ \n\n\nResources (recording and code)\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "sessions/day1/resources.html",
    "href": "sessions/day1/resources.html",
    "title": "Resources (recording and code)",
    "section": "",
    "text": "Recordings from previous years\n\n\n\n\n\n2023"
  },
  {
    "objectID": "sessions/day1/resources.html#recording",
    "href": "sessions/day1/resources.html#recording",
    "title": "Resources (recording and code)",
    "section": "",
    "text": "Recordings from previous years\n\n\n\n\n\n2023"
  },
  {
    "objectID": "sessions/day1/resources.html#code-demonstrated-in-workshop-sessions",
    "href": "sessions/day1/resources.html#code-demonstrated-in-workshop-sessions",
    "title": "Resources (recording and code)",
    "section": "Code demonstrated in workshop sessions",
    "text": "Code demonstrated in workshop sessions\nClick here to download the demo code from this session"
  },
  {
    "objectID": "sessions/day3/resources.html",
    "href": "sessions/day3/resources.html",
    "title": "Resources (recording and code)",
    "section": "",
    "text": "Recordings from previous years\n\n\n\n\n\n2023"
  },
  {
    "objectID": "sessions/day3/resources.html#recording",
    "href": "sessions/day3/resources.html#recording",
    "title": "Resources (recording and code)",
    "section": "",
    "text": "Recordings from previous years\n\n\n\n\n\n2023"
  },
  {
    "objectID": "sessions/day3/resources.html#code-demonstrated-in-workshop-sessions",
    "href": "sessions/day3/resources.html#code-demonstrated-in-workshop-sessions",
    "title": "Resources (recording and code)",
    "section": "Code demonstrated in workshop sessions",
    "text": "Code demonstrated in workshop sessions\nClick here to download the demo code from this session"
  },
  {
    "objectID": "sessions/day5/resources.html#code-demonstrated-in-workshop-sessions",
    "href": "sessions/day5/resources.html#code-demonstrated-in-workshop-sessions",
    "title": "Resources (recording and code)",
    "section": "Code demonstrated in workshop sessions",
    "text": "Code demonstrated in workshop sessions\nClick here to download the demo code from this session"
  },
  {
    "objectID": "sessions/day2/adjustments.html",
    "href": "sessions/day2/adjustments.html",
    "title": "Transforming / adjusting time series",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "sessions/day3/forecasting.html",
    "href": "sessions/day3/forecasting.html",
    "title": "Basic modeling / forecasting",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "sessions/day4/arima.html",
    "href": "sessions/day4/arima.html",
    "title": "ARIMA",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "sessions/day5/accuracy.html",
    "href": "sessions/day5/accuracy.html",
    "title": "Basic training and test accuracy",
    "section": "",
    "text": "View slides in full screen"
  },
  {
    "objectID": "sessions/day1/basics.html",
    "href": "sessions/day1/basics.html",
    "title": "Basics of time series and data structures",
    "section": "",
    "text": "View slides in full screen\n\nView slides in full screen"
  },
  {
    "objectID": "sessions/day5/exercises.html",
    "href": "sessions/day5/exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "There are several approaches which can be used to evaluate the performance of a forecasting model. These methods can be split into two main categories: accuracy summaries and diagnostic checks. Accuracy summaries are single valued representations of the modelâ€™s performance (like features, but on forecast errors), while diagnostic checks involve plotting the residuals to identify any shortcomings in the model.\n\n\nIn the previous exercise we looked at forecasting the total Australian retail turnover using ETS and ARIMA models, and we produced these forecasts:\n\n\n\n\n\n\n\n\n\nPlotting the forecasts simultaneously makes it easy to compare them. This allows us to see that while the seasonality and intervals from both models are similar, the trend from ARIMA is stronger than ETS. But which model is better?\nTo answer this we can summarise the forecasting performance of the models. The simplest is the accuracy on the historical training data.\nIn addition to the forecasts shown above, we produce 1-step forecasts on the training data when fitting the model. We can obtain these from the .fitted column of the augment() output.\n\n\n\n\n\n\n\n\n\nBoth models match the historical data closely, but which is more accurate? For this we can use the accuracy() function, which summarises the errors into a single summary statistic.\n\n\n# A tibble: 2 Ã— 10\n  .model .type        ME  RMSE   MAE     MPE  MAPE  MASE RMSSE    ACF1\n  &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 ets    Training   7.76  421.  327.  0.0486  1.55 0.261 0.294 -0.101 \n2 arima  Training -15.4   382.  277. -0.0713  1.22 0.221 0.266  0.0797\n\n\nBy default you will see a set of statistics that summarise the point forecasting accuracy, where the models closer to 0 are more accurate. MAE and RMSE are commonly used, but their scale independent versions MASE and RMSSE are useful if youâ€™re comparing between multiple datasets.\nIn all accuracy metrics we see that the ARIMA model is more accurate on the training data. But is it more accurate for forecasting?\n\n\n\n\n\n\nYour turn!\n\n\n\nCompare the in-sample accuracy statistics for all models used to forecast Australiaâ€™s print media turnover. Which is most accurate, and which is least? Does this align with your expectations?\n\n\nA more genuine approach to calculating forecasting performance is to use a training and test set split. This separates the data into two parts, and because the test data is not used in producing the forecasts, it should provide a reliable indication of how well the model is likely to forecast on new data.\nTo withhold some data for forecast evaluation, we first filter() the data to exclude the test period before training the model.\n\n\n\n\n\n\n\n\n\nWe can then calculate the same accuracy metrics on the forecasted test set using accuracy() again, but this time we need to provide the data used.\n\n\n# A tibble: 2 Ã— 10\n  .model .type     ME  RMSE   MAE   MPE  MAPE  MASE RMSSE    ACF1\n  &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 arima  Test  -1217. 1340. 1217. -2.41  2.41 0.979 0.935  0.0605\n2 ets    Test   -737.  913.  761. -1.44  1.49 0.612 0.637 -0.199 \n\n\nLooks like ETS was actually more accurate at forecasting the two year test set.\n\n\n\n\n\n\nYour turn!\n\n\n\nNow compare the out-of-sample (test set) accuracy statistics for all models used to forecast Australiaâ€™s print media turnover. Which is most accurate, and which is least? Does it differ to the results from the in-sample (training set) accuracy?\n\n\nHowever a test set of two years isnâ€™t a very reliable indication of forecasting performance - what if these two years looked slightly different from the training data and one model got lucky?\nThe gold standard in forecasting performance evaluation is to use time series cross-validation. This involves creating many training and test splits across many time points in the data. The most common is to use a stretching window, which incrementally grows the training data to include new information. Instead of using filter() to create the training set, we will now use stretch_tsibble() to create the stretching â€˜foldsâ€™ of training data. The .id column identifies the fold of cross-validation for each series.\n\n\n\n\n\n\nCross-validation options\n\n\n\nIt is useful to set a few options in stretch_tsibble(), as the default can easily create 100s of folds. This helps reduce the workload for your computer while still giving a reasonable indication of your modelâ€™s forecasting performance!\n\n.init controls the initial fold size, Iâ€™ve set it to 48 months to include 4 years of data to start with\n.step controls how much additional data is introduced in each fold, 12 months will increase the training dataâ€™s length by 1 year at a time.\n\n\n\n\n\n# A tsibble: 7,920 x 3 [1M]\n# Key:       .id [33]\n      Month Turnover   .id\n      &lt;mth&gt;    &lt;dbl&gt; &lt;int&gt;\n 1 1982 Apr    6225.     1\n 2 1982 May    6382.     1\n 3 1982 Jun    6162.     1\n 4 1982 Jul    6399.     1\n 5 1982 Aug    6163.     1\n 6 1982 Sep    6331.     1\n 7 1982 Oct    6535.     1\n 8 1982 Nov    7022.     1\n 9 1982 Dec    9322.     1\n10 1983 Jan    6286.     1\n# â„¹ 7,910 more rows\n\n\n\n\n\n\n\n\n\n\n\nTo produce forecasts on the cross-validation folds and compute cross-validated accuracy summaries, we again train the models and use accuracy() but this time on the cross-validated data. This might take a while since we are now estimating a model making a forecast for every fold in cross-validated data!\n\n\n# A tibble: 2 Ã— 10\n  .model .type    ME  RMSE   MAE    MPE  MAPE  MASE RMSSE  ACF1\n  &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 arima  Test  -73.9  557.  429. -0.307  1.80 0.344 0.389 0.469\n2 ets    Test   70.3  653.  515.  0.430  2.19 0.414 0.456 0.503\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nNow compare the cross-validated accuracy statistics for all models used to forecast Australiaâ€™s print media turnover. Which is most accurate, and which is least? Does it differ to the results from the in-sample (training set) and out-of-sample (test set) accuracy?"
  },
  {
    "objectID": "sessions/day5/exercises.html#accuracy-summaries",
    "href": "sessions/day5/exercises.html#accuracy-summaries",
    "title": "Exercises",
    "section": "",
    "text": "In the previous exercise we looked at forecasting the total Australian retail turnover using ETS and ARIMA models, and we produced these forecasts:\n\n\n\n\n\n\n\n\n\nPlotting the forecasts simultaneously makes it easy to compare them. This allows us to see that while the seasonality and intervals from both models are similar, the trend from ARIMA is stronger than ETS. But which model is better?\nTo answer this we can summarise the forecasting performance of the models. The simplest is the accuracy on the historical training data.\nIn addition to the forecasts shown above, we produce 1-step forecasts on the training data when fitting the model. We can obtain these from the .fitted column of the augment() output.\n\n\n\n\n\n\n\n\n\nBoth models match the historical data closely, but which is more accurate? For this we can use the accuracy() function, which summarises the errors into a single summary statistic.\n\n\n# A tibble: 2 Ã— 10\n  .model .type        ME  RMSE   MAE     MPE  MAPE  MASE RMSSE    ACF1\n  &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 ets    Training   7.76  421.  327.  0.0486  1.55 0.261 0.294 -0.101 \n2 arima  Training -15.4   382.  277. -0.0713  1.22 0.221 0.266  0.0797\n\n\nBy default you will see a set of statistics that summarise the point forecasting accuracy, where the models closer to 0 are more accurate. MAE and RMSE are commonly used, but their scale independent versions MASE and RMSSE are useful if youâ€™re comparing between multiple datasets.\nIn all accuracy metrics we see that the ARIMA model is more accurate on the training data. But is it more accurate for forecasting?\n\n\n\n\n\n\nYour turn!\n\n\n\nCompare the in-sample accuracy statistics for all models used to forecast Australiaâ€™s print media turnover. Which is most accurate, and which is least? Does this align with your expectations?\n\n\nA more genuine approach to calculating forecasting performance is to use a training and test set split. This separates the data into two parts, and because the test data is not used in producing the forecasts, it should provide a reliable indication of how well the model is likely to forecast on new data.\nTo withhold some data for forecast evaluation, we first filter() the data to exclude the test period before training the model.\n\n\n\n\n\n\n\n\n\nWe can then calculate the same accuracy metrics on the forecasted test set using accuracy() again, but this time we need to provide the data used.\n\n\n# A tibble: 2 Ã— 10\n  .model .type     ME  RMSE   MAE   MPE  MAPE  MASE RMSSE    ACF1\n  &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 arima  Test  -1217. 1340. 1217. -2.41  2.41 0.979 0.935  0.0605\n2 ets    Test   -737.  913.  761. -1.44  1.49 0.612 0.637 -0.199 \n\n\nLooks like ETS was actually more accurate at forecasting the two year test set.\n\n\n\n\n\n\nYour turn!\n\n\n\nNow compare the out-of-sample (test set) accuracy statistics for all models used to forecast Australiaâ€™s print media turnover. Which is most accurate, and which is least? Does it differ to the results from the in-sample (training set) accuracy?\n\n\nHowever a test set of two years isnâ€™t a very reliable indication of forecasting performance - what if these two years looked slightly different from the training data and one model got lucky?\nThe gold standard in forecasting performance evaluation is to use time series cross-validation. This involves creating many training and test splits across many time points in the data. The most common is to use a stretching window, which incrementally grows the training data to include new information. Instead of using filter() to create the training set, we will now use stretch_tsibble() to create the stretching â€˜foldsâ€™ of training data. The .id column identifies the fold of cross-validation for each series.\n\n\n\n\n\n\nCross-validation options\n\n\n\nIt is useful to set a few options in stretch_tsibble(), as the default can easily create 100s of folds. This helps reduce the workload for your computer while still giving a reasonable indication of your modelâ€™s forecasting performance!\n\n.init controls the initial fold size, Iâ€™ve set it to 48 months to include 4 years of data to start with\n.step controls how much additional data is introduced in each fold, 12 months will increase the training dataâ€™s length by 1 year at a time.\n\n\n\n\n\n# A tsibble: 7,920 x 3 [1M]\n# Key:       .id [33]\n      Month Turnover   .id\n      &lt;mth&gt;    &lt;dbl&gt; &lt;int&gt;\n 1 1982 Apr    6225.     1\n 2 1982 May    6382.     1\n 3 1982 Jun    6162.     1\n 4 1982 Jul    6399.     1\n 5 1982 Aug    6163.     1\n 6 1982 Sep    6331.     1\n 7 1982 Oct    6535.     1\n 8 1982 Nov    7022.     1\n 9 1982 Dec    9322.     1\n10 1983 Jan    6286.     1\n# â„¹ 7,910 more rows\n\n\n\n\n\n\n\n\n\n\n\nTo produce forecasts on the cross-validation folds and compute cross-validated accuracy summaries, we again train the models and use accuracy() but this time on the cross-validated data. This might take a while since we are now estimating a model making a forecast for every fold in cross-validated data!\n\n\n# A tibble: 2 Ã— 10\n  .model .type    ME  RMSE   MAE    MPE  MAPE  MASE RMSSE  ACF1\n  &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 arima  Test  -73.9  557.  429. -0.307  1.80 0.344 0.389 0.469\n2 ets    Test   70.3  653.  515.  0.430  2.19 0.414 0.456 0.503\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nNow compare the cross-validated accuracy statistics for all models used to forecast Australiaâ€™s print media turnover. Which is most accurate, and which is least? Does it differ to the results from the in-sample (training set) and out-of-sample (test set) accuracy?"
  },
  {
    "objectID": "sessions/day3/exercises.html",
    "href": "sessions/day3/exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Forecasting involves modelling the historical patterns in the data and then projecting them into the future. Some models use time information alone, while others use additional information. Importantly, forecasting models assume the patterns in the past will continue into the future.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThere are four basic forecasting models which are commonly used as â€˜benchmarksâ€™ for other more sophisticated methods. These are:\n\nMEAN() - the average of the data (mean)\nNAIVE() - the most recent value (naive)\nSNAIVE() - the most recent value from the same season (seasonal naive)\nRW(y ~ drift()) - a straight between the first and last values (random walk with drift)\n\n\n\nDespite their simplicity, these models work well for many time series and can be difficult to improve upon!\n\n\n\n\n\n\nFit for purpose\n\n\n\nEach of these methods work for a specific pattern that might exist in the data.\n\nMEAN() - no pattern\nNAIVE() - unit root process\nSNAIVE() - seasonality\nRW(y ~ drift()) - simple trend\n\n\n\nThe models used for forecasting should match the patterns identified when plotting the time series.\nLetâ€™s look at the population of the United Kingdon\n\n\n\n\n\n\n\n\n\nThis time series shows an upward trend and no seasonality, so the random walk with drift is the most appropriate method from the four simple benchmark models above.\nSimilar to how we estimated an STL model, we use model() to train a model specification (RW(Population ~ drift())) to the data.\n\n\n# A mable: 1 x 2\n# Key:     Country [1]\n  Country        `RW(Population ~ drift())`\n  &lt;fct&gt;                             &lt;model&gt;\n1 United Kingdom              &lt;RW w/ drift&gt;\n\n\n\n\n\n\n\n\nThe model formula\n\n\n\nModels in {fable} are specified using a model formula (lhs ~ rhs).\nOn the left of ~ we specify the response variable (what we want to forecast) along with any transformations weâ€™ve made to simplify the patterns.\nOn the right of ~ we specify the model specials, which describe the patterns in the data we will use when forecasting. This is model specific, so check the help file of the model with ?RW for more information!\n\n\nTo produce a forecast from this model we use the forecast() function, and specify how far ahead we wish to forecast with the h (horizon) argument. The h argument can be a number for how many steps to forecast, or plain text describing the duration.\n\n\n# A fable: 10 x 5 [1Y]\n# Key:     Country, .model [1]\n   Country        .model                    Year\n   &lt;fct&gt;          &lt;chr&gt;                    &lt;dbl&gt;\n 1 United Kingdom RW(Population ~ drift())  2018\n 2 United Kingdom RW(Population ~ drift())  2019\n 3 United Kingdom RW(Population ~ drift())  2020\n 4 United Kingdom RW(Population ~ drift())  2021\n 5 United Kingdom RW(Population ~ drift())  2022\n 6 United Kingdom RW(Population ~ drift())  2023\n 7 United Kingdom RW(Population ~ drift())  2024\n 8 United Kingdom RW(Population ~ drift())  2025\n 9 United Kingdom RW(Population ~ drift())  2026\n10 United Kingdom RW(Population ~ drift())  2027\n# â„¹ 2 more variables: Population &lt;dist&gt;, .mean &lt;dbl&gt;\n\n\nHere we have a fable - a forecasting table. It looks like a tsibble, but the response variable Population contains entire distributions of possible future values at each step in the future. We can look at these forecasts using the autoplot() function.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContext is key\n\n\n\nWhen plotting the forecasts it is useful to also show some historical data. This helps us see if the forecasts seem reasonable. To add historical data, add the original dataset to the first argument of the autoplot() function.\n\n\n\n\n\n\n\n\n\n\n\nNot bad. These forecasts are trended upward but likely a bit flat. Verify that this forecast simply continues the line that connects the first and last observations. This trend is known as a global trend (or â€˜driftâ€™ for this model), but we can see the trend changes over time for this data. Later weâ€™ll see more advanced models which can handle changing (local) trends.\n\n\n\n\n\n\nYour turn!\n\n\n\nChoose a country from the global_economy dataset and select the most suitable benchmark method. Produce forecasts of population for 15 years into the future, and comment on the suitability of these forecasts based on a plot of them and the data.\n\n\nNext letâ€™s forecast the household wealth of the four countries in the hh_budget dataset.\n\n\n\n\n\n\n\n\n\nThese time series all show some trend that changes over time. There isnâ€™t any seasonality here, so the random walk with drift model would also work well here. The model() function will apply the specified model to all time series in the data, so the code looks very similar to above.\n\n\n# A mable: 4 x 2\n# Key:     Country [4]\n  Country   `RW(Wealth ~ drift())`\n  &lt;chr&gt;                    &lt;model&gt;\n1 Australia          &lt;RW w/ drift&gt;\n2 Canada             &lt;RW w/ drift&gt;\n3 Japan              &lt;RW w/ drift&gt;\n4 USA                &lt;RW w/ drift&gt;\n\n\nHere we have four random walk with drift models that have been trained on the household wealth from each of the four countries in the dataset. We can forecast from all four models using the forecast() function, and then plot them with autoplot().\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nComment on the suitability of these forecasts.\n\n\nLetâ€™s try to forecast the future turnover of Australiaâ€™s print media industry. Recall this plot from the previous exercises.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMost appropriate model\n\n\n\nWhich model would be most appropriate for this dataset? In this case none of the methods can capture all of the patterns here.\nThis dataset has a strong seasonal pattern, which a trend that changes over time.\nThe random walk with drift can handle trends, but in this case the changing trend does not match the global trend that this model will use.\nThe seasonal naive model can handle the seasonality, but it is unable to handle the trend too.\nNone of the four basic models can capture all of the patterns in this dataset, but the seasonal naive model is most appropriate since it can handle some of the patterns in the data.\n\n\n\n\n\n\n\n\n\n\n\nAs expected, the forecasts have the same seasonal pattern as the recent data but donâ€™t have any trend. Weâ€™ll need more advanced models to capture both.\n\n\n\n\n\n\nMultiple models\n\n\n\nWe can compare the forecasts from multiple models by specifying several models in the model() function.\n\n\n\n\n# A mable: 1 x 2\n    snaive       rwdrift\n   &lt;model&gt;       &lt;model&gt;\n1 &lt;SNAIVE&gt; &lt;RW w/ drift&gt;\n\n\nHere we have a column for each of the models that we have specified. Forecasts from both of these models can be created using forecast(), and compared visually with autoplot().\n\n\n\n\n\n\n\n\n\nThe seasonal naive method looks much better than the random walk with drift. The forecast intervals of the drift method are very wide, and the forecasts are trended slightly upward despite the recent turnover trending downward.\n\n\n\n\n\n\nYour turn!\n\n\n\nProduce forecasts from two suitable models for the total Australian retail turnover, and select the most appropriate one based on visual inspection of the forecast plot.\n\n\n\n\n\nLinear regression can also be used to forecast time series, and by carefully constructing predictors we can use it to capture trends, seasonality, and relationships with other variables all at once.\nA regression model is estimated using TSLM(), and there are some useful model specials which help create predictors for trend and seasonality.\n\n\n\n\n\n\nRegression specials\n\n\n\nA linear trend can be created with the trend() special. You can also specify changepoints in the trend by describing the â€˜knotâ€™ location(s) with trend(knots = yearmonth(\"2010 Jan\")), which will create different trends before and after these knot(s).\nSeasonal patterns can be modelled with the season() special, which will create dummy variables for each time point in the season. Donâ€™t forget to transform your data first, since the season() special assumes all seasons have the same size and shape.\n\n\nLetâ€™s try to create a regression model for the Australian print media turnover. Iâ€™ve used a log() transformation to regularise the variance, but a box-cox transformation would work even better.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel misspecification\n\n\n\nThose forecasts look bad! The seasonality matches the right shape, but the trend is completely wrong and the forecasts are very far from the most recent data. We need to improve our trend parameter with some knots.\n\n\n\n\n\n\n\n\n\n\n\nMuch better! Adding a knot just as the trend changes in 2011 allows the forecasts to follow the more recent trend.\n\n\n\n\n\n\nYour turn!\n\n\n\nProduce suitable forecasts from a regression model for the total Australian retail turnover that captures both the trend and seasonality in the data. Compare these forecasts with the two basic models produced earlier, which model produces the most reasonable forecasts and why?\n\n\nThe coefficients from this model can be obtained with the tidy() function, glance() provides a summary of the model and augment() returns a tsibble of the modelâ€™s predictions and errors on the training data. These functions are useful for better understanding the model that was used to produce the forecasts.\n\n\n# A tibble: 14 Ã— 6\n   .model term                            estimate std.error statistic   p.value\n   &lt;chr&gt;  &lt;chr&gt;                              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 lm     \"(Intercept)\"                    5.13    0.0154      334.    0        \n 2 lm     \"trend(knots = yearmonth(\\\"201â€¦  0.00248 0.0000396    62.6   3.45e-217\n 3 lm     \"trend(knots = yearmonth(\\\"201â€¦ -0.00843 0.000215    -39.3   7.68e-144\n 4 lm     \"season()year2\"                 -0.00887 0.0189       -0.469 6.39e-  1\n 5 lm     \"season()year3\"                  0.0262  0.0189        1.39  1.66e-  1\n 6 lm     \"season()year4\"                 -0.0830  0.0188       -4.42  1.24e-  5\n 7 lm     \"season()year5\"                 -0.0278  0.0188       -1.48  1.40e-  1\n 8 lm     \"season()year6\"                 -0.0864  0.0188       -4.60  5.52e-  6\n 9 lm     \"season()year7\"                 -0.0220  0.0188       -1.17  2.42e-  1\n10 lm     \"season()year8\"                  0.00197 0.0188        0.105 9.17e-  1\n11 lm     \"season()year9\"                 -0.0450  0.0188       -2.39  1.71e-  2\n12 lm     \"season()year10\"                -0.0257  0.0188       -1.37  1.72e-  1\n13 lm     \"season()year11\"                 0.0167  0.0188        0.888 3.75e-  1\n14 lm     \"season()year12\"                 0.274   0.0188       14.6   1.75e- 39\n\n\nThe initial trend is upward (+0.002477/month), but after 2011 the trend decreases (0.002477-0.008432=-0.005955/month). The seasonality peaks in December, which is +0.27426 more than January.\n\n\n# A tibble: 1 Ã— 15\n  .model r_squared adj_r_squared  sigma2 statistic   p_value    df log_lik\n  &lt;chr&gt;      &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 lm         0.913         0.910 0.00643      345. 6.25e-217    14    494.\n# â„¹ 7 more variables: AIC &lt;dbl&gt;, AICc &lt;dbl&gt;, BIC &lt;dbl&gt;, CV &lt;dbl&gt;,\n#   deviance &lt;dbl&gt;, df.residual &lt;int&gt;, rank &lt;int&gt;\n\n\nThe r-squared of this model is high, at 0.91.\n\n\n\n\n\n\n\n\n\nThe model matches the historical data quite well, but the small changes in trend before 2010 can be improved upon.\nRegression models can also use additional information from other variables in the data. Letâ€™s consider the household budget again.\n\n\n# A tsibble: 88 x 8 [1Y]\n# Key:       Country [4]\n   Country    Year  Debt     DI Expenditure Savings Wealth Unemployment\n   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt;\n 1 Australia  1995  95.7 3.72          3.40   5.24    315.         8.47\n 2 Australia  1996  99.5 3.98          2.97   6.47    315.         8.51\n 3 Australia  1997 108.  2.52          4.95   3.74    323.         8.36\n 4 Australia  1998 115.  4.02          5.73   1.29    339.         7.68\n 5 Australia  1999 121.  3.84          4.26   0.638   354.         6.87\n 6 Australia  2000 126.  3.77          3.18   1.99    350.         6.29\n 7 Australia  2001 132.  4.36          3.10   3.24    348.         6.74\n 8 Australia  2002 149.  0.0218        4.03  -1.15    349.         6.37\n 9 Australia  2003 159.  6.06          5.04  -0.413   360.         5.93\n10 Australia  2004 170.  5.53          4.54   0.657   379.         5.40\n# â„¹ 78 more rows\n\n\nHere we have lots of information about the households in these countries, including their debt, disposable income, savings, and more. We can use this information when modelling household wealth.\n\n\n\n\n\n\n\n\n\nThis seems to produce a better model than the random walk with drift, as it can better anticipate the drops in wealth before they happen. However thereâ€™s a catch, when we come to forecasting we need to know the futureâ€¦\n\n\nError in `mutate()`:\nâ„¹ In argument: `TSLM(Wealth ~ trend() + Expenditure) = (function\n  (object, ...) ...`.\nCaused by error in `value[[3L]]()`:\n! object 'Expenditure' not found\n  Unable to compute required variables from provided `new_data`.\n  Does your model require extra variables to produce forecasts?\n\n\n\n\n\n\n\n\nExtra information\n\n\n\n\nobject 'Expenditure' not found, â€¦, Does your model require extra variables to produce forecasts?\n\nTo produce forecasts from models that use extra information for transforming or modelling the data, you will need to provide the future values of these variables when forecasting! Often these are just as difficult to forecast as your response variable!\nHowever if you cannot forecast these variables, the model can still be useful for scenario analysis.\n\n\nThe future values of extra variables used in the model must be provided to the forecast(new_data = ???) argument. The new_data argument is for a tsibble containing the future points in time, and values of other variables, needed to produce the forecasts. We can produce a tsibble with the future time points easily using the new_data() function.\n\n\n# A tsibble: 20 x 2 [1Y]\n# Key:       Country [4]\n   Country    Year\n   &lt;chr&gt;     &lt;dbl&gt;\n 1 Australia  2017\n 2 Australia  2018\n 3 Australia  2019\n 4 Australia  2020\n 5 Australia  2021\n 6 Canada     2017\n 7 Canada     2018\n 8 Canada     2019\n 9 Canada     2020\n10 Canada     2021\n11 Japan      2017\n12 Japan      2018\n13 Japan      2019\n14 Japan      2020\n15 Japan      2021\n16 USA        2017\n17 USA        2018\n18 USA        2019\n19 USA        2020\n20 USA        2021\n\n\nAdding the future values for Expenditure is tricky though - we can forecast it or set up scenarios. For simplicity weâ€™ll just see what happens if the expenditure has a growth rate of 3% for all countries over the 5 years.\n\n\n# A tsibble: 20 x 3 [1Y]\n# Key:       Country [4]\n   Country    Year Expenditure\n   &lt;chr&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n 1 Australia  2017           3\n 2 Australia  2018           3\n 3 Australia  2019           3\n 4 Australia  2020           3\n 5 Australia  2021           3\n 6 Canada     2017           3\n 7 Canada     2018           3\n 8 Canada     2019           3\n 9 Canada     2020           3\n10 Canada     2021           3\n11 Japan      2017           3\n12 Japan      2018           3\n13 Japan      2019           3\n14 Japan      2020           3\n15 Japan      2021           3\n16 USA        2017           3\n17 USA        2018           3\n18 USA        2019           3\n19 USA        2020           3\n20 USA        2021           3\n\n\n\n\n\n\n\n\n\nA better estimate of Expenditure will produce better forecasts."
  },
  {
    "objectID": "sessions/day3/exercises.html#basic-forecasting-models",
    "href": "sessions/day3/exercises.html#basic-forecasting-models",
    "title": "Exercises",
    "section": "",
    "text": "Tip\n\n\n\nThere are four basic forecasting models which are commonly used as â€˜benchmarksâ€™ for other more sophisticated methods. These are:\n\nMEAN() - the average of the data (mean)\nNAIVE() - the most recent value (naive)\nSNAIVE() - the most recent value from the same season (seasonal naive)\nRW(y ~ drift()) - a straight between the first and last values (random walk with drift)\n\n\n\nDespite their simplicity, these models work well for many time series and can be difficult to improve upon!\n\n\n\n\n\n\nFit for purpose\n\n\n\nEach of these methods work for a specific pattern that might exist in the data.\n\nMEAN() - no pattern\nNAIVE() - unit root process\nSNAIVE() - seasonality\nRW(y ~ drift()) - simple trend\n\n\n\nThe models used for forecasting should match the patterns identified when plotting the time series.\nLetâ€™s look at the population of the United Kingdon\n\n\n\n\n\n\n\n\n\nThis time series shows an upward trend and no seasonality, so the random walk with drift is the most appropriate method from the four simple benchmark models above.\nSimilar to how we estimated an STL model, we use model() to train a model specification (RW(Population ~ drift())) to the data.\n\n\n# A mable: 1 x 2\n# Key:     Country [1]\n  Country        `RW(Population ~ drift())`\n  &lt;fct&gt;                             &lt;model&gt;\n1 United Kingdom              &lt;RW w/ drift&gt;\n\n\n\n\n\n\n\n\nThe model formula\n\n\n\nModels in {fable} are specified using a model formula (lhs ~ rhs).\nOn the left of ~ we specify the response variable (what we want to forecast) along with any transformations weâ€™ve made to simplify the patterns.\nOn the right of ~ we specify the model specials, which describe the patterns in the data we will use when forecasting. This is model specific, so check the help file of the model with ?RW for more information!\n\n\nTo produce a forecast from this model we use the forecast() function, and specify how far ahead we wish to forecast with the h (horizon) argument. The h argument can be a number for how many steps to forecast, or plain text describing the duration.\n\n\n# A fable: 10 x 5 [1Y]\n# Key:     Country, .model [1]\n   Country        .model                    Year\n   &lt;fct&gt;          &lt;chr&gt;                    &lt;dbl&gt;\n 1 United Kingdom RW(Population ~ drift())  2018\n 2 United Kingdom RW(Population ~ drift())  2019\n 3 United Kingdom RW(Population ~ drift())  2020\n 4 United Kingdom RW(Population ~ drift())  2021\n 5 United Kingdom RW(Population ~ drift())  2022\n 6 United Kingdom RW(Population ~ drift())  2023\n 7 United Kingdom RW(Population ~ drift())  2024\n 8 United Kingdom RW(Population ~ drift())  2025\n 9 United Kingdom RW(Population ~ drift())  2026\n10 United Kingdom RW(Population ~ drift())  2027\n# â„¹ 2 more variables: Population &lt;dist&gt;, .mean &lt;dbl&gt;\n\n\nHere we have a fable - a forecasting table. It looks like a tsibble, but the response variable Population contains entire distributions of possible future values at each step in the future. We can look at these forecasts using the autoplot() function.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContext is key\n\n\n\nWhen plotting the forecasts it is useful to also show some historical data. This helps us see if the forecasts seem reasonable. To add historical data, add the original dataset to the first argument of the autoplot() function.\n\n\n\n\n\n\n\n\n\n\n\nNot bad. These forecasts are trended upward but likely a bit flat. Verify that this forecast simply continues the line that connects the first and last observations. This trend is known as a global trend (or â€˜driftâ€™ for this model), but we can see the trend changes over time for this data. Later weâ€™ll see more advanced models which can handle changing (local) trends.\n\n\n\n\n\n\nYour turn!\n\n\n\nChoose a country from the global_economy dataset and select the most suitable benchmark method. Produce forecasts of population for 15 years into the future, and comment on the suitability of these forecasts based on a plot of them and the data.\n\n\nNext letâ€™s forecast the household wealth of the four countries in the hh_budget dataset.\n\n\n\n\n\n\n\n\n\nThese time series all show some trend that changes over time. There isnâ€™t any seasonality here, so the random walk with drift model would also work well here. The model() function will apply the specified model to all time series in the data, so the code looks very similar to above.\n\n\n# A mable: 4 x 2\n# Key:     Country [4]\n  Country   `RW(Wealth ~ drift())`\n  &lt;chr&gt;                    &lt;model&gt;\n1 Australia          &lt;RW w/ drift&gt;\n2 Canada             &lt;RW w/ drift&gt;\n3 Japan              &lt;RW w/ drift&gt;\n4 USA                &lt;RW w/ drift&gt;\n\n\nHere we have four random walk with drift models that have been trained on the household wealth from each of the four countries in the dataset. We can forecast from all four models using the forecast() function, and then plot them with autoplot().\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nComment on the suitability of these forecasts.\n\n\nLetâ€™s try to forecast the future turnover of Australiaâ€™s print media industry. Recall this plot from the previous exercises.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMost appropriate model\n\n\n\nWhich model would be most appropriate for this dataset? In this case none of the methods can capture all of the patterns here.\nThis dataset has a strong seasonal pattern, which a trend that changes over time.\nThe random walk with drift can handle trends, but in this case the changing trend does not match the global trend that this model will use.\nThe seasonal naive model can handle the seasonality, but it is unable to handle the trend too.\nNone of the four basic models can capture all of the patterns in this dataset, but the seasonal naive model is most appropriate since it can handle some of the patterns in the data.\n\n\n\n\n\n\n\n\n\n\n\nAs expected, the forecasts have the same seasonal pattern as the recent data but donâ€™t have any trend. Weâ€™ll need more advanced models to capture both.\n\n\n\n\n\n\nMultiple models\n\n\n\nWe can compare the forecasts from multiple models by specifying several models in the model() function.\n\n\n\n\n# A mable: 1 x 2\n    snaive       rwdrift\n   &lt;model&gt;       &lt;model&gt;\n1 &lt;SNAIVE&gt; &lt;RW w/ drift&gt;\n\n\nHere we have a column for each of the models that we have specified. Forecasts from both of these models can be created using forecast(), and compared visually with autoplot().\n\n\n\n\n\n\n\n\n\nThe seasonal naive method looks much better than the random walk with drift. The forecast intervals of the drift method are very wide, and the forecasts are trended slightly upward despite the recent turnover trending downward.\n\n\n\n\n\n\nYour turn!\n\n\n\nProduce forecasts from two suitable models for the total Australian retail turnover, and select the most appropriate one based on visual inspection of the forecast plot."
  },
  {
    "objectID": "sessions/day3/exercises.html#regression-forecasting",
    "href": "sessions/day3/exercises.html#regression-forecasting",
    "title": "Exercises",
    "section": "",
    "text": "Linear regression can also be used to forecast time series, and by carefully constructing predictors we can use it to capture trends, seasonality, and relationships with other variables all at once.\nA regression model is estimated using TSLM(), and there are some useful model specials which help create predictors for trend and seasonality.\n\n\n\n\n\n\nRegression specials\n\n\n\nA linear trend can be created with the trend() special. You can also specify changepoints in the trend by describing the â€˜knotâ€™ location(s) with trend(knots = yearmonth(\"2010 Jan\")), which will create different trends before and after these knot(s).\nSeasonal patterns can be modelled with the season() special, which will create dummy variables for each time point in the season. Donâ€™t forget to transform your data first, since the season() special assumes all seasons have the same size and shape.\n\n\nLetâ€™s try to create a regression model for the Australian print media turnover. Iâ€™ve used a log() transformation to regularise the variance, but a box-cox transformation would work even better.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel misspecification\n\n\n\nThose forecasts look bad! The seasonality matches the right shape, but the trend is completely wrong and the forecasts are very far from the most recent data. We need to improve our trend parameter with some knots.\n\n\n\n\n\n\n\n\n\n\n\nMuch better! Adding a knot just as the trend changes in 2011 allows the forecasts to follow the more recent trend.\n\n\n\n\n\n\nYour turn!\n\n\n\nProduce suitable forecasts from a regression model for the total Australian retail turnover that captures both the trend and seasonality in the data. Compare these forecasts with the two basic models produced earlier, which model produces the most reasonable forecasts and why?\n\n\nThe coefficients from this model can be obtained with the tidy() function, glance() provides a summary of the model and augment() returns a tsibble of the modelâ€™s predictions and errors on the training data. These functions are useful for better understanding the model that was used to produce the forecasts.\n\n\n# A tibble: 14 Ã— 6\n   .model term                            estimate std.error statistic   p.value\n   &lt;chr&gt;  &lt;chr&gt;                              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 lm     \"(Intercept)\"                    5.13    0.0154      334.    0        \n 2 lm     \"trend(knots = yearmonth(\\\"201â€¦  0.00248 0.0000396    62.6   3.45e-217\n 3 lm     \"trend(knots = yearmonth(\\\"201â€¦ -0.00843 0.000215    -39.3   7.68e-144\n 4 lm     \"season()year2\"                 -0.00887 0.0189       -0.469 6.39e-  1\n 5 lm     \"season()year3\"                  0.0262  0.0189        1.39  1.66e-  1\n 6 lm     \"season()year4\"                 -0.0830  0.0188       -4.42  1.24e-  5\n 7 lm     \"season()year5\"                 -0.0278  0.0188       -1.48  1.40e-  1\n 8 lm     \"season()year6\"                 -0.0864  0.0188       -4.60  5.52e-  6\n 9 lm     \"season()year7\"                 -0.0220  0.0188       -1.17  2.42e-  1\n10 lm     \"season()year8\"                  0.00197 0.0188        0.105 9.17e-  1\n11 lm     \"season()year9\"                 -0.0450  0.0188       -2.39  1.71e-  2\n12 lm     \"season()year10\"                -0.0257  0.0188       -1.37  1.72e-  1\n13 lm     \"season()year11\"                 0.0167  0.0188        0.888 3.75e-  1\n14 lm     \"season()year12\"                 0.274   0.0188       14.6   1.75e- 39\n\n\nThe initial trend is upward (+0.002477/month), but after 2011 the trend decreases (0.002477-0.008432=-0.005955/month). The seasonality peaks in December, which is +0.27426 more than January.\n\n\n# A tibble: 1 Ã— 15\n  .model r_squared adj_r_squared  sigma2 statistic   p_value    df log_lik\n  &lt;chr&gt;      &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 lm         0.913         0.910 0.00643      345. 6.25e-217    14    494.\n# â„¹ 7 more variables: AIC &lt;dbl&gt;, AICc &lt;dbl&gt;, BIC &lt;dbl&gt;, CV &lt;dbl&gt;,\n#   deviance &lt;dbl&gt;, df.residual &lt;int&gt;, rank &lt;int&gt;\n\n\nThe r-squared of this model is high, at 0.91.\n\n\n\n\n\n\n\n\n\nThe model matches the historical data quite well, but the small changes in trend before 2010 can be improved upon.\nRegression models can also use additional information from other variables in the data. Letâ€™s consider the household budget again.\n\n\n# A tsibble: 88 x 8 [1Y]\n# Key:       Country [4]\n   Country    Year  Debt     DI Expenditure Savings Wealth Unemployment\n   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt;\n 1 Australia  1995  95.7 3.72          3.40   5.24    315.         8.47\n 2 Australia  1996  99.5 3.98          2.97   6.47    315.         8.51\n 3 Australia  1997 108.  2.52          4.95   3.74    323.         8.36\n 4 Australia  1998 115.  4.02          5.73   1.29    339.         7.68\n 5 Australia  1999 121.  3.84          4.26   0.638   354.         6.87\n 6 Australia  2000 126.  3.77          3.18   1.99    350.         6.29\n 7 Australia  2001 132.  4.36          3.10   3.24    348.         6.74\n 8 Australia  2002 149.  0.0218        4.03  -1.15    349.         6.37\n 9 Australia  2003 159.  6.06          5.04  -0.413   360.         5.93\n10 Australia  2004 170.  5.53          4.54   0.657   379.         5.40\n# â„¹ 78 more rows\n\n\nHere we have lots of information about the households in these countries, including their debt, disposable income, savings, and more. We can use this information when modelling household wealth.\n\n\n\n\n\n\n\n\n\nThis seems to produce a better model than the random walk with drift, as it can better anticipate the drops in wealth before they happen. However thereâ€™s a catch, when we come to forecasting we need to know the futureâ€¦\n\n\nError in `mutate()`:\nâ„¹ In argument: `TSLM(Wealth ~ trend() + Expenditure) = (function\n  (object, ...) ...`.\nCaused by error in `value[[3L]]()`:\n! object 'Expenditure' not found\n  Unable to compute required variables from provided `new_data`.\n  Does your model require extra variables to produce forecasts?\n\n\n\n\n\n\n\n\nExtra information\n\n\n\n\nobject 'Expenditure' not found, â€¦, Does your model require extra variables to produce forecasts?\n\nTo produce forecasts from models that use extra information for transforming or modelling the data, you will need to provide the future values of these variables when forecasting! Often these are just as difficult to forecast as your response variable!\nHowever if you cannot forecast these variables, the model can still be useful for scenario analysis.\n\n\nThe future values of extra variables used in the model must be provided to the forecast(new_data = ???) argument. The new_data argument is for a tsibble containing the future points in time, and values of other variables, needed to produce the forecasts. We can produce a tsibble with the future time points easily using the new_data() function.\n\n\n# A tsibble: 20 x 2 [1Y]\n# Key:       Country [4]\n   Country    Year\n   &lt;chr&gt;     &lt;dbl&gt;\n 1 Australia  2017\n 2 Australia  2018\n 3 Australia  2019\n 4 Australia  2020\n 5 Australia  2021\n 6 Canada     2017\n 7 Canada     2018\n 8 Canada     2019\n 9 Canada     2020\n10 Canada     2021\n11 Japan      2017\n12 Japan      2018\n13 Japan      2019\n14 Japan      2020\n15 Japan      2021\n16 USA        2017\n17 USA        2018\n18 USA        2019\n19 USA        2020\n20 USA        2021\n\n\nAdding the future values for Expenditure is tricky though - we can forecast it or set up scenarios. For simplicity weâ€™ll just see what happens if the expenditure has a growth rate of 3% for all countries over the 5 years.\n\n\n# A tsibble: 20 x 3 [1Y]\n# Key:       Country [4]\n   Country    Year Expenditure\n   &lt;chr&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n 1 Australia  2017           3\n 2 Australia  2018           3\n 3 Australia  2019           3\n 4 Australia  2020           3\n 5 Australia  2021           3\n 6 Canada     2017           3\n 7 Canada     2018           3\n 8 Canada     2019           3\n 9 Canada     2020           3\n10 Canada     2021           3\n11 Japan      2017           3\n12 Japan      2018           3\n13 Japan      2019           3\n14 Japan      2020           3\n15 Japan      2021           3\n16 USA        2017           3\n17 USA        2018           3\n18 USA        2019           3\n19 USA        2020           3\n20 USA        2021           3\n\n\n\n\n\n\n\n\n\nA better estimate of Expenditure will produce better forecasts."
  }
]